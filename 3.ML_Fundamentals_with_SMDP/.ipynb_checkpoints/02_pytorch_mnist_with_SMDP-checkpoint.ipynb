{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Training using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Host](#Host)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "MNIST is a widely used dataset for handwritten digit classification. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits). This tutorial will show how to train and test an MNIST model on SageMaker using PyTorch.\n",
    "\n",
    "For more information about the PyTorch in SageMaker, please visit [sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers) and [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) github repositories.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.m4.xlarge notebook instance._\n",
    "\n",
    "Let's start by creating a SageMaker session and specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the `sagemaker.get_execution_role()` with a the appropriate full IAM role arn string(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "install_needed = True  # should only be True once\n",
    "install_needed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "\n",
    "if install_needed:\n",
    "    print(\"installing deps and restarting kernel\")\n",
    "    !{sys.executable} -m pip install --upgrade pip \n",
    "    !{sys.executable} -m pip install -U sagemaker smdebug\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.debugger import Rule, ProfilerRule, rule_configs\n",
    "from sagemaker.debugger import ProfilerConfig, FrameworkProfile\n",
    "import time\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-pytorch-mnist'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.24.2'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Getting the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "datasets.MNIST('data', download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the data to S3\n",
    "We are going to use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use later when we start the training job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-east-1-322537213286/sagemaker/DEMO-pytorch-mnist\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path='data', bucket=bucket, key_prefix=prefix)\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "### Training script\n",
    "The `mnist.py` script provides all the code we need for training and hosting a SageMaker model (`model_fn` function to load a model).\n",
    "The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, such as:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string representing the path to the directory to write model artifacts to.\n",
    "  These artifacts are uploaded to S3 for model hosting.\n",
    "* `SM_NUM_GPUS`: The number of gpus available in the current container.\n",
    "* `SM_CURRENT_HOST`: The name of the current container on the container network.\n",
    "* `SM_HOSTS`: JSON encoded list containing all the hosts .\n",
    "\n",
    "Supposing one input channel, 'training', was used in the call to the PyTorch estimator's `fit()` method, the following will be set, following the format `SM_CHANNEL_[channel_name]`:\n",
    "\n",
    "* `SM_CHANNEL_TRAINING`: A string representing the path to the directory containing data in the 'training' channel.\n",
    "\n",
    "For more information about training environment variables, please visit [SageMaker Containers](https://github.com/aws/sagemaker-containers).\n",
    "\n",
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to `model_dir` so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an `argparse.ArgumentParser` instance.\n",
    "\n",
    "Because the SageMaker imports the training script, you should put your training code in a main guard (``if __name__=='__main__':``) if you are using the same script to host your model as we do in this example, so that SageMaker does not inadvertently run your training code at the wrong point in execution.\n",
    "\n",
    "For example, the script run by this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[37m# import sagemaker_containers\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mdist\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datasets, transforms\n",
      "\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "logger.setLevel(logging.DEBUG)\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
      "\n",
      "\n",
      "\u001b[37m# Based on https://github.com/pytorch/examples/blob/master/mnist/main.py\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mNet\u001b[39;49;00m(nn.Module):\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
      "        \u001b[36msuper\u001b[39;49;00m(Net, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\n",
      "        \u001b[36mself\u001b[39;49;00m.conv1 = nn.Conv2d(\u001b[34m1\u001b[39;49;00m, \u001b[34m10\u001b[39;49;00m, kernel_size=\u001b[34m5\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.conv2 = nn.Conv2d(\u001b[34m10\u001b[39;49;00m, \u001b[34m20\u001b[39;49;00m, kernel_size=\u001b[34m5\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.conv2_drop = nn.Dropout2d()\n",
      "        \u001b[36mself\u001b[39;49;00m.fc1 = nn.Linear(\u001b[34m320\u001b[39;49;00m, \u001b[34m50\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.fc2 = nn.Linear(\u001b[34m50\u001b[39;49;00m, \u001b[34m10\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\n",
      "        x = F.relu(F.max_pool2d(\u001b[36mself\u001b[39;49;00m.conv1(x), \u001b[34m2\u001b[39;49;00m))\n",
      "        x = F.relu(F.max_pool2d(\u001b[36mself\u001b[39;49;00m.conv2_drop(\u001b[36mself\u001b[39;49;00m.conv2(x)), \u001b[34m2\u001b[39;49;00m))\n",
      "        x = x.view(-\u001b[34m1\u001b[39;49;00m, \u001b[34m320\u001b[39;49;00m)\n",
      "        x = F.relu(\u001b[36mself\u001b[39;49;00m.fc1(x))\n",
      "        x = F.dropout(x, training=\u001b[36mself\u001b[39;49;00m.training)\n",
      "        x = \u001b[36mself\u001b[39;49;00m.fc2(x)\n",
      "        \u001b[34mreturn\u001b[39;49;00m F.log_softmax(x, dim=\u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_train_data_loader\u001b[39;49;00m(args, is_distributed, **kwargs):\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet train data loader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    dataset = datasets.MNIST(args.data_dir, train=\u001b[34mTrue\u001b[39;49;00m, transform=transforms.Compose([\n",
      "        transforms.ToTensor(),\n",
      "        transforms.Normalize((\u001b[34m0.1307\u001b[39;49;00m,), (\u001b[34m0.3081\u001b[39;49;00m,))\n",
      "    ]))\n",
      "    train_sampler = torch.utils.data.distributed.DistributedSampler(dataset) \u001b[34mif\u001b[39;49;00m is_distributed \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=train_sampler \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m,\n",
      "                                       sampler=train_sampler, **kwargs)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_test_data_loader\u001b[39;49;00m(args, **kwargs):\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet test data loader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m torch.utils.data.DataLoader(\n",
      "        datasets.MNIST(args.data_dir, train=\u001b[34mFalse\u001b[39;49;00m, transform=transforms.Compose([\n",
      "            transforms.ToTensor(),\n",
      "            transforms.Normalize((\u001b[34m0.1307\u001b[39;49;00m,), (\u001b[34m0.3081\u001b[39;49;00m,))\n",
      "        ])),\n",
      "        batch_size=args.test_batch_size, shuffle=\u001b[34mTrue\u001b[39;49;00m, **kwargs)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(args):\n",
      "    is_distributed = \u001b[34mNone\u001b[39;49;00m\n",
      "    use_cuda = torch.cuda.is_available()\n",
      "    kwargs = {\u001b[33m'\u001b[39;49;00m\u001b[33mnum_workers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[34m4\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpin_memory\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[34mTrue\u001b[39;49;00m} \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m {}\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# set the seed for generating random numbers\u001b[39;49;00m\n",
      "    torch.manual_seed(args.seed)\n",
      "    \u001b[34mif\u001b[39;49;00m use_cuda:\n",
      "        torch.cuda.manual_seed(args.seed)\n",
      "\n",
      "    train_loader = _get_train_data_loader(args, is_distributed, **kwargs)\n",
      "    test_loader = _get_test_data_loader(args, **kwargs)\n",
      "\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mProcesses \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m) of train data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\n",
      "        \u001b[36mlen\u001b[39;49;00m(train_loader.sampler), \u001b[36mlen\u001b[39;49;00m(train_loader.dataset),\n",
      "        \u001b[34m100.\u001b[39;49;00m * \u001b[36mlen\u001b[39;49;00m(train_loader.sampler) / \u001b[36mlen\u001b[39;49;00m(train_loader.dataset)\n",
      "    ))\n",
      "\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mProcesses \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m) of test data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\n",
      "        \u001b[36mlen\u001b[39;49;00m(test_loader.sampler), \u001b[36mlen\u001b[39;49;00m(test_loader.dataset),\n",
      "        \u001b[34m100.\u001b[39;49;00m * \u001b[36mlen\u001b[39;49;00m(test_loader.sampler) / \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)\n",
      "    ))\n",
      "\n",
      "    model = Net().to(device)\n",
      "    model = torch.nn.DataParallel(model)\n",
      "\n",
      "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
      "\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, args.epochs + \u001b[34m1\u001b[39;49;00m):\n",
      "        model.train()\n",
      "        \u001b[34mfor\u001b[39;49;00m batch_idx, (data, target) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader, \u001b[34m1\u001b[39;49;00m):\n",
      "            data, target = data.to(device), target.to(device)\n",
      "            optimizer.zero_grad()\n",
      "            output = model(data)\n",
      "            loss = F.nll_loss(output, target)\n",
      "            loss.backward()\n",
      "            optimizer.step()\n",
      "            \n",
      "            \u001b[34mif\u001b[39;49;00m batch_idx % args.log_interval == \u001b[34m0\u001b[39;49;00m:\n",
      "                logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mTrain Epoch: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m [\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)] Loss: \u001b[39;49;00m\u001b[33m{:.6f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\n",
      "                    epoch, batch_idx * \u001b[36mlen\u001b[39;49;00m(data), \u001b[36mlen\u001b[39;49;00m(train_loader.sampler),\n",
      "                    \u001b[34m100.\u001b[39;49;00m * batch_idx / \u001b[36mlen\u001b[39;49;00m(train_loader), loss.item()))\n",
      "        test(model, test_loader, device)\n",
      "    \n",
      "    save_model(model, args.model_dir)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest\u001b[39;49;00m(model, test_loader, device):\n",
      "    model.eval()\n",
      "    test_loss = \u001b[34m0\u001b[39;49;00m\n",
      "    correct = \u001b[34m0\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n",
      "        \u001b[34mfor\u001b[39;49;00m data, target \u001b[35min\u001b[39;49;00m test_loader:\n",
      "            data, target = data.to(device), target.to(device)\n",
      "            output = model(data)\n",
      "            test_loss += F.nll_loss(output, target, size_average=\u001b[34mFalse\u001b[39;49;00m).item()  \u001b[37m# sum up batch loss\u001b[39;49;00m\n",
      "            pred = output.max(\u001b[34m1\u001b[39;49;00m, keepdim=\u001b[34mTrue\u001b[39;49;00m)[\u001b[34m1\u001b[39;49;00m]  \u001b[37m# get the index of the max log-probability\u001b[39;49;00m\n",
      "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
      "\n",
      "    test_loss /= \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mTest set: Average loss: \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m, Accuracy: \u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\n",
      "        test_loss, \u001b[34m100.\u001b[39;49;00m * correct / \u001b[36mlen\u001b[39;49;00m(test_loader.dataset), correct, \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)))\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model = torch.nn.DataParallel(Net())\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.load_state_dict(torch.load(f))\n",
      "    \u001b[34mreturn\u001b[39;49;00m model.to(device)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_model\u001b[39;49;00m(model, model_dir):\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving the model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[37m# recommended way from http://pytorch.org/docs/master/notes/serialization.html\u001b[39;49;00m\n",
      "    torch.save(model.cpu().state_dict(), path)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# Data and model checkpoints directories\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test-batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1000\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for testing (default: 1000)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.01\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mlearning rate (default: 0.01)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.5\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mSGD momentum (default: 0.5)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--log-interval\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m100\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mhow many batches to wait before logging training status\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# SageMaker Container environment\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33m./model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33m./data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    args = parser.parse_args()\n",
      "    \n",
      "    \u001b[34mtry\u001b[39;49;00m:\n",
      "        args.model_dir = os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        args.data_dir = os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mKeyError\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m e:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mThe model starts training on the local host without SageMaker TrainingJob.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(args.model_dir):\n",
      "            os.makedirs(args.model_dir)\n",
      "        \u001b[34mpass\u001b[39;49;00m\n",
      "\n",
      "    train(args)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize mnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model starts training on the local host without SageMaker TrainingJob.\n",
      "Get train data loader\n",
      "Get test data loader\n",
      "Processes 60000/60000 (100%) of train data\n",
      "Processes 10000/10000 (100%) of test data\n",
      "Train Epoch: 1 [6400/60000 (11%)] Loss: 2.010908\n",
      "Train Epoch: 1 [12800/60000 (21%)] Loss: 1.009527\n",
      "Train Epoch: 1 [19200/60000 (32%)] Loss: 0.877344\n",
      "Train Epoch: 1 [25600/60000 (43%)] Loss: 0.815463\n",
      "Train Epoch: 1 [32000/60000 (53%)] Loss: 0.641028\n",
      "Train Epoch: 1 [38400/60000 (64%)] Loss: 0.507817\n",
      "Train Epoch: 1 [44800/60000 (75%)] Loss: 0.535475\n",
      "Train Epoch: 1 [51200/60000 (85%)] Loss: 0.532028\n",
      "Train Epoch: 1 [57600/60000 (96%)] Loss: 0.443236\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "Test set: Average loss: 0.1925, Accuracy: 94% (9423/10000)\n",
      "\n",
      "Train Epoch: 2 [6400/60000 (11%)] Loss: 0.528714\n",
      "Train Epoch: 2 [12800/60000 (21%)] Loss: 0.444617\n",
      "Train Epoch: 2 [19200/60000 (32%)] Loss: 0.498515\n",
      "Train Epoch: 2 [25600/60000 (43%)] Loss: 0.324654\n",
      "Train Epoch: 2 [32000/60000 (53%)] Loss: 0.362580\n",
      "Train Epoch: 2 [38400/60000 (64%)] Loss: 0.355552\n",
      "Train Epoch: 2 [44800/60000 (75%)] Loss: 0.239494\n",
      "Train Epoch: 2 [51200/60000 (85%)] Loss: 0.507443\n",
      "Train Epoch: 2 [57600/60000 (96%)] Loss: 0.420418\n",
      "Test set: Average loss: 0.1232, Accuracy: 96% (9611/10000)\n",
      "\n",
      "Train Epoch: 3 [6400/60000 (11%)] Loss: 0.388234\n",
      "Train Epoch: 3 [12800/60000 (21%)] Loss: 0.423777\n",
      "Train Epoch: 3 [19200/60000 (32%)] Loss: 0.238433\n",
      "Train Epoch: 3 [25600/60000 (43%)] Loss: 0.506704\n",
      "Train Epoch: 3 [32000/60000 (53%)] Loss: 0.294763\n",
      "Train Epoch: 3 [38400/60000 (64%)] Loss: 0.205159\n",
      "Train Epoch: 3 [44800/60000 (75%)] Loss: 0.284953\n",
      "Train Epoch: 3 [51200/60000 (85%)] Loss: 0.201325\n",
      "Train Epoch: 3 [57600/60000 (96%)] Loss: 0.422757\n",
      "Test set: Average loss: 0.0980, Accuracy: 97% (9703/10000)\n",
      "\n",
      "Train Epoch: 4 [6400/60000 (11%)] Loss: 0.481137\n",
      "Train Epoch: 4 [12800/60000 (21%)] Loss: 0.174972\n",
      "Train Epoch: 4 [19200/60000 (32%)] Loss: 0.472706\n",
      "Train Epoch: 4 [25600/60000 (43%)] Loss: 0.185762\n",
      "Train Epoch: 4 [32000/60000 (53%)] Loss: 0.227326\n",
      "Train Epoch: 4 [38400/60000 (64%)] Loss: 0.306106\n",
      "Train Epoch: 4 [44800/60000 (75%)] Loss: 0.341378\n",
      "Train Epoch: 4 [51200/60000 (85%)] Loss: 0.154664\n",
      "Train Epoch: 4 [57600/60000 (96%)] Loss: 0.425848\n",
      "Test set: Average loss: 0.0827, Accuracy: 97% (9740/10000)\n",
      "\n",
      "Train Epoch: 5 [6400/60000 (11%)] Loss: 0.117861\n",
      "Train Epoch: 5 [12800/60000 (21%)] Loss: 0.243307\n",
      "Train Epoch: 5 [19200/60000 (32%)] Loss: 0.310509\n",
      "Train Epoch: 5 [25600/60000 (43%)] Loss: 0.137007\n",
      "Train Epoch: 5 [32000/60000 (53%)] Loss: 0.217726\n",
      "Train Epoch: 5 [38400/60000 (64%)] Loss: 0.250942\n",
      "Train Epoch: 5 [44800/60000 (75%)] Loss: 0.223275\n",
      "Train Epoch: 5 [51200/60000 (85%)] Loss: 0.116138\n",
      "Train Epoch: 5 [57600/60000 (96%)] Loss: 0.253724\n",
      "Test set: Average loss: 0.0726, Accuracy: 98% (9777/10000)\n",
      "\n",
      "Train Epoch: 6 [6400/60000 (11%)] Loss: 0.297286\n",
      "Train Epoch: 6 [12800/60000 (21%)] Loss: 0.204465\n",
      "Train Epoch: 6 [19200/60000 (32%)] Loss: 0.274777\n",
      "Train Epoch: 6 [25600/60000 (43%)] Loss: 0.263141\n",
      "Train Epoch: 6 [32000/60000 (53%)] Loss: 0.150751\n",
      "Train Epoch: 6 [38400/60000 (64%)] Loss: 0.267518\n",
      "Train Epoch: 6 [44800/60000 (75%)] Loss: 0.134127\n",
      "Train Epoch: 6 [51200/60000 (85%)] Loss: 0.221532\n",
      "Train Epoch: 6 [57600/60000 (96%)] Loss: 0.103764\n",
      "Test set: Average loss: 0.0699, Accuracy: 98% (9779/10000)\n",
      "\n",
      "Train Epoch: 7 [6400/60000 (11%)] Loss: 0.186492\n",
      "Train Epoch: 7 [12800/60000 (21%)] Loss: 0.255765\n",
      "Train Epoch: 7 [19200/60000 (32%)] Loss: 0.090896\n",
      "Train Epoch: 7 [25600/60000 (43%)] Loss: 0.203243\n",
      "Train Epoch: 7 [32000/60000 (53%)] Loss: 0.166611\n",
      "Train Epoch: 7 [38400/60000 (64%)] Loss: 0.258279\n",
      "Train Epoch: 7 [44800/60000 (75%)] Loss: 0.213229\n",
      "Train Epoch: 7 [51200/60000 (85%)] Loss: 0.124558\n",
      "Train Epoch: 7 [57600/60000 (96%)] Loss: 0.104446\n",
      "Test set: Average loss: 0.0607, Accuracy: 98% (9819/10000)\n",
      "\n",
      "Train Epoch: 8 [6400/60000 (11%)] Loss: 0.132228\n",
      "Train Epoch: 8 [12800/60000 (21%)] Loss: 0.193818\n",
      "Train Epoch: 8 [19200/60000 (32%)] Loss: 0.108591\n",
      "Train Epoch: 8 [25600/60000 (43%)] Loss: 0.132536\n",
      "Train Epoch: 8 [32000/60000 (53%)] Loss: 0.219873\n",
      "Train Epoch: 8 [38400/60000 (64%)] Loss: 0.112305\n",
      "Train Epoch: 8 [44800/60000 (75%)] Loss: 0.139741\n",
      "Train Epoch: 8 [51200/60000 (85%)] Loss: 0.208339\n",
      "Train Epoch: 8 [57600/60000 (96%)] Loss: 0.134547\n",
      "Test set: Average loss: 0.0544, Accuracy: 98% (9831/10000)\n",
      "\n",
      "Train Epoch: 9 [6400/60000 (11%)] Loss: 0.168099\n",
      "Train Epoch: 9 [12800/60000 (21%)] Loss: 0.169516\n",
      "Train Epoch: 9 [19200/60000 (32%)] Loss: 0.241020\n",
      "Train Epoch: 9 [25600/60000 (43%)] Loss: 0.114131\n",
      "Train Epoch: 9 [32000/60000 (53%)] Loss: 0.198015\n",
      "Train Epoch: 9 [38400/60000 (64%)] Loss: 0.171733\n",
      "Train Epoch: 9 [44800/60000 (75%)] Loss: 0.079162\n",
      "Train Epoch: 9 [51200/60000 (85%)] Loss: 0.267699\n",
      "Train Epoch: 9 [57600/60000 (96%)] Loss: 0.183090\n",
      "Test set: Average loss: 0.0529, Accuracy: 98% (9835/10000)\n",
      "\n",
      "Train Epoch: 10 [6400/60000 (11%)] Loss: 0.179440\n",
      "Train Epoch: 10 [12800/60000 (21%)] Loss: 0.343906\n",
      "Train Epoch: 10 [19200/60000 (32%)] Loss: 0.247909\n",
      "Train Epoch: 10 [25600/60000 (43%)] Loss: 0.129979\n",
      "Train Epoch: 10 [32000/60000 (53%)] Loss: 0.235235\n",
      "Train Epoch: 10 [38400/60000 (64%)] Loss: 0.178963\n",
      "Train Epoch: 10 [44800/60000 (75%)] Loss: 0.139076\n",
      "Train Epoch: 10 [51200/60000 (85%)] Loss: 0.087906\n",
      "Train Epoch: 10 [57600/60000 (96%)] Loss: 0.145033\n",
      "Test set: Average loss: 0.0512, Accuracy: 98% (9836/10000)\n",
      "\n",
      "Train Epoch: 11 [6400/60000 (11%)] Loss: 0.112347\n",
      "Train Epoch: 11 [12800/60000 (21%)] Loss: 0.109956\n",
      "Train Epoch: 11 [19200/60000 (32%)] Loss: 0.337645\n",
      "Train Epoch: 11 [25600/60000 (43%)] Loss: 0.117836\n",
      "Train Epoch: 11 [32000/60000 (53%)] Loss: 0.124942\n",
      "Train Epoch: 11 [38400/60000 (64%)] Loss: 0.103035\n",
      "Train Epoch: 11 [44800/60000 (75%)] Loss: 0.194975\n",
      "Train Epoch: 11 [51200/60000 (85%)] Loss: 0.225677\n",
      "Train Epoch: 11 [57600/60000 (96%)] Loss: 0.093505\n",
      "Test set: Average loss: 0.0528, Accuracy: 98% (9832/10000)\n",
      "\n",
      "Train Epoch: 12 [6400/60000 (11%)] Loss: 0.089804\n",
      "Train Epoch: 12 [12800/60000 (21%)] Loss: 0.095779\n",
      "Train Epoch: 12 [19200/60000 (32%)] Loss: 0.152955\n",
      "Train Epoch: 12 [25600/60000 (43%)] Loss: 0.143132\n",
      "Train Epoch: 12 [32000/60000 (53%)] Loss: 0.132360\n",
      "Train Epoch: 12 [38400/60000 (64%)] Loss: 0.157541\n",
      "Train Epoch: 12 [44800/60000 (75%)] Loss: 0.161552\n",
      "Train Epoch: 12 [51200/60000 (85%)] Loss: 0.155948\n",
      "Train Epoch: 12 [57600/60000 (96%)] Loss: 0.041595\n",
      "Test set: Average loss: 0.0487, Accuracy: 98% (9845/10000)\n",
      "\n",
      "Train Epoch: 13 [6400/60000 (11%)] Loss: 0.072971\n",
      "Train Epoch: 13 [12800/60000 (21%)] Loss: 0.189937\n",
      "Train Epoch: 13 [19200/60000 (32%)] Loss: 0.157842\n",
      "Train Epoch: 13 [25600/60000 (43%)] Loss: 0.277062\n",
      "Train Epoch: 13 [32000/60000 (53%)] Loss: 0.129922\n",
      "Train Epoch: 13 [38400/60000 (64%)] Loss: 0.165459\n",
      "Train Epoch: 13 [44800/60000 (75%)] Loss: 0.083179\n",
      "Train Epoch: 13 [51200/60000 (85%)] Loss: 0.419412\n",
      "Train Epoch: 13 [57600/60000 (96%)] Loss: 0.229802\n",
      "Test set: Average loss: 0.0445, Accuracy: 99% (9858/10000)\n",
      "\n",
      "Train Epoch: 14 [6400/60000 (11%)] Loss: 0.129008\n",
      "Train Epoch: 14 [12800/60000 (21%)] Loss: 0.104240\n",
      "Train Epoch: 14 [19200/60000 (32%)] Loss: 0.193954\n",
      "Train Epoch: 14 [25600/60000 (43%)] Loss: 0.143529\n",
      "Train Epoch: 14 [32000/60000 (53%)] Loss: 0.101811\n",
      "Train Epoch: 14 [38400/60000 (64%)] Loss: 0.131396\n",
      "Train Epoch: 14 [44800/60000 (75%)] Loss: 0.135647\n",
      "Train Epoch: 14 [51200/60000 (85%)] Loss: 0.115956\n",
      "Train Epoch: 14 [57600/60000 (96%)] Loss: 0.057453\n",
      "Test set: Average loss: 0.0448, Accuracy: 99% (9860/10000)\n",
      "\n",
      "Train Epoch: 15 [6400/60000 (11%)] Loss: 0.162703\n",
      "Train Epoch: 15 [12800/60000 (21%)] Loss: 0.145482\n",
      "Train Epoch: 15 [19200/60000 (32%)] Loss: 0.311715\n",
      "Train Epoch: 15 [25600/60000 (43%)] Loss: 0.086519\n",
      "Train Epoch: 15 [32000/60000 (53%)] Loss: 0.156471\n",
      "Train Epoch: 15 [38400/60000 (64%)] Loss: 0.018298\n",
      "Train Epoch: 15 [44800/60000 (75%)] Loss: 0.227623\n",
      "Train Epoch: 15 [51200/60000 (85%)] Loss: 0.105089\n",
      "Train Epoch: 15 [57600/60000 (96%)] Loss: 0.232861\n",
      "Test set: Average loss: 0.0461, Accuracy: 99% (9867/10000)\n",
      "\n",
      "Train Epoch: 16 [6400/60000 (11%)] Loss: 0.093308\n",
      "Train Epoch: 16 [12800/60000 (21%)] Loss: 0.046667\n",
      "Train Epoch: 16 [19200/60000 (32%)] Loss: 0.202491\n",
      "Train Epoch: 16 [25600/60000 (43%)] Loss: 0.162306\n",
      "Train Epoch: 16 [32000/60000 (53%)] Loss: 0.057842\n",
      "Train Epoch: 16 [38400/60000 (64%)] Loss: 0.077769\n",
      "Train Epoch: 16 [44800/60000 (75%)] Loss: 0.163895\n",
      "Train Epoch: 16 [51200/60000 (85%)] Loss: 0.225365\n",
      "Train Epoch: 16 [57600/60000 (96%)] Loss: 0.131262\n",
      "Test set: Average loss: 0.0454, Accuracy: 99% (9859/10000)\n",
      "\n",
      "Train Epoch: 17 [6400/60000 (11%)] Loss: 0.233300\n",
      "Train Epoch: 17 [12800/60000 (21%)] Loss: 0.151779\n",
      "Train Epoch: 17 [19200/60000 (32%)] Loss: 0.062071\n",
      "Train Epoch: 17 [25600/60000 (43%)] Loss: 0.110518\n",
      "Train Epoch: 17 [32000/60000 (53%)] Loss: 0.083245\n",
      "Train Epoch: 17 [38400/60000 (64%)] Loss: 0.190204\n",
      "Train Epoch: 17 [44800/60000 (75%)] Loss: 0.153745\n",
      "Train Epoch: 17 [51200/60000 (85%)] Loss: 0.081340\n",
      "Train Epoch: 17 [57600/60000 (96%)] Loss: 0.093878\n",
      "Test set: Average loss: 0.0430, Accuracy: 99% (9871/10000)\n",
      "\n",
      "Train Epoch: 18 [6400/60000 (11%)] Loss: 0.094497\n",
      "Train Epoch: 18 [12800/60000 (21%)] Loss: 0.113848\n",
      "Train Epoch: 18 [19200/60000 (32%)] Loss: 0.157372\n",
      "Train Epoch: 18 [25600/60000 (43%)] Loss: 0.229536\n",
      "Train Epoch: 18 [32000/60000 (53%)] Loss: 0.043970\n",
      "Train Epoch: 18 [38400/60000 (64%)] Loss: 0.066081\n",
      "Train Epoch: 18 [44800/60000 (75%)] Loss: 0.212884\n",
      "Train Epoch: 18 [51200/60000 (85%)] Loss: 0.080500\n",
      "Train Epoch: 18 [57600/60000 (96%)] Loss: 0.085080\n",
      "Test set: Average loss: 0.0412, Accuracy: 99% (9870/10000)\n",
      "\n",
      "Train Epoch: 19 [6400/60000 (11%)] Loss: 0.011736\n",
      "Train Epoch: 19 [12800/60000 (21%)] Loss: 0.128758\n",
      "Train Epoch: 19 [19200/60000 (32%)] Loss: 0.156266\n",
      "Train Epoch: 19 [25600/60000 (43%)] Loss: 0.063241\n",
      "Train Epoch: 19 [32000/60000 (53%)] Loss: 0.047496\n",
      "Train Epoch: 19 [38400/60000 (64%)] Loss: 0.250015\n",
      "Train Epoch: 19 [44800/60000 (75%)] Loss: 0.120952\n",
      "Train Epoch: 19 [51200/60000 (85%)] Loss: 0.036525\n",
      "Train Epoch: 19 [57600/60000 (96%)] Loss: 0.129542\n",
      "Test set: Average loss: 0.0437, Accuracy: 99% (9870/10000)\n",
      "\n",
      "Train Epoch: 20 [6400/60000 (11%)] Loss: 0.104188\n",
      "Train Epoch: 20 [12800/60000 (21%)] Loss: 0.238953\n",
      "Train Epoch: 20 [19200/60000 (32%)] Loss: 0.134072\n",
      "Train Epoch: 20 [25600/60000 (43%)] Loss: 0.209449\n",
      "Train Epoch: 20 [32000/60000 (53%)] Loss: 0.127300\n",
      "Train Epoch: 20 [38400/60000 (64%)] Loss: 0.165102\n",
      "Train Epoch: 20 [44800/60000 (75%)] Loss: 0.089569\n",
      "Train Epoch: 20 [51200/60000 (85%)] Loss: 0.045309\n",
      "Train Epoch: 20 [57600/60000 (96%)] Loss: 0.140786\n",
      "Test set: Average loss: 0.0412, Accuracy: 99% (9886/10000)\n",
      "\n",
      "Saving the model.\n"
     ]
    }
   ],
   "source": [
    "!python mnist.py --epochs 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training in SageMaker Data Parallel\n",
    "\n",
    "\n",
    "AWS에서 Multigpu distributed training은 data_parallel와 model_parallel 를 모두 사용할 수 있으며, 아래 예제는 data_parallel 중심으로 학습을 하게 됩니다.\n",
    "\n",
    "SageMaker Distributed Data Parallel : AWS의 네트워크 인프라와 Balanced Fusion Buffers 를 이용하여 AWS SageMaker에 최적화된 data parallel 분산학습 알고리즘을 제공합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions=[\n",
    "     {'Name': 'train:Loss', 'Regex': 'Loss: (.*?),'},\n",
    "     {'Name': 'test:Accuracy', 'Regex': 'Accuracy: (.*?)%'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules=[ \n",
    "    Rule.sagemaker(rule_configs.loss_not_decreasing()),\n",
    "    ProfilerRule.sagemaker(rule_configs.ProfilerReport()),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler_config=ProfilerConfig(\n",
    "    system_monitor_interval_millis=500,\n",
    "    framework_profile_params=FrameworkProfile(\n",
    "        start_step=5,num_steps=10,\n",
    "        detailed_profiling_config=DetailedProfilingConfig(start_step=2, num_steps=1),\n",
    "        dataloader_profiling_config=DataloaderProfilingConfig(start_step=3, num_steps=1),\n",
    "        python_profiling_config=PythonProfilingConfig(start_step=4, num_steps=1), # cprofile / Pyinstrument\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = {\"smdistributed\": {\n",
    "                    \"dataparallel\": {\n",
    "                            \"enabled\": True\n",
    "                    }\n",
    "               }\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = 'ml.p3.16xlarge'\n",
    "instance_count = 1\n",
    "entry_point = 'mnist_smdp.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point=entry_point,\n",
    "                    role=role,\n",
    "                    framework_version='1.6.0',\n",
    "                    py_version='py36',\n",
    "                    instance_count=instance_count,\n",
    "                    instance_type=instance_type,\n",
    "                    distribution=distribution,\n",
    "                    metric_definitions=metric_definitions,\n",
    "                    profiler_config=profiler_config,\n",
    "                    rules=rules,\n",
    "                    use_spot_instances=True,\n",
    "                    max_wait=3*60*60,\n",
    "                    max_run=3*60*60,\n",
    "                    hyperparameters={\n",
    "                        'epochs': 50\n",
    "                    }\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our `PyTorch` object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 14:35:12 Starting - Starting the training job...\n",
      "2021-02-04 14:35:37 Starting - Launching requested ML instancesLossNotDecreasing: InProgress\n",
      "ProfilerReport: InProgress\n",
      ".........\n",
      "2021-02-04 14:37:09 Starting - Preparing the instances for training.........\n",
      "2021-02-04 14:38:42 Downloading - Downloading input data\n",
      "2021-02-04 14:38:42 Training - Downloading the training image.................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-02-04 14:41:27,217 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-02-04 14:41:27,296 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\n",
      "2021-02-04 14:41:44 Training - Training image download completed. Training in progress.\u001b[34m2021-02-04 14:41:33,542 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel\u001b[0m\n",
      "\u001b[34m2021-02-04 14:41:33,543 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-02-04 14:41:34,034 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2021-02-04 14:41:34,034 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2021-02-04 14:41:34,037 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2021-02-04 14:41:34,038 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2021-02-04 14:41:34,038 sagemaker-training-toolkit INFO     Host: ['algo-1']\u001b[0m\n",
      "\u001b[34m2021-02-04 14:41:34,116 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": true,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 50\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"training-job-1612449311\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-322537213286/training-job-1612449311/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist_smdp\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist_smdp.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":50}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist_smdp.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist_smdp\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-322537213286/training-job-1612449311/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":50},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"training-job-1612449311\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-322537213286/training-job-1612449311/source/sourcedir.tar.gz\",\"module_name\":\"mnist_smdp\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist_smdp.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"50\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=50\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1 -np 8 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca plm_rsh_num_concurrent 1 -x NCCL_SOCKET_IFNAME=eth0 -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_SINGLENODE=1 -x FI_PROVIDER=sockets -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so smddprun /opt/conda/bin/python -m mpi4py mnist_smdp.py --epochs 50\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Distributed training - True\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Get train data loader\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Distributed training - True\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Get train data loader\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Distributed training - True\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Get train data loader\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Running smdistributed.dataparallel v1.0.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Distributed training - True\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Get train data loader\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Distributed training - True\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Distributed training - True\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Get train data loader\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Get train data loader\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Get test data loader\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Get test data loader\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Processes 7500/60000 (12%) of train data\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Processes 7500/60000 (12%) of train data\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Distributed training - True\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Get test data loader\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Get train data loader\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Processes 7500/60000 (12%) of train data\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Get test data loader\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Processes 7500/60000 (12%) of train data\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Get test data loader\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Get test data loader\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Processes 7500/60000 (12%) of train data\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Processes 7500/60000 (12%) of train data\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Get test data loader\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Processes 7500/60000 (12%) of train data\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Distributed training - True\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Get train data loader\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Get test data loader\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Processes 7500/60000 (12%) of train data\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-02-04 14:41:44.000 algo-1:41 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-02-04 14:41:44.001 algo-1:37 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-02-04 14:41:44.008 algo-1:46 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-02-04 14:41:44.008 algo-1:45 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-02-04 14:41:44.010 algo-1:43 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-02-04 14:41:44.010 algo-1:35 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-02-04 14:41:44.011 algo-1:47 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-02-04 14:41:44.012 algo-1:39 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-02-04 14:41:44.427 algo-1:43 INFO profiler_config_parser.py:102] Using config at /opt/ml/input/config/profilerconfig.json.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-02-04 14:41:44.427 algo-1:37 INFO profiler_config_parser.py:102] Using config at /opt/ml/input/config/profilerconfig.json.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-02-04 14:41:44.427 algo-1:35 INFO profiler_config_parser.py:102] Using config at /opt/ml/input/config/profilerconfig.json.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-02-04 14:41:44.427 algo-1:45 INFO profiler_config_parser.py:102] Using config at /opt/ml/input/config/profilerconfig.json.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-02-04 14:41:44.427 algo-1:41 INFO profiler_config_parser.py:102] Using config at /opt/ml/input/config/profilerconfig.json.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-02-04 14:41:44.427 algo-1:46 INFO profiler_config_parser.py:102] Using config at /opt/ml/input/config/profilerconfig.json.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-02-04 14:41:44.427 algo-1:47 INFO profiler_config_parser.py:102] Using config at /opt/ml/input/config/profilerconfig.json.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-02-04 14:41:44.427 algo-1:39 INFO profiler_config_parser.py:102] Using config at /opt/ml/input/config/profilerconfig.json.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-02-04 14:41:44.428 algo-1:43 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-02-04 14:41:44.428 algo-1:35 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-02-04 14:41:44.428 algo-1:45 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-02-04 14:41:44.428 algo-1:46 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-02-04 14:41:44.428 algo-1:47 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-02-04 14:41:44.428 algo-1:39 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-02-04 14:41:44.428 algo-1:37 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-02-04 14:41:44.428 algo-1:41 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-02-04 14:41:44.431 algo-1:35 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-02-04 14:41:44.431 algo-1:46 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-02-04 14:41:44.431 algo-1:47 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-02-04 14:41:44.431 algo-1:45 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-02-04 14:41:44.431 algo-1:37 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-02-04 14:41:44.431 algo-1:43 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-02-04 14:41:44.431 algo-1:39 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-02-04 14:41:44.431 algo-1:41 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-02-04 14:41:44.622 algo-1:37 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-02-04 14:41:44.621 algo-1:39 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-02-04 14:41:44.622 algo-1:46 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-02-04 14:41:44.622 algo-1:41 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-02-04 14:41:44.622 algo-1:45 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-02-04 14:41:44.622 algo-1:35 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-02-04 14:41:44.622 algo-1:47 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-02-04 14:41:44.622 algo-1:43 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-02-04 14:41:44.622 algo-1:39 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-02-04 14:41:44.622 algo-1:37 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-02-04 14:41:44.622 algo-1:46 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-02-04 14:41:44.622 algo-1:35 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-02-04 14:41:44.622 algo-1:41 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-02-04 14:41:44.622 algo-1:47 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-02-04 14:41:44.622 algo-1:43 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-02-04 14:41:44.622 algo-1:45 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-02-04 14:41:45.001 algo-1:35 INFO hook.py:550] name:module.conv1.weight count_params:250\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-02-04 14:41:45.001 algo-1:43 INFO hook.py:550] name:module.conv1.weight count_params:250\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-02-04 14:41:45.002 algo-1:35 INFO hook.py:550] name:module.conv1.bias count_params:10\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-02-04 14:41:45.002 algo-1:43 INFO hook.py:550] name:module.conv1.bias count_params:10\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-02-04 14:41:45.002 algo-1:35 INFO hook.py:550] name:module.conv2.weight count_params:5000\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-02-04 14:41:45.003 algo-1:35 INFO hook.py:550] name:module.conv2.bias count_params:20\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-02-04 14:41:45.003 algo-1:43 INFO hook.py:550] name:module.conv2.weight count_params:5000\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-02-04 14:41:45.003 algo-1:35 INFO hook.py:550] name:module.fc1.weight count_params:16000\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-02-04 14:41:45.003 algo-1:43 INFO hook.py:550] name:module.conv2.bias count_params:20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-02-04 14:41:45.003 algo-1:35 INFO hook.py:550] name:module.fc1.bias count_params:50\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-02-04 14:41:45.004 algo-1:35 INFO hook.py:550] name:module.fc2.weight count_params:500\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-02-04 14:41:45.004 algo-1:43 INFO hook.py:550] name:module.fc1.weight count_params:16000\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-02-04 14:41:45.004 algo-1:35 INFO hook.py:550] name:module.fc2.bias count_params:10\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-02-04 14:41:45.005 algo-1:35 INFO hook.py:552] Total Trainable Params: 21840\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-02-04 14:41:45.005 algo-1:43 INFO hook.py:550] name:module.fc1.bias count_params:50\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-02-04 14:41:45.005 algo-1:39 INFO hook.py:550] name:module.conv1.weight count_params:250\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-02-04 14:41:45.005 algo-1:41 INFO hook.py:550] name:module.conv1.weight count_params:250\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-02-04 14:41:45.005 algo-1:39 INFO hook.py:550] name:module.conv1.bias count_params:10\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-02-04 14:41:45.006 algo-1:35 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-02-04 14:41:45.005 algo-1:43 INFO hook.py:550] name:module.fc2.weight count_params:500\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-02-04 14:41:45.006 algo-1:39 INFO hook.py:550] name:module.conv2.weight count_params:5000\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-02-04 14:41:45.006 algo-1:41 INFO hook.py:550] name:module.conv1.bias count_params:10\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-02-04 14:41:45.006 algo-1:39 INFO hook.py:550] name:module.conv2.bias count_params:20\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-02-04 14:41:45.006 algo-1:43 INFO hook.py:550] name:module.fc2.bias count_params:10\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-02-04 14:41:45.006 algo-1:41 INFO hook.py:550] name:module.conv2.weight count_params:5000\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-02-04 14:41:45.007 algo-1:39 INFO hook.py:550] name:module.fc1.weight count_params:16000\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-02-04 14:41:45.007 algo-1:41 INFO hook.py:550] name:module.conv2.bias count_params:20\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-02-04 14:41:45.007 algo-1:43 INFO hook.py:552] Total Trainable Params: 21840\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-02-04 14:41:45.007 algo-1:39 INFO hook.py:550] name:module.fc1.bias count_params:50\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-02-04 14:41:45.008 algo-1:41 INFO hook.py:550] name:module.fc1.weight count_params:16000\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-02-04 14:41:45.008 algo-1:39 INFO hook.py:550] name:module.fc2.weight count_params:500\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-02-04 14:41:45.008 algo-1:45 INFO hook.py:550] name:module.conv1.weight count_params:250\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-02-04 14:41:45.008 algo-1:41 INFO hook.py:550] name:module.fc1.bias count_params:50\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-02-04 14:41:45.008 algo-1:46 INFO hook.py:550] name:module.conv1.weight count_params:250\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-02-04 14:41:45.008 algo-1:39 INFO hook.py:550] name:module.fc2.bias count_params:10\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-02-04 14:41:45.008 algo-1:47 INFO hook.py:550] name:module.conv1.weight count_params:250\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-02-04 14:41:45.008 algo-1:45 INFO hook.py:550] name:module.conv1.bias count_params:10\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-02-04 14:41:45.008 algo-1:43 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-02-04 14:41:45.009 algo-1:39 INFO hook.py:552] Total Trainable Params: 21840\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-02-04 14:41:45.009 algo-1:41 INFO hook.py:550] name:module.fc2.weight count_params:500\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-02-04 14:41:45.009 algo-1:45 INFO hook.py:550] name:module.conv2.weight count_params:5000\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-02-04 14:41:45.009 algo-1:46 INFO hook.py:550] name:module.conv1.bias count_params:10\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-02-04 14:41:45.009 algo-1:47 INFO hook.py:550] name:module.conv1.bias count_params:10\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-02-04 14:41:45.009 algo-1:41 INFO hook.py:550] name:module.fc2.bias count_params:10\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-02-04 14:41:45.009 algo-1:45 INFO hook.py:550] name:module.conv2.bias count_params:20\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-02-04 14:41:45.010 algo-1:47 INFO hook.py:550] name:module.conv2.weight count_params:5000\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-02-04 14:41:45.010 algo-1:41 INFO hook.py:552] Total Trainable Params: 21840\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-02-04 14:41:45.010 algo-1:46 INFO hook.py:550] name:module.conv2.weight count_params:5000\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-02-04 14:41:45.010 algo-1:45 INFO hook.py:550] name:module.fc1.weight count_params:16000\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-02-04 14:41:45.010 algo-1:39 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-02-04 14:41:45.011 algo-1:45 INFO hook.py:550] name:module.fc1.bias count_params:50\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-02-04 14:41:45.011 algo-1:47 INFO hook.py:550] name:module.conv2.bias count_params:20\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-02-04 14:41:45.011 algo-1:46 INFO hook.py:550] name:module.conv2.bias count_params:20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-02-04 14:41:45.011 algo-1:35 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/0-algo-1/prestepzero-*-start-1612449704428186.2_global-0-stepstart-1612449705008412.0/python_stats.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-02-04 14:41:45.011 algo-1:41 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-02-04 14:41:45.010 algo-1:37 INFO hook.py:550] name:module.conv1.weight count_params:250\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-02-04 14:41:45.011 algo-1:45 INFO hook.py:550] name:module.fc2.weight count_params:500\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-02-04 14:41:45.011 algo-1:46 INFO hook.py:550] name:module.fc1.weight count_params:16000\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-02-04 14:41:45.012 algo-1:47 INFO hook.py:550] name:module.fc1.weight count_params:16000\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-02-04 14:41:45.012 algo-1:45 INFO hook.py:550] name:module.fc2.bias count_params:10\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-02-04 14:41:45.012 algo-1:37 INFO hook.py:550] name:module.conv1.bias count_params:10\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-02-04 14:41:45.012 algo-1:45 INFO hook.py:552] Total Trainable Params: 21840\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-02-04 14:41:45.012 algo-1:46 INFO hook.py:550] name:module.fc1.bias count_params:50\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-02-04 14:41:45.012 algo-1:47 INFO hook.py:550] name:module.fc1.bias count_params:50\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-02-04 14:41:45.013 algo-1:37 INFO hook.py:550] name:module.conv2.weight count_params:5000\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-02-04 14:41:45.013 algo-1:46 INFO hook.py:550] name:module.fc2.weight count_params:500\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-02-04 14:41:45.013 algo-1:47 INFO hook.py:550] name:module.fc2.weight count_params:500\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-02-04 14:41:45.013 algo-1:45 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-02-04 14:41:45.013 algo-1:37 INFO hook.py:550] name:module.conv2.bias count_params:20\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-02-04 14:41:45.014 algo-1:47 INFO hook.py:550] name:module.fc2.bias count_params:10\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-02-04 14:41:45.014 algo-1:46 INFO hook.py:550] name:module.fc2.bias count_params:10\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-02-04 14:41:45.014 algo-1:37 INFO hook.py:550] name:module.fc1.weight count_params:16000\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-02-04 14:41:45.015 algo-1:47 INFO hook.py:552] Total Trainable Params: 21840\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-02-04 14:41:45.015 algo-1:46 INFO hook.py:552] Total Trainable Params: 21840\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-02-04 14:41:45.015 algo-1:37 INFO hook.py:550] name:module.fc1.bias count_params:50\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-02-04 14:41:45.016 algo-1:37 INFO hook.py:550] name:module.fc2.weight count_params:500\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-02-04 14:41:45.016 algo-1:47 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-02-04 14:41:45.016 algo-1:46 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-02-04 14:41:45.016 algo-1:37 INFO hook.py:550] name:module.fc2.bias count_params:10\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-02-04 14:41:45.017 algo-1:39 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/2-algo-1/prestepzero-*-start-1612449704428197.2_global-0-stepstart-1612449705012983.5/python_stats.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-02-04 14:41:45.017 algo-1:37 INFO hook.py:552] Total Trainable Params: 21840\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-02-04 14:41:45.018 algo-1:43 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/4-algo-1/prestepzero-*-start-1612449704428187.2_global-0-stepstart-1612449705012994.2/python_stats.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-02-04 14:41:45.019 algo-1:41 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/3-algo-1/prestepzero-*-start-1612449704428190.2_global-0-stepstart-1612449705014674.5/python_stats.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-02-04 14:41:45.019 algo-1:37 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-02-04 14:41:45.021 algo-1:45 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/5-algo-1/prestepzero-*-start-1612449704428189.5_global-0-stepstart-1612449705016528.5/python_stats.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-02-04 14:41:45.025 algo-1:46 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/6-algo-1/prestepzero-*-start-1612449704428202.2_global-0-stepstart-1612449705020006.0/python_stats.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-02-04 14:41:45.026 algo-1:47 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/7-algo-1/prestepzero-*-start-1612449704428254.5_global-0-stepstart-1612449705019814.8/python_stats.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-02-04 14:41:45.028 algo-1:37 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/1-algo-1/prestepzero-*-start-1612449704428228.8_global-0-stepstart-1612449705022473.8/python_stats.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-02-04 14:42:10.849 algo-1:35 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/0-algo-1/global-5-stepstart-1612449712561168.2_global-5-forwardpassend-1612449730847894.0/python_stats.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-02-04 14:42:10.978 algo-1:37 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/1-algo-1/global-5-stepstart-1612449712563308.2_global-5-forwardpassend-1612449730976974.5/python_stats.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-02-04 14:42:11.013 algo-1:45 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/5-algo-1/global-5-stepstart-1612449712561233.8_global-5-forwardpassend-1612449731012359.8/python_stats.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-02-04 14:42:11.368 algo-1:39 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/2-algo-1/global-5-stepstart-1612449712561025.5_global-5-forwardpassend-1612449731366986.8/python_stats.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-02-04 14:42:12.254 algo-1:41 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/3-algo-1/global-5-stepstart-1612449712562290.2_global-5-forwardpassend-1612449732253406.5/python_stats.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-02-04 14:42:12.747 algo-1:47 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/7-algo-1/global-5-stepstart-1612449712561190.2_global-5-forwardpassend-1612449732746433.0/python_stats.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-02-04 14:42:12.971 algo-1:46 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/6-algo-1/global-5-stepstart-1612449712562600.2_global-5-forwardpassend-1612449732970490.2/python_stats.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-02-04 14:42:13.059 algo-1:43 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/4-algo-1/global-5-stepstart-1612449712562275.8_global-5-forwardpassend-1612449733059044.2/python_stats.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-02-04 14:42:13.086 algo-1:45 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/5-algo-1/global-5-forwardpassend-1612449731015598.2_global-6-stepstart-1612449733084155.0/python_stats.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-02-04 14:42:13.086 algo-1:43 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/4-algo-1/global-5-forwardpassend-1612449733061875.8_global-6-stepstart-1612449733084223.8/python_stats.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-02-04 14:42:13.086 algo-1:35 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/0-algo-1/global-5-forwardpassend-1612449730851211.0_global-6-stepstart-1612449733084519.2/python_stats.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-02-04 14:42:13.086 algo-1:46 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/6-algo-1/global-5-forwardpassend-1612449732973358.8_global-6-stepstart-1612449733084134.0/python_stats.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-02-04 14:42:13.086 algo-1:39 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/2-algo-1/global-5-forwardpassend-1612449731370958.5_global-6-stepstart-1612449733084617.5/python_stats.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-02-04 14:42:13.086 algo-1:41 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/3-algo-1/global-5-forwardpassend-1612449732256746.2_global-6-stepstart-1612449733084479.2/python_stats.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-02-04 14:42:13.088 algo-1:47 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/7-algo-1/global-5-forwardpassend-1612449732749636.0_global-6-stepstart-1612449733086986.8/python_stats.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-02-04 14:42:13.091 algo-1:37 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/1-algo-1/global-5-forwardpassend-1612449730980290.0_global-6-stepstart-1612449733090008.0/python_stats.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 1 [6400/7500 (85%)], Loss: 2.039122,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 1 [6400/7500 (85%)], Loss: 1.937220,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [6400/7500 (85%)], Loss: 1.903584,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 1 [6400/7500 (85%)], Loss: 2.000551,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 1 [6400/7500 (85%)], Loss: 1.962627,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 1 [6400/7500 (85%)], Loss: 1.833978,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 1 [6400/7500 (85%)], Loss: 1.976260,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 1 [6400/7500 (85%)], Loss: 1.907692,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 1.5936, Accuracy: 61% (6052/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 1.5936, Accuracy: 61% (6052/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 1.5936, Accuracy: 61% (6052/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 1.5936, Accuracy: 61% (6052/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 1.5936, Accuracy: 61% (6052/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 1.5936, Accuracy: 61% (6052/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 1.5936, Accuracy: 61% (6052/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 1.5936, Accuracy: 61% (6052/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 2 [6400/7500 (85%)], Loss: 0.889906,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 2 [6400/7500 (85%)], Loss: 0.848255,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 2 [6400/7500 (85%)], Loss: 1.029645,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 2 [6400/7500 (85%)], Loss: 1.155627,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 2 [6400/7500 (85%)], Loss: 0.966535,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 2 [6400/7500 (85%)], Loss: 0.983148,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 2 [6400/7500 (85%)], Loss: 0.969325,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 2 [6400/7500 (85%)], Loss: 1.074617,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.5766, Accuracy: 85% (8533/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.5766, Accuracy: 85% (8533/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.5766, Accuracy: 85% (8533/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.5766, Accuracy: 85% (8533/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.5766, Accuracy: 85% (8533/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.5766, Accuracy: 85% (8533/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.5766, Accuracy: 85% (8533/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.5766, Accuracy: 85% (8533/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 3 [6400/7500 (85%)], Loss: 0.613692,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 3 [6400/7500 (85%)], Loss: 0.837238,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 3 [6400/7500 (85%)], Loss: 0.770405,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 3 [6400/7500 (85%)], Loss: 1.018595,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 3 [6400/7500 (85%)], Loss: 0.754835,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 3 [6400/7500 (85%)], Loss: 0.785177,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 3 [6400/7500 (85%)], Loss: 0.623159,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 3 [6400/7500 (85%)], Loss: 0.439544,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.4019, Accuracy: 89% (8923/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.4019, Accuracy: 89% (8923/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.4019, Accuracy: 89% (8923/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.4019, Accuracy: 89% (8923/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.4019, Accuracy: 89% (8923/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.4019, Accuracy: 89% (8923/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.4019, Accuracy: 89% (8923/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.4019, Accuracy: 89% (8923/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 4 [6400/7500 (85%)], Loss: 0.417510,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 4 [6400/7500 (85%)], Loss: 0.841170,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 4 [6400/7500 (85%)], Loss: 0.628861,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 4 [6400/7500 (85%)], Loss: 0.567102,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 4 [6400/7500 (85%)], Loss: 0.710980,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 4 [6400/7500 (85%)], Loss: 0.625195,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 4 [6400/7500 (85%)], Loss: 0.691051,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 4 [6400/7500 (85%)], Loss: 0.765467,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.3140, Accuracy: 91% (9135/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.3140, Accuracy: 91% (9135/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.3140, Accuracy: 91% (9135/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.3140, Accuracy: 91% (9135/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.3140, Accuracy: 91% (9135/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.3140, Accuracy: 91% (9135/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.3140, Accuracy: 91% (9135/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.3140, Accuracy: 91% (9135/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 5 [6400/7500 (85%)], Loss: 0.322811,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 5 [6400/7500 (85%)], Loss: 0.478022,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 5 [6400/7500 (85%)], Loss: 0.624151,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 5 [6400/7500 (85%)], Loss: 0.598461,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 5 [6400/7500 (85%)], Loss: 0.552300,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 5 [6400/7500 (85%)], Loss: 0.433276,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 5 [6400/7500 (85%)], Loss: 0.582738,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 5 [6400/7500 (85%)], Loss: 0.715893,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.2690, Accuracy: 92% (9229/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.2690, Accuracy: 92% (9229/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.2690, Accuracy: 92% (9229/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.2690, Accuracy: 92% (9229/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.2690, Accuracy: 92% (9229/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.2690, Accuracy: 92% (9229/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.2690, Accuracy: 92% (9229/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.2690, Accuracy: 92% (9229/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 6 [6400/7500 (85%)], Loss: 0.398360,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 6 [6400/7500 (85%)], Loss: 0.392116,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 6 [6400/7500 (85%)], Loss: 0.485909,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 6 [6400/7500 (85%)], Loss: 0.466512,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 6 [6400/7500 (85%)], Loss: 0.496269,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 6 [6400/7500 (85%)], Loss: 0.606178,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 6 [6400/7500 (85%)], Loss: 0.511172,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 6 [6400/7500 (85%)], Loss: 0.619746,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.2356, Accuracy: 93% (9302/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.2356, Accuracy: 93% (9302/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.2356, Accuracy: 93% (9302/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.2356, Accuracy: 93% (9302/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.2356, Accuracy: 93% (9302/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.2356, Accuracy: 93% (9302/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.2356, Accuracy: 93% (9302/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.2356, Accuracy: 93% (9302/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 7 [6400/7500 (85%)], Loss: 0.306830,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 7 [6400/7500 (85%)], Loss: 0.371284,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 7 [6400/7500 (85%)], Loss: 0.659523,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 7 [6400/7500 (85%)], Loss: 0.564826,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 7 [6400/7500 (85%)], Loss: 0.512547,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 7 [6400/7500 (85%)], Loss: 0.489648,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 7 [6400/7500 (85%)], Loss: 0.268358,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 7 [6400/7500 (85%)], Loss: 0.580881,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.2093, Accuracy: 94% (9379/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.2093, Accuracy: 94% (9379/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.2093, Accuracy: 94% (9379/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.2093, Accuracy: 94% (9379/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.2093, Accuracy: 94% (9379/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.2093, Accuracy: 94% (9379/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.2093, Accuracy: 94% (9379/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.2093, Accuracy: 94% (9379/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 8 [6400/7500 (85%)], Loss: 0.438595,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 8 [6400/7500 (85%)], Loss: 0.585312,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 8 [6400/7500 (85%)], Loss: 0.365954,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 8 [6400/7500 (85%)], Loss: 0.236647,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 8 [6400/7500 (85%)], Loss: 0.396442,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 8 [6400/7500 (85%)], Loss: 0.353629,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 8 [6400/7500 (85%)], Loss: 0.408938,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 8 [6400/7500 (85%)], Loss: 0.535200,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.1925, Accuracy: 94% (9435/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.1925, Accuracy: 94% (9435/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.1925, Accuracy: 94% (9435/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.1925, Accuracy: 94% (9435/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.1925, Accuracy: 94% (9435/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.1925, Accuracy: 94% (9435/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.1925, Accuracy: 94% (9435/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.1925, Accuracy: 94% (9435/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 9 [6400/7500 (85%)], Loss: 0.235385,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 9 [6400/7500 (85%)], Loss: 0.272139,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 9 [6400/7500 (85%)], Loss: 0.473126,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 9 [6400/7500 (85%)], Loss: 0.567992,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 9 [6400/7500 (85%)], Loss: 0.417317,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 9 [6400/7500 (85%)], Loss: 0.467510,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 9 [6400/7500 (85%)], Loss: 0.525224,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 9 [6400/7500 (85%)], Loss: 0.392755,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.1776, Accuracy: 95% (9458/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.1776, Accuracy: 95% (9458/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.1776, Accuracy: 95% (9458/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.1776, Accuracy: 95% (9458/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.1776, Accuracy: 95% (9458/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.1776, Accuracy: 95% (9458/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.1776, Accuracy: 95% (9458/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.1776, Accuracy: 95% (9458/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 10 [6400/7500 (85%)], Loss: 0.297844,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 10 [6400/7500 (85%)], Loss: 0.161063,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 10 [6400/7500 (85%)], Loss: 0.308844,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 10 [6400/7500 (85%)], Loss: 0.403168,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 10 [6400/7500 (85%)], Loss: 0.352178,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 10 [6400/7500 (85%)], Loss: 0.522263,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 10 [6400/7500 (85%)], Loss: 0.236347,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 10 [6400/7500 (85%)], Loss: 0.378090,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.1645, Accuracy: 95% (9495/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.1645, Accuracy: 95% (9495/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.1645, Accuracy: 95% (9495/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.1645, Accuracy: 95% (9495/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.1645, Accuracy: 95% (9495/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.1645, Accuracy: 95% (9495/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.1645, Accuracy: 95% (9495/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.1645, Accuracy: 95% (9495/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 11 [6400/7500 (85%)], Loss: 0.278551,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 11 [6400/7500 (85%)], Loss: 0.452474,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 11 [6400/7500 (85%)], Loss: 0.405510,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 11 [6400/7500 (85%)], Loss: 0.227708,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 11 [6400/7500 (85%)], Loss: 0.551520,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 11 [6400/7500 (85%)], Loss: 0.363979,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 11 [6400/7500 (85%)], Loss: 0.387668,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 11 [6400/7500 (85%)], Loss: 0.184609,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.1535, Accuracy: 95% (9524/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.1535, Accuracy: 95% (9524/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.1535, Accuracy: 95% (9524/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.1535, Accuracy: 95% (9524/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.1535, Accuracy: 95% (9524/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.1535, Accuracy: 95% (9524/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.1535, Accuracy: 95% (9524/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.1535, Accuracy: 95% (9524/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 12 [6400/7500 (85%)], Loss: 0.235509,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 12 [6400/7500 (85%)], Loss: 0.315393,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 12 [6400/7500 (85%)], Loss: 0.397290,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 12 [6400/7500 (85%)], Loss: 0.385782,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 12 [6400/7500 (85%)], Loss: 0.545321,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 12 [6400/7500 (85%)], Loss: 0.286278,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 12 [6400/7500 (85%)], Loss: 0.267331,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 12 [6400/7500 (85%)], Loss: 0.205561,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.1444, Accuracy: 96% (9552/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.1444, Accuracy: 96% (9552/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.1444, Accuracy: 96% (9552/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.1444, Accuracy: 96% (9552/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.1444, Accuracy: 96% (9552/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.1444, Accuracy: 96% (9552/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.1444, Accuracy: 96% (9552/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.1444, Accuracy: 96% (9552/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 13 [6400/7500 (85%)], Loss: 0.543975,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 13 [6400/7500 (85%)], Loss: 0.311293,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 13 [6400/7500 (85%)], Loss: 0.143994,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 13 [6400/7500 (85%)], Loss: 0.238856,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 13 [6400/7500 (85%)], Loss: 0.427225,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 13 [6400/7500 (85%)], Loss: 0.217050,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 13 [6400/7500 (85%)], Loss: 0.322240,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 13 [6400/7500 (85%)], Loss: 0.333603,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.1375, Accuracy: 96% (9570/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.1375, Accuracy: 96% (9570/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.1375, Accuracy: 96% (9570/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.1375, Accuracy: 96% (9570/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.1375, Accuracy: 96% (9570/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.1375, Accuracy: 96% (9570/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.1375, Accuracy: 96% (9570/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.1375, Accuracy: 96% (9570/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 14 [6400/7500 (85%)], Loss: 0.218772,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 14 [6400/7500 (85%)], Loss: 0.420437,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 14 [6400/7500 (85%)], Loss: 0.388806,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 14 [6400/7500 (85%)], Loss: 0.534038,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 14 [6400/7500 (85%)], Loss: 0.418357,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 14 [6400/7500 (85%)], Loss: 0.365500,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 14 [6400/7500 (85%)], Loss: 0.198366,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 14 [6400/7500 (85%)], Loss: 0.366336,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.1291, Accuracy: 96% (9596/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.1291, Accuracy: 96% (9596/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.1291, Accuracy: 96% (9596/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.1291, Accuracy: 96% (9596/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.1291, Accuracy: 96% (9596/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.1291, Accuracy: 96% (9596/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.1291, Accuracy: 96% (9596/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.1291, Accuracy: 96% (9596/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 15 [6400/7500 (85%)], Loss: 0.323277,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 15 [6400/7500 (85%)], Loss: 0.251022,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 15 [6400/7500 (85%)], Loss: 0.212399,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 15 [6400/7500 (85%)], Loss: 0.253264,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 15 [6400/7500 (85%)], Loss: 0.380926,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 15 [6400/7500 (85%)], Loss: 0.235139,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 15 [6400/7500 (85%)], Loss: 0.172352,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 15 [6400/7500 (85%)], Loss: 0.288806,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.1252, Accuracy: 96% (9611/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.1252, Accuracy: 96% (9611/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.1252, Accuracy: 96% (9611/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.1252, Accuracy: 96% (9611/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.1252, Accuracy: 96% (9611/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.1252, Accuracy: 96% (9611/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.1252, Accuracy: 96% (9611/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.1252, Accuracy: 96% (9611/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 16 [6400/7500 (85%)], Loss: 0.196549,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 16 [6400/7500 (85%)], Loss: 0.232979,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 16 [6400/7500 (85%)], Loss: 0.332575,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 16 [6400/7500 (85%)], Loss: 0.248120,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 16 [6400/7500 (85%)], Loss: 0.343417,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 16 [6400/7500 (85%)], Loss: 0.395390,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 16 [6400/7500 (85%)], Loss: 0.263780,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 16 [6400/7500 (85%)], Loss: 0.149052,\u001b[0m\n",
      "LossNotDecreasing: IssuesFound\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.1199, Accuracy: 96% (9630/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.1199, Accuracy: 96% (9630/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.1199, Accuracy: 96% (9630/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.1199, Accuracy: 96% (9630/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.1199, Accuracy: 96% (9630/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.1199, Accuracy: 96% (9630/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.1199, Accuracy: 96% (9630/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.1199, Accuracy: 96% (9630/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 17 [6400/7500 (85%)], Loss: 0.132256,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 17 [6400/7500 (85%)], Loss: 0.188138,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 17 [6400/7500 (85%)], Loss: 0.315701,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 17 [6400/7500 (85%)], Loss: 0.316890,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 17 [6400/7500 (85%)], Loss: 0.212504,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 17 [6400/7500 (85%)], Loss: 0.374188,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 17 [6400/7500 (85%)], Loss: 0.337954,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 17 [6400/7500 (85%)], Loss: 0.164485,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.1148, Accuracy: 96% (9641/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.1148, Accuracy: 96% (9641/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.1148, Accuracy: 96% (9641/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.1148, Accuracy: 96% (9641/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.1148, Accuracy: 96% (9641/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.1148, Accuracy: 96% (9641/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.1148, Accuracy: 96% (9641/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.1148, Accuracy: 96% (9641/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 18 [6400/7500 (85%)], Loss: 0.199781,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 18 [6400/7500 (85%)], Loss: 0.291861,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 18 [6400/7500 (85%)], Loss: 0.413266,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 18 [6400/7500 (85%)], Loss: 0.262899,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 18 [6400/7500 (85%)], Loss: 0.152639,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 18 [6400/7500 (85%)], Loss: 0.224007,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 18 [6400/7500 (85%)], Loss: 0.221324,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 18 [6400/7500 (85%)], Loss: 0.234233,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.1101, Accuracy: 97% (9652/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.1101, Accuracy: 97% (9652/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.1101, Accuracy: 97% (9652/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.1101, Accuracy: 97% (9652/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.1101, Accuracy: 97% (9652/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.1101, Accuracy: 97% (9652/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.1101, Accuracy: 97% (9652/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.1101, Accuracy: 97% (9652/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 19 [6400/7500 (85%)], Loss: 0.278678,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 19 [6400/7500 (85%)], Loss: 0.103457,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 19 [6400/7500 (85%)], Loss: 0.303444,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 19 [6400/7500 (85%)], Loss: 0.086625,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 19 [6400/7500 (85%)], Loss: 0.172200,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 19 [6400/7500 (85%)], Loss: 0.292658,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 19 [6400/7500 (85%)], Loss: 0.173194,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 19 [6400/7500 (85%)], Loss: 0.193962,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.1069, Accuracy: 97% (9664/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.1069, Accuracy: 97% (9664/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.1069, Accuracy: 97% (9664/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.1069, Accuracy: 97% (9664/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.1069, Accuracy: 97% (9664/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.1069, Accuracy: 97% (9664/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.1069, Accuracy: 97% (9664/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.1069, Accuracy: 97% (9664/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 20 [6400/7500 (85%)], Loss: 0.351843,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 20 [6400/7500 (85%)], Loss: 0.239208,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 20 [6400/7500 (85%)], Loss: 0.226938,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 20 [6400/7500 (85%)], Loss: 0.150440,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 20 [6400/7500 (85%)], Loss: 0.278698,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 20 [6400/7500 (85%)], Loss: 0.143084,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 20 [6400/7500 (85%)], Loss: 0.116436,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 20 [6400/7500 (85%)], Loss: 0.168399,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.1040, Accuracy: 97% (9666/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.1040, Accuracy: 97% (9666/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.1040, Accuracy: 97% (9666/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.1040, Accuracy: 97% (9666/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.1040, Accuracy: 97% (9666/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.1040, Accuracy: 97% (9666/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.1040, Accuracy: 97% (9666/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.1040, Accuracy: 97% (9666/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 21 [6400/7500 (85%)], Loss: 0.178569,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 21 [6400/7500 (85%)], Loss: 0.571865,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 21 [6400/7500 (85%)], Loss: 0.251325,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 21 [6400/7500 (85%)], Loss: 0.076691,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 21 [6400/7500 (85%)], Loss: 0.218308,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 21 [6400/7500 (85%)], Loss: 0.216770,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 21 [6400/7500 (85%)], Loss: 0.111926,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 21 [6400/7500 (85%)], Loss: 0.186303,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.1003, Accuracy: 97% (9679/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.1003, Accuracy: 97% (9679/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.1003, Accuracy: 97% (9679/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.1003, Accuracy: 97% (9679/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.1003, Accuracy: 97% (9679/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.1003, Accuracy: 97% (9679/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.1003, Accuracy: 97% (9679/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.1003, Accuracy: 97% (9679/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 22 [6400/7500 (85%)], Loss: 0.196956,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 22 [6400/7500 (85%)], Loss: 0.221383,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 22 [6400/7500 (85%)], Loss: 0.311865,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 22 [6400/7500 (85%)], Loss: 0.186855,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 22 [6400/7500 (85%)], Loss: 0.192455,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 22 [6400/7500 (85%)], Loss: 0.346426,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 22 [6400/7500 (85%)], Loss: 0.446132,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 22 [6400/7500 (85%)], Loss: 0.151477,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0978, Accuracy: 97% (9687/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0978, Accuracy: 97% (9687/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0978, Accuracy: 97% (9687/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0978, Accuracy: 97% (9687/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0978, Accuracy: 97% (9687/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0978, Accuracy: 97% (9687/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0978, Accuracy: 97% (9687/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0978, Accuracy: 97% (9687/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 23 [6400/7500 (85%)], Loss: 0.219875,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 23 [6400/7500 (85%)], Loss: 0.342351,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 23 [6400/7500 (85%)], Loss: 0.084064,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 23 [6400/7500 (85%)], Loss: 0.226650,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 23 [6400/7500 (85%)], Loss: 0.134611,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 23 [6400/7500 (85%)], Loss: 0.171246,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 23 [6400/7500 (85%)], Loss: 0.294375,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 23 [6400/7500 (85%)], Loss: 0.182080,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0950, Accuracy: 97% (9684/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0950, Accuracy: 97% (9684/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0950, Accuracy: 97% (9684/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0950, Accuracy: 97% (9684/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0950, Accuracy: 97% (9684/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0950, Accuracy: 97% (9684/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0950, Accuracy: 97% (9684/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0950, Accuracy: 97% (9684/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 24 [6400/7500 (85%)], Loss: 0.174904,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 24 [6400/7500 (85%)], Loss: 0.359346,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 24 [6400/7500 (85%)], Loss: 0.404688,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 24 [6400/7500 (85%)], Loss: 0.152194,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 24 [6400/7500 (85%)], Loss: 0.161168,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 24 [6400/7500 (85%)], Loss: 0.224549,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 24 [6400/7500 (85%)], Loss: 0.244237,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 24 [6400/7500 (85%)], Loss: 0.212931,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0928, Accuracy: 97% (9697/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0928, Accuracy: 97% (9697/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0928, Accuracy: 97% (9697/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0928, Accuracy: 97% (9697/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0928, Accuracy: 97% (9697/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0928, Accuracy: 97% (9697/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0928, Accuracy: 97% (9697/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0928, Accuracy: 97% (9697/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 25 [6400/7500 (85%)], Loss: 0.250835,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 25 [6400/7500 (85%)], Loss: 0.290157,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 25 [6400/7500 (85%)], Loss: 0.217160,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 25 [6400/7500 (85%)], Loss: 0.346833,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 25 [6400/7500 (85%)], Loss: 0.271926,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 25 [6400/7500 (85%)], Loss: 0.153836,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 25 [6400/7500 (85%)], Loss: 0.153847,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 25 [6400/7500 (85%)], Loss: 0.118025,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0889, Accuracy: 97% (9701/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0889, Accuracy: 97% (9701/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0889, Accuracy: 97% (9701/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0889, Accuracy: 97% (9701/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0889, Accuracy: 97% (9701/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0889, Accuracy: 97% (9701/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0889, Accuracy: 97% (9701/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0889, Accuracy: 97% (9701/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 26 [6400/7500 (85%)], Loss: 0.140909,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 26 [6400/7500 (85%)], Loss: 0.164491,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 26 [6400/7500 (85%)], Loss: 0.215113,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 26 [6400/7500 (85%)], Loss: 0.247163,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 26 [6400/7500 (85%)], Loss: 0.128383,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 26 [6400/7500 (85%)], Loss: 0.266938,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 26 [6400/7500 (85%)], Loss: 0.308419,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 26 [6400/7500 (85%)], Loss: 0.211193,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0881, Accuracy: 97% (9715/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0881, Accuracy: 97% (9715/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0881, Accuracy: 97% (9715/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0881, Accuracy: 97% (9715/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0881, Accuracy: 97% (9715/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0881, Accuracy: 97% (9715/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0881, Accuracy: 97% (9715/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0881, Accuracy: 97% (9715/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 27 [6400/7500 (85%)], Loss: 0.103973,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 27 [6400/7500 (85%)], Loss: 0.284478,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 27 [6400/7500 (85%)], Loss: 0.258273,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 27 [6400/7500 (85%)], Loss: 0.182337,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 27 [6400/7500 (85%)], Loss: 0.436370,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 27 [6400/7500 (85%)], Loss: 0.140051,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 27 [6400/7500 (85%)], Loss: 0.146327,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 27 [6400/7500 (85%)], Loss: 0.128545,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0850, Accuracy: 97% (9723/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0850, Accuracy: 97% (9723/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0850, Accuracy: 97% (9723/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0850, Accuracy: 97% (9723/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0850, Accuracy: 97% (9723/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0850, Accuracy: 97% (9723/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0850, Accuracy: 97% (9723/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0850, Accuracy: 97% (9723/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 28 [6400/7500 (85%)], Loss: 0.192767,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 28 [6400/7500 (85%)], Loss: 0.171796,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 28 [6400/7500 (85%)], Loss: 0.235417,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 28 [6400/7500 (85%)], Loss: 0.186967,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 28 [6400/7500 (85%)], Loss: 0.300715,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 28 [6400/7500 (85%)], Loss: 0.224003,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 28 [6400/7500 (85%)], Loss: 0.294621,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 28 [6400/7500 (85%)], Loss: 0.133353,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0832, Accuracy: 97% (9723/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0832, Accuracy: 97% (9723/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0832, Accuracy: 97% (9723/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0832, Accuracy: 97% (9723/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0832, Accuracy: 97% (9723/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0832, Accuracy: 97% (9723/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0832, Accuracy: 97% (9723/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0832, Accuracy: 97% (9723/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 29 [6400/7500 (85%)], Loss: 0.096876,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 29 [6400/7500 (85%)], Loss: 0.171070,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 29 [6400/7500 (85%)], Loss: 0.111888,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 29 [6400/7500 (85%)], Loss: 0.346855,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 29 [6400/7500 (85%)], Loss: 0.126370,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 29 [6400/7500 (85%)], Loss: 0.213332,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 29 [6400/7500 (85%)], Loss: 0.196099,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 29 [6400/7500 (85%)], Loss: 0.169854,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0813, Accuracy: 97% (9737/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0813, Accuracy: 97% (9737/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0813, Accuracy: 97% (9737/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0813, Accuracy: 97% (9737/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0813, Accuracy: 97% (9737/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0813, Accuracy: 97% (9737/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0813, Accuracy: 97% (9737/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0813, Accuracy: 97% (9737/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 30 [6400/7500 (85%)], Loss: 0.160498,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 30 [6400/7500 (85%)], Loss: 0.408353,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 30 [6400/7500 (85%)], Loss: 0.460860,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 30 [6400/7500 (85%)], Loss: 0.109485,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 30 [6400/7500 (85%)], Loss: 0.257817,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 30 [6400/7500 (85%)], Loss: 0.257405,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 30 [6400/7500 (85%)], Loss: 0.197252,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 30 [6400/7500 (85%)], Loss: 0.129824,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0813, Accuracy: 97% (9739/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0813, Accuracy: 97% (9739/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0813, Accuracy: 97% (9739/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0813, Accuracy: 97% (9739/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0813, Accuracy: 97% (9739/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0813, Accuracy: 97% (9739/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0813, Accuracy: 97% (9739/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0813, Accuracy: 97% (9739/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 31 [6400/7500 (85%)], Loss: 0.140033,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 31 [6400/7500 (85%)], Loss: 0.193979,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 31 [6400/7500 (85%)], Loss: 0.192816,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 31 [6400/7500 (85%)], Loss: 0.186119,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 31 [6400/7500 (85%)], Loss: 0.283922,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 31 [6400/7500 (85%)], Loss: 0.081981,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 31 [6400/7500 (85%)], Loss: 0.119982,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 31 [6400/7500 (85%)], Loss: 0.179794,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0786, Accuracy: 97% (9745/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0786, Accuracy: 97% (9745/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0786, Accuracy: 97% (9745/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0786, Accuracy: 97% (9745/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0786, Accuracy: 97% (9745/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0786, Accuracy: 97% (9745/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0786, Accuracy: 97% (9745/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0786, Accuracy: 97% (9745/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 32 [6400/7500 (85%)], Loss: 0.229164,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 32 [6400/7500 (85%)], Loss: 0.216571,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 32 [6400/7500 (85%)], Loss: 0.089466,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 32 [6400/7500 (85%)], Loss: 0.146107,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 32 [6400/7500 (85%)], Loss: 0.192109,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 32 [6400/7500 (85%)], Loss: 0.367804,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 32 [6400/7500 (85%)], Loss: 0.186537,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 32 [6400/7500 (85%)], Loss: 0.348698,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0773, Accuracy: 98% (9760/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0773, Accuracy: 98% (9760/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0773, Accuracy: 98% (9760/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0773, Accuracy: 98% (9760/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0773, Accuracy: 98% (9760/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0773, Accuracy: 98% (9760/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0773, Accuracy: 98% (9760/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0773, Accuracy: 98% (9760/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 33 [6400/7500 (85%)], Loss: 0.167502,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 33 [6400/7500 (85%)], Loss: 0.123347,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 33 [6400/7500 (85%)], Loss: 0.349294,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 33 [6400/7500 (85%)], Loss: 0.089418,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 33 [6400/7500 (85%)], Loss: 0.187566,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 33 [6400/7500 (85%)], Loss: 0.145700,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 33 [6400/7500 (85%)], Loss: 0.031561,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 33 [6400/7500 (85%)], Loss: 0.173473,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0763, Accuracy: 97% (9746/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0763, Accuracy: 97% (9746/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0763, Accuracy: 97% (9746/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0763, Accuracy: 97% (9746/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0763, Accuracy: 97% (9746/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0763, Accuracy: 97% (9746/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0763, Accuracy: 97% (9746/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0763, Accuracy: 97% (9746/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 34 [6400/7500 (85%)], Loss: 0.325369,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 34 [6400/7500 (85%)], Loss: 0.132435,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 34 [6400/7500 (85%)], Loss: 0.091419,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 34 [6400/7500 (85%)], Loss: 0.118064,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 34 [6400/7500 (85%)], Loss: 0.160916,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 34 [6400/7500 (85%)], Loss: 0.129970,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 34 [6400/7500 (85%)], Loss: 0.271828,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 34 [6400/7500 (85%)], Loss: 0.244961,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0747, Accuracy: 98% (9752/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0747, Accuracy: 98% (9752/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0747, Accuracy: 98% (9752/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0747, Accuracy: 98% (9752/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0747, Accuracy: 98% (9752/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0747, Accuracy: 98% (9752/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0747, Accuracy: 98% (9752/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0747, Accuracy: 98% (9752/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 35 [6400/7500 (85%)], Loss: 0.193583,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 35 [6400/7500 (85%)], Loss: 0.071134,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 35 [6400/7500 (85%)], Loss: 0.083748,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 35 [6400/7500 (85%)], Loss: 0.195544,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 35 [6400/7500 (85%)], Loss: 0.201734,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 35 [6400/7500 (85%)], Loss: 0.328503,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 35 [6400/7500 (85%)], Loss: 0.196778,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 35 [6400/7500 (85%)], Loss: 0.257573,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0726, Accuracy: 98% (9768/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0726, Accuracy: 98% (9768/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0726, Accuracy: 98% (9768/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0726, Accuracy: 98% (9768/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0726, Accuracy: 98% (9768/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0726, Accuracy: 98% (9768/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0726, Accuracy: 98% (9768/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0726, Accuracy: 98% (9768/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 36 [6400/7500 (85%)], Loss: 0.180152,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 36 [6400/7500 (85%)], Loss: 0.172998,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 36 [6400/7500 (85%)], Loss: 0.254696,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 36 [6400/7500 (85%)], Loss: 0.134537,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 36 [6400/7500 (85%)], Loss: 0.142097,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 36 [6400/7500 (85%)], Loss: 0.385568,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 36 [6400/7500 (85%)], Loss: 0.201280,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 36 [6400/7500 (85%)], Loss: 0.068670,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0732, Accuracy: 98% (9760/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0732, Accuracy: 98% (9760/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0732, Accuracy: 98% (9760/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0732, Accuracy: 98% (9760/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0732, Accuracy: 98% (9760/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0732, Accuracy: 98% (9760/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0732, Accuracy: 98% (9760/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0732, Accuracy: 98% (9760/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 37 [6400/7500 (85%)], Loss: 0.129934,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 37 [6400/7500 (85%)], Loss: 0.179233,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 37 [6400/7500 (85%)], Loss: 0.125113,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 37 [6400/7500 (85%)], Loss: 0.081318,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 37 [6400/7500 (85%)], Loss: 0.258855,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 37 [6400/7500 (85%)], Loss: 0.166581,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 37 [6400/7500 (85%)], Loss: 0.271978,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 37 [6400/7500 (85%)], Loss: 0.087985,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0708, Accuracy: 98% (9771/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0708, Accuracy: 98% (9771/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0708, Accuracy: 98% (9771/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0708, Accuracy: 98% (9771/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0708, Accuracy: 98% (9771/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0708, Accuracy: 98% (9771/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0708, Accuracy: 98% (9771/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0708, Accuracy: 98% (9771/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 38 [6400/7500 (85%)], Loss: 0.222237,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 38 [6400/7500 (85%)], Loss: 0.108114,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 38 [6400/7500 (85%)], Loss: 0.106782,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 38 [6400/7500 (85%)], Loss: 0.147638,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 38 [6400/7500 (85%)], Loss: 0.062648,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 38 [6400/7500 (85%)], Loss: 0.200861,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 38 [6400/7500 (85%)], Loss: 0.148081,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 38 [6400/7500 (85%)], Loss: 0.265966,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0702, Accuracy: 98% (9777/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0702, Accuracy: 98% (9777/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0702, Accuracy: 98% (9777/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0702, Accuracy: 98% (9777/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0702, Accuracy: 98% (9777/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0702, Accuracy: 98% (9777/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0702, Accuracy: 98% (9777/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0702, Accuracy: 98% (9777/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 39 [6400/7500 (85%)], Loss: 0.085881,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 39 [6400/7500 (85%)], Loss: 0.147433,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 39 [6400/7500 (85%)], Loss: 0.263204,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 39 [6400/7500 (85%)], Loss: 0.170802,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 39 [6400/7500 (85%)], Loss: 0.099608,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 39 [6400/7500 (85%)], Loss: 0.093071,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 39 [6400/7500 (85%)], Loss: 0.138818,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 39 [6400/7500 (85%)], Loss: 0.196561,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0688, Accuracy: 98% (9785/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0688, Accuracy: 98% (9785/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0688, Accuracy: 98% (9785/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0688, Accuracy: 98% (9785/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0688, Accuracy: 98% (9785/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0688, Accuracy: 98% (9785/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0688, Accuracy: 98% (9785/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0688, Accuracy: 98% (9785/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 40 [6400/7500 (85%)], Loss: 0.287514,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 40 [6400/7500 (85%)], Loss: 0.239398,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 40 [6400/7500 (85%)], Loss: 0.155391,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 40 [6400/7500 (85%)], Loss: 0.067853,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 40 [6400/7500 (85%)], Loss: 0.336208,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 40 [6400/7500 (85%)], Loss: 0.180528,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 40 [6400/7500 (85%)], Loss: 0.139520,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 40 [6400/7500 (85%)], Loss: 0.169609,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0674, Accuracy: 98% (9786/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0674, Accuracy: 98% (9786/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0674, Accuracy: 98% (9786/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0674, Accuracy: 98% (9786/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0674, Accuracy: 98% (9786/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0674, Accuracy: 98% (9786/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0674, Accuracy: 98% (9786/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0674, Accuracy: 98% (9786/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 41 [6400/7500 (85%)], Loss: 0.269486,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 41 [6400/7500 (85%)], Loss: 0.207860,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 41 [6400/7500 (85%)], Loss: 0.237875,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 41 [6400/7500 (85%)], Loss: 0.122245,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 41 [6400/7500 (85%)], Loss: 0.100925,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 41 [6400/7500 (85%)], Loss: 0.090488,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 41 [6400/7500 (85%)], Loss: 0.157561,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 41 [6400/7500 (85%)], Loss: 0.277342,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0679, Accuracy: 98% (9781/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0679, Accuracy: 98% (9781/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0679, Accuracy: 98% (9781/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0679, Accuracy: 98% (9781/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0679, Accuracy: 98% (9781/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0679, Accuracy: 98% (9781/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0679, Accuracy: 98% (9781/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0679, Accuracy: 98% (9781/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 42 [6400/7500 (85%)], Loss: 0.089672,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 42 [6400/7500 (85%)], Loss: 0.156018,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 42 [6400/7500 (85%)], Loss: 0.165238,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 42 [6400/7500 (85%)], Loss: 0.236706,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 42 [6400/7500 (85%)], Loss: 0.389697,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 42 [6400/7500 (85%)], Loss: 0.175989,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 42 [6400/7500 (85%)], Loss: 0.137048,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 42 [6400/7500 (85%)], Loss: 0.061067,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0655, Accuracy: 98% (9794/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0655, Accuracy: 98% (9794/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0655, Accuracy: 98% (9794/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0655, Accuracy: 98% (9794/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0655, Accuracy: 98% (9794/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0655, Accuracy: 98% (9794/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0655, Accuracy: 98% (9794/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0655, Accuracy: 98% (9794/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 43 [6400/7500 (85%)], Loss: 0.105933,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 43 [6400/7500 (85%)], Loss: 0.215953,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 43 [6400/7500 (85%)], Loss: 0.078817,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 43 [6400/7500 (85%)], Loss: 0.185518,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 43 [6400/7500 (85%)], Loss: 0.139400,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 43 [6400/7500 (85%)], Loss: 0.079674,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 43 [6400/7500 (85%)], Loss: 0.152548,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 43 [6400/7500 (85%)], Loss: 0.263613,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0646, Accuracy: 98% (9794/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0646, Accuracy: 98% (9794/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0646, Accuracy: 98% (9794/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0646, Accuracy: 98% (9794/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0646, Accuracy: 98% (9794/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0646, Accuracy: 98% (9794/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0646, Accuracy: 98% (9794/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0646, Accuracy: 98% (9794/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 44 [6400/7500 (85%)], Loss: 0.125455,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 44 [6400/7500 (85%)], Loss: 0.133097,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 44 [6400/7500 (85%)], Loss: 0.129457,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 44 [6400/7500 (85%)], Loss: 0.232044,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 44 [6400/7500 (85%)], Loss: 0.131348,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 44 [6400/7500 (85%)], Loss: 0.163664,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 44 [6400/7500 (85%)], Loss: 0.056109,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 44 [6400/7500 (85%)], Loss: 0.118247,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0635, Accuracy: 98% (9799/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0635, Accuracy: 98% (9799/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0635, Accuracy: 98% (9799/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0635, Accuracy: 98% (9799/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0635, Accuracy: 98% (9799/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0635, Accuracy: 98% (9799/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0635, Accuracy: 98% (9799/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0635, Accuracy: 98% (9799/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 45 [6400/7500 (85%)], Loss: 0.121775,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 45 [6400/7500 (85%)], Loss: 0.116207,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 45 [6400/7500 (85%)], Loss: 0.151615,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 45 [6400/7500 (85%)], Loss: 0.105962,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 45 [6400/7500 (85%)], Loss: 0.323580,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 45 [6400/7500 (85%)], Loss: 0.163951,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 45 [6400/7500 (85%)], Loss: 0.165188,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 45 [6400/7500 (85%)], Loss: 0.120882,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0631, Accuracy: 98% (9799/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0631, Accuracy: 98% (9799/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0631, Accuracy: 98% (9799/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0631, Accuracy: 98% (9799/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0631, Accuracy: 98% (9799/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0631, Accuracy: 98% (9799/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0631, Accuracy: 98% (9799/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0631, Accuracy: 98% (9799/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 46 [6400/7500 (85%)], Loss: 0.200855,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 46 [6400/7500 (85%)], Loss: 0.109888,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 46 [6400/7500 (85%)], Loss: 0.091251,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 46 [6400/7500 (85%)], Loss: 0.106811,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 46 [6400/7500 (85%)], Loss: 0.191081,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 46 [6400/7500 (85%)], Loss: 0.193871,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 46 [6400/7500 (85%)], Loss: 0.122511,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 46 [6400/7500 (85%)], Loss: 0.165376,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0612, Accuracy: 98% (9806/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0612, Accuracy: 98% (9806/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0612, Accuracy: 98% (9806/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0612, Accuracy: 98% (9806/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0612, Accuracy: 98% (9806/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0612, Accuracy: 98% (9806/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0612, Accuracy: 98% (9806/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0612, Accuracy: 98% (9806/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 47 [6400/7500 (85%)], Loss: 0.070770,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 47 [6400/7500 (85%)], Loss: 0.302663,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 47 [6400/7500 (85%)], Loss: 0.131826,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 47 [6400/7500 (85%)], Loss: 0.107913,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 47 [6400/7500 (85%)], Loss: 0.109337,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 47 [6400/7500 (85%)], Loss: 0.200420,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 47 [6400/7500 (85%)], Loss: 0.171121,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 47 [6400/7500 (85%)], Loss: 0.124616,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0621, Accuracy: 98% (9807/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0621, Accuracy: 98% (9807/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0621, Accuracy: 98% (9807/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0621, Accuracy: 98% (9807/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0621, Accuracy: 98% (9807/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0621, Accuracy: 98% (9807/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0621, Accuracy: 98% (9807/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0621, Accuracy: 98% (9807/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 48 [6400/7500 (85%)], Loss: 0.126104,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 48 [6400/7500 (85%)], Loss: 0.245530,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 48 [6400/7500 (85%)], Loss: 0.163480,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 48 [6400/7500 (85%)], Loss: 0.056201,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 48 [6400/7500 (85%)], Loss: 0.048499,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 48 [6400/7500 (85%)], Loss: 0.179627,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 48 [6400/7500 (85%)], Loss: 0.265684,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 48 [6400/7500 (85%)], Loss: 0.135713,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0612, Accuracy: 98% (9803/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0612, Accuracy: 98% (9803/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0612, Accuracy: 98% (9803/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0612, Accuracy: 98% (9803/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0612, Accuracy: 98% (9803/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0612, Accuracy: 98% (9803/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0612, Accuracy: 98% (9803/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0612, Accuracy: 98% (9803/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 49 [6400/7500 (85%)], Loss: 0.086972,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 49 [6400/7500 (85%)], Loss: 0.278777,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 49 [6400/7500 (85%)], Loss: 0.223137,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 49 [6400/7500 (85%)], Loss: 0.092415,\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 49 [6400/7500 (85%)], Loss: 0.154773,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 49 [6400/7500 (85%)], Loss: 0.193058,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 49 [6400/7500 (85%)], Loss: 0.119890,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 49 [6400/7500 (85%)], Loss: 0.132821,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0611, Accuracy: 98% (9810/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0611, Accuracy: 98% (9810/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0611, Accuracy: 98% (9810/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0611, Accuracy: 98% (9810/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0611, Accuracy: 98% (9810/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0611, Accuracy: 98% (9810/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0611, Accuracy: 98% (9810/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0611, Accuracy: 98% (9810/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 50 [6400/7500 (85%)], Loss: 0.257446,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Train Epoch: 50 [6400/7500 (85%)], Loss: 0.105023,\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Train Epoch: 50 [6400/7500 (85%)], Loss: 0.194482,\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Train Epoch: 50 [6400/7500 (85%)], Loss: 0.255152,\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Train Epoch: 50 [6400/7500 (85%)], Loss: 0.144576,\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Train Epoch: 50 [6400/7500 (85%)], Loss: 0.102031,\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Train Epoch: 50 [6400/7500 (85%)], Loss: 0.090279,\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Train Epoch: 50 [6400/7500 (85%)], Loss: 0.145486,\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Test set: Average loss: 0.0595, Accuracy: 98% (9808/10000)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Saving the model.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Test set: Average loss: 0.0595, Accuracy: 98% (9808/10000)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Saving the model.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Test set: Average loss: 0.0595, Accuracy: 98% (9808/10000)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Saving the model.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Test set: Average loss: 0.0595, Accuracy: 98% (9808/10000)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Saving the model.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Test set: Average loss: 0.0595, Accuracy: 98% (9808/10000)\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Saving the model.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Test set: Average loss: 0.0595, Accuracy: 98% (9808/10000)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Saving the model.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Test set: Average loss: 0.0595, Accuracy: 98% (9808/10000)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:Saving the model.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Test set: Average loss: 0.0595, Accuracy: 98% (9808/10000)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Saving the model.\u001b[0m\n",
      "\n",
      "2021-02-04 14:45:16 Uploading - Uploading generated training model\u001b[34m2021-02-04 14:45:12,992 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-02-04 14:45:36 Completed - Training job completed\n",
      "ProfilerReport: IssuesFound\n",
      "Training seconds: 415\n",
      "Billable seconds: 124\n",
      "Managed Spot Training savings: 70.1%\n"
     ]
    }
   ],
   "source": [
    "job_name = \"training-job-{}\".format(int(time.time()))\n",
    "\n",
    "estimator.fit({'training': inputs},\n",
    "                     job_name=job_name\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugger Profiling Report 다운로드 받기\n",
    "Profiling report rule은 html report `profiler-report.html` 생성합니다. 이 Report에는 built-in rules 과 다음 단계에 대한 recommenadation에 대한 요약을 포함하고 있습니다. Report는 S3 bucket에 있으며 아래 cell을 실행하여 노트북으로 다운로드를 받습니다. 자세한 사항은 [SageMaker Debugger Profiling Report](https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-profiling-report.html) 에서 확인이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will find the profiler report in s3://sagemaker-us-east-1-322537213286/training-job-1612449311/rule-output\n"
     ]
    }
   ],
   "source": [
    "rule_output_path = estimator.output_path + estimator.latest_training_job.job_name + \"/rule-output\"\n",
    "print(f\"You will find the profiler report in {rule_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE profiler-reports/\n",
      "2021-02-04 14:45:33     428098 profiler-report.html\n",
      "2021-02-04 14:45:33     292281 profiler-report.ipynb\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls {rule_output_path}/ProfilerReport/profiler-output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = './output'\n",
    "!rm -rf $output_dir\n",
    "\n",
    "profile_output = output_dir+'/ProfilerReport'\n",
    "\n",
    "if not os.path.exists(profile_output):\n",
    "    os.makedirs(profile_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-322537213286/training-job-1612449311/rule-output/ProfilerReport/profiler-output/profiler-report.html to output/ProfilerReport/profiler-report.html\n",
      "download: s3://sagemaker-us-east-1-322537213286/training-job-1612449311/rule-output/ProfilerReport/profiler-output/profiler-reports/Dataloader.json to output/ProfilerReport/profiler-reports/Dataloader.json\n",
      "download: s3://sagemaker-us-east-1-322537213286/training-job-1612449311/rule-output/ProfilerReport/profiler-output/profiler-reports/CPUBottleneck.json to output/ProfilerReport/profiler-reports/CPUBottleneck.json\n",
      "download: s3://sagemaker-us-east-1-322537213286/training-job-1612449311/rule-output/ProfilerReport/profiler-output/profiler-reports/BatchSize.json to output/ProfilerReport/profiler-reports/BatchSize.json\n",
      "download: s3://sagemaker-us-east-1-322537213286/training-job-1612449311/rule-output/ProfilerReport/profiler-output/profiler-reports/LowGPUUtilization.json to output/ProfilerReport/profiler-reports/LowGPUUtilization.json\n",
      "download: s3://sagemaker-us-east-1-322537213286/training-job-1612449311/rule-output/ProfilerReport/profiler-output/profiler-report.ipynb to output/ProfilerReport/profiler-report.ipynb\n",
      "download: s3://sagemaker-us-east-1-322537213286/training-job-1612449311/rule-output/ProfilerReport/profiler-output/profiler-reports/OverallFrameworkMetrics.json to output/ProfilerReport/profiler-reports/OverallFrameworkMetrics.json\n",
      "download: s3://sagemaker-us-east-1-322537213286/training-job-1612449311/rule-output/ProfilerReport/profiler-output/profiler-reports/MaxInitializationTime.json to output/ProfilerReport/profiler-reports/MaxInitializationTime.json\n",
      "download: s3://sagemaker-us-east-1-322537213286/training-job-1612449311/rule-output/ProfilerReport/profiler-output/profiler-reports/OverallSystemUsage.json to output/ProfilerReport/profiler-reports/OverallSystemUsage.json\n",
      "download: s3://sagemaker-us-east-1-322537213286/training-job-1612449311/rule-output/ProfilerReport/profiler-output/profiler-reports/StepOutlier.json to output/ProfilerReport/profiler-reports/StepOutlier.json\n",
      "download: s3://sagemaker-us-east-1-322537213286/training-job-1612449311/rule-output/ProfilerReport/profiler-output/profiler-reports/IOBottleneck.json to output/ProfilerReport/profiler-reports/IOBottleneck.json\n",
      "download: s3://sagemaker-us-east-1-322537213286/training-job-1612449311/rule-output/ProfilerReport/profiler-output/profiler-reports/LoadBalancing.json to output/ProfilerReport/profiler-reports/LoadBalancing.json\n",
      "download: s3://sagemaker-us-east-1-322537213286/training-job-1612449311/rule-output/ProfilerReport/profiler-output/profiler-reports/GPUMemoryIncrease.json to output/ProfilerReport/profiler-reports/GPUMemoryIncrease.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {rule_output_path}/ProfilerReport/profiler-output/ {output_dir}/ProfilerReport/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>ProfilerReport : <a href=\"./output/ProfilerReport/profiler-report.html\">Profiler Report</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>ProfilerReport : <a href=\"{}profiler-report.html\">Profiler Report</a></b>'.format(output_dir+\"/ProfilerReport/\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host\n",
    "### Create endpoint\n",
    "After training, we use the `PyTorch` estimator object to build and deploy a `PyTorchPredictor`. This creates a Sagemaker Endpoint -- a hosted prediction service that we can use to perform inference.\n",
    "\n",
    "As mentioned above we have implementation of `model_fn` in the `mnist.py` script that is required. We are going to use default implementations of `input_fn`, `predict_fn`, `output_fn` and `transform_fm` defined in [sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers).\n",
    "\n",
    "The arguments to the deploy function allow us to set the number and type of instances that will be used for the Endpoint. These do not need to be the same as the values we used for the training job. For example, you can train a model on a set of GPU-based instances, and then deploy the Endpoint to a fleet of CPU-based instances, but you need to make sure that you return or save your model as a cpu model similar to what we did in `mnist.py`. Here we will deploy the model to a single ```ml.m4.xlarge``` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "We can now use this predictor to classify hand-written digits. Drawing into the image box loads the pixel data into a `data` variable in this notebook, which we can then pass to the `predictor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(open(\"input.html\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "image = np.array([data], dtype=np.float32)\n",
    "response = predictor.predict(image)\n",
    "prediction = response.argmax(axis=1)[0]\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
