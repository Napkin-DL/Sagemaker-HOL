{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Processing Job \n",
    "\n",
    "\n",
    "기계 학습 (ML) 프로세스는 몇 단계로 구성됩니다. 먼저, 다양한 ETL 작업으로 데이터를 수집 한 다음 data의 pre-processing, 전통적인 기법 또는 사전 knowledge를 이용하여 데이터의 feature화, 마지막으로 알고리즘을 이용한 ML 모델을 학습합니다.\n",
    "\n",
    "Apache Spark와 같은 분산 데이터 처리 프레임 워크는 학습을 위해 dataset의 pre-processing하는데 사용합니다. 이 노트북에서는 Amazon SageMaker Processing에서 기본 설치된 Apache Spark의 기능을 활용하여 처리 워크로드를 실행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/prepare_dataset_bert.png)\n",
    "\n",
    "![](img/processing.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment\n",
    "\n",
    "\n",
    "* 모델 학습에 사용되는 S3 bucket과 prefix 가 필요합니다.\n",
    "* 학습과 processing을 위해 IAM role은 dataset에 액세스가 가능해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-322537213286/amazon-reviews-pds/tsv/\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "s3_input_data = 's3://{}/amazon-reviews-pds/tsv/'.format(bucket)\n",
    "print(s3_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-31 08:48:04   18997559 amazon_reviews_us_Digital_Software_v1_00.tsv.gz\r\n",
      "2020-07-31 08:48:07   27442648 amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\r\n",
      "2020-07-31 08:48:08  193389086 amazon_reviews_us_Musical_Instruments_v1_00.tsv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $s3_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Job을 수행할 Spark Docker Image\n",
    "\n",
    "이 HOL에서는 `./container` 폴더 내에 Spark container 이미지를 포함합니다. container는 모든 Spark 구성의 부트스트랩을 처리하고 `spark-submit` CLI를 wrapper해서 제공합니다. 상위 레벨에서는,\n",
    "\n",
    "* A set of default Spark/YARN/Hadoop configurations\n",
    "* A bootstrapping script for configuring and starting up Spark master/worker nodes\n",
    "* A wrapper around the `spark-submit` CLI to submit a Spark application\n",
    "\n",
    "container 빌드와 push 절차가 완료된 후 dataset의 처리를 수행하는 관리형 분산 Spark 어플리케이션을 수행사는 것은 Amazon SageMaker Python SDK 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_repo = 'amazon-reviews-spark-processor'\n",
    "docker_tag = 'latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  4.385MB\n",
      "Step 1/37 : FROM openjdk:8-jre-slim\n",
      " ---> fec2d7a65b07\n",
      "Step 2/37 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> e88bd9f80f9f\n",
      "Step 3/37 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\n",
      " ---> Using cache\n",
      " ---> b898d99fac21\n",
      "Step 4/37 : RUN pip3 install py4j psutil==5.6.5 numpy==1.17.4\n",
      " ---> Using cache\n",
      " ---> 0e16d95264d8\n",
      "Step 5/37 : RUN apt-get clean\n",
      " ---> Using cache\n",
      " ---> 4f3243262295\n",
      "Step 6/37 : RUN rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> d9634587550e\n",
      "Step 7/37 : ENV PYTHONHASHSEED 0\n",
      " ---> Using cache\n",
      " ---> 49b8b187803d\n",
      "Step 8/37 : ENV PYTHONIOENCODING UTF-8\n",
      " ---> Using cache\n",
      " ---> 08c7b9e2cf28\n",
      "Step 9/37 : ENV PIP_DISABLE_PIP_VERSION_CHECK 1\n",
      " ---> Using cache\n",
      " ---> af67654cc520\n",
      "Step 10/37 : ENV HADOOP_VERSION 3.2.1\n",
      " ---> Using cache\n",
      " ---> 62ce8d352867\n",
      "Step 11/37 : ENV HADOOP_HOME /usr/hadoop-$HADOOP_VERSION\n",
      " ---> Using cache\n",
      " ---> 854a7d7d0d70\n",
      "Step 12/37 : ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n",
      " ---> Using cache\n",
      " ---> 400d4b134bce\n",
      "Step 13/37 : ENV PATH $PATH:$HADOOP_HOME/bin\n",
      " ---> Using cache\n",
      " ---> 184b18416cc9\n",
      "Step 14/37 : RUN curl -sL --retry 3   \"http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\"   | gunzip   | tar -x -C /usr/  && rm -rf $HADOOP_HOME/share/doc  && chown -R root:root $HADOOP_HOME\n",
      " ---> Using cache\n",
      " ---> 208955450d5a\n",
      "Step 15/37 : ENV SPARK_VERSION 2.4.6\n",
      " ---> Using cache\n",
      " ---> 62f8746216e4\n",
      "Step 16/37 : ENV SPARK_PACKAGE spark-${SPARK_VERSION}-bin-without-hadoop\n",
      " ---> Using cache\n",
      " ---> 2ff2a9d23647\n",
      "Step 17/37 : ENV SPARK_HOME /usr/spark-${SPARK_VERSION}\n",
      " ---> Using cache\n",
      " ---> 148ecd8698c6\n",
      "Step 18/37 : ENV SPARK_DIST_CLASSPATH=\"$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*\"\n",
      " ---> Using cache\n",
      " ---> f09c61f7d6c8\n",
      "Step 19/37 : ENV PATH $PATH:${SPARK_HOME}/bin\n",
      " ---> Using cache\n",
      " ---> ab5983283d72\n",
      "Step 20/37 : RUN curl -sL --retry 3   \"https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/${SPARK_PACKAGE}.tgz\"   | gunzip   | tar x -C /usr/  && mv /usr/$SPARK_PACKAGE $SPARK_HOME  && chown -R root:root $SPARK_HOME\n",
      " ---> Using cache\n",
      " ---> 4e569553b171\n",
      "Step 21/37 : ENV PYSPARK_PYTHON=/usr/bin/python3\n",
      " ---> Using cache\n",
      " ---> bee53666ed1a\n",
      "Step 22/37 : ENV PATH=\"/usr/bin:/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 7de8cf86b65d\n",
      "Step 23/37 : ENV YARN_RESOURCEMANAGER_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> e2634507ec06\n",
      "Step 24/37 : ENV YARN_NODEMANAGER_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 3e0d6a398325\n",
      "Step 25/37 : ENV HDFS_NAMENODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> ca1d0fbe2933\n",
      "Step 26/37 : ENV HDFS_DATANODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> ed79d78fd120\n",
      "Step 27/37 : ENV HDFS_SECONDARYNAMENODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 12a9485f0d30\n",
      "Step 28/37 : COPY program /opt/program\n",
      " ---> Using cache\n",
      " ---> 5e031d34ac9d\n",
      "Step 29/37 : RUN chmod +x /opt/program/submit\n",
      " ---> Using cache\n",
      " ---> 33c0b5b50e9b\n",
      "Step 30/37 : COPY hadoop-config /opt/hadoop-config\n",
      " ---> Using cache\n",
      " ---> 3585dba7f7db\n",
      "Step 31/37 : COPY jars /usr/jars\n",
      " ---> Using cache\n",
      " ---> 51ca48937f9d\n",
      "Step 32/37 : WORKDIR $SPARK_HOME\n",
      " ---> Using cache\n",
      " ---> a0f25abb3c18\n",
      "Step 33/37 : RUN pip3 install -q pip --upgrade\n",
      " ---> Using cache\n",
      " ---> 5df708048309\n",
      "Step 34/37 : RUN pip3 install -q wrapt --upgrade --ignore-installed\n",
      " ---> Using cache\n",
      " ---> 17c4a7a01842\n",
      "Step 35/37 : RUN pip3 install -q transformers==2.8.0\n",
      " ---> Using cache\n",
      " ---> 4c5c1515c802\n",
      "Step 36/37 : RUN pip3 install -q tensorflow==2.1.0 --upgrade --ignore-installed\n",
      " ---> Using cache\n",
      " ---> 94634518f065\n",
      "Step 37/37 : ENTRYPOINT [\"/opt/program/submit\"]\n",
      " ---> Using cache\n",
      " ---> d1b5d2cd4440\n",
      "Successfully built d1b5d2cd4440\n",
      "Successfully tagged amazon-reviews-spark-processor:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $docker_repo:$docker_tag -f container/Dockerfile ./container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark container의 Amazon Elastic Container Registry(Amazon ECR) 리포지토리를 생성하고 image를 push합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322537213286.dkr.ecr.us-east-1.amazonaws.com/amazon-reviews-spark-processor:latest\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "image_uri = '{}.dkr.ecr.{}.amazonaws.com/{}:{}'.format(account_id, region, docker_repo, docker_tag)\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECR repository 생성과 docker image를 push하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `RepositoryNotFoundException` 오류는 무시하셔도 됩니다. 즉시 repository를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"repositories\": [\r\n",
      "        {\r\n",
      "            \"repositoryArn\": \"arn:aws:ecr:us-east-1:322537213286:repository/amazon-reviews-spark-processor\",\r\n",
      "            \"registryId\": \"322537213286\",\r\n",
      "            \"repositoryName\": \"amazon-reviews-spark-processor\",\r\n",
      "            \"repositoryUri\": \"322537213286.dkr.ecr.us-east-1.amazonaws.com/amazon-reviews-spark-processor\",\r\n",
      "            \"createdAt\": 1596262176.0,\r\n",
      "            \"imageTagMutability\": \"MUTABLE\",\r\n",
      "            \"imageScanningConfiguration\": {\r\n",
      "                \"scanOnPush\": false\r\n",
      "            }\r\n",
      "        }\r\n",
      "    ]\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!aws ecr describe-repositories --repository-names $docker_repo || aws ecr create-repository --repository-name $docker_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker tag $docker_repo:$docker_tag $image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [322537213286.dkr.ecr.us-east-1.amazonaws.com/amazon-reviews-spark-processor]\n",
      "\n",
      "\u001b[1B17e4a6cc: Preparing \n",
      "\u001b[1B70cf88ab: Preparing \n",
      "\u001b[1B9c81145b: Preparing \n",
      "\u001b[1B1c38d96f: Preparing \n",
      "\u001b[1B1a6011de: Preparing \n",
      "\u001b[1B60e645c3: Preparing \n",
      "\u001b[1B7573735c: Preparing \n",
      "\u001b[1B1887bc1e: Preparing \n",
      "\u001b[1B158a1221: Preparing \n",
      "\u001b[1B582e15ed: Preparing \n",
      "\u001b[1B47ad5106: Preparing \n",
      "\u001b[1Ba7f65aee: Preparing \n",
      "\u001b[1B0eeb3a13: Preparing \n",
      "\u001b[1Bb9f2d7c4: Preparing \n",
      "\u001b[1Ba4d413ef: Preparing \n",
      "\u001b[11B0e645c3: Waiting g \n",
      "\u001b[1B38d128fe: Preparing \n",
      "\u001b[1Be510e849: Preparing \n",
      "\u001b[1B25a32043: Layer already exists \u001b[17A\u001b[2K\u001b[2A\u001b[2Klatest: digest: sha256:00c3463d03d4d50e1c03f0e4d15b935ee0117645e7da3fc889256dffea6cd42e size: 4318\n"
     ]
    }
   ],
   "source": [
    "!docker push $image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Processing Jobs 으로 Job 수행\n",
    "\n",
    "Amazon SageMaker Python SDK를 사용하여 Processing job을 실행합니다. Spark container와 job configuration에서 processing에 대한 Spark ML script를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m unicode_literals\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mshutil\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'pip', '--upgrade'])\u001b[39;49;00m\r\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'wrapt', '--upgrade', '--ignore-installed'])\u001b[39;49;00m\r\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0', '--ignore-installed'])\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[36mprint\u001b[39;49;00m(tf.__version__)\r\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'transformers==2.8.0'])\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SparkSession\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mml\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Pipeline\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mml\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mlinalg\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DenseVector\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m split\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m udf, col\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtypes\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\r\n",
      "\r\n",
      "tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[37m# We set sequences to be at most 128 tokens long.\u001b[39;49;00m\r\n",
      "MAX_SEQ_LENGTH = \u001b[34m128\u001b[39;49;00m\r\n",
      "DATA_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "LABEL_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "LABEL_VALUES = [\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\r\n",
      "\r\n",
      "label_map = {}\r\n",
      "\u001b[34mfor\u001b[39;49;00m (i, label) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(LABEL_VALUES):\r\n",
      "    label_map[label] = i\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInputFeatures\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\r\n",
      "  \u001b[33m\"\"\"BERT feature vectors.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m,\r\n",
      "               input_ids,\r\n",
      "               input_mask,\r\n",
      "               segment_ids,\r\n",
      "               label_id):\r\n",
      "    \u001b[36mself\u001b[39;49;00m.input_ids = input_ids\r\n",
      "    \u001b[36mself\u001b[39;49;00m.input_mask = input_mask\r\n",
      "    \u001b[36mself\u001b[39;49;00m.segment_ids = segment_ids\r\n",
      "    \u001b[36mself\u001b[39;49;00m.label_id = label_id\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInput\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\r\n",
      "  \u001b[33m\"\"\"A single training/test input for sequence classification.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, text, label=\u001b[34mNone\u001b[39;49;00m):\r\n",
      "    \u001b[33m\"\"\"Constructs an Input.\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m      text: string. The untokenized text of the first sequence. For single\u001b[39;49;00m\r\n",
      "\u001b[33m        sequence tasks, only this sequence must be specified.\u001b[39;49;00m\r\n",
      "\u001b[33m      label: (Optional) string. The label of the example. This should be\u001b[39;49;00m\r\n",
      "\u001b[33m        specified for train and dev examples, but not for test examples.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[36mself\u001b[39;49;00m.text = text\r\n",
      "    \u001b[36mself\u001b[39;49;00m.label = label\r\n",
      "    \r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mconvert_input\u001b[39;49;00m(label, text):\r\n",
      "    \u001b[37m# First, we need to preprocess our data so that it matches the data BERT was trained on:\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# 1. Lowercase our text (if we're using a BERT lowercase model)\u001b[39;49;00m\r\n",
      "    \u001b[37m# 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m# 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# Fortunately, the Transformers tokenizer does this for us!\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m#    tokens = tokenizer.tokenize(text_input.text)\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# Next, we need to do the following:\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# 4. Map our words to indexes using a vocab file that BERT provides\u001b[39;49;00m\r\n",
      "    \u001b[37m# 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\u001b[39;49;00m\r\n",
      "    \u001b[37m# 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# Again, the Transformers tokenizer does this for us!\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    encode_plus_tokens = tokenizer.encode_plus(text,\r\n",
      "                                               pad_to_max_length=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                                               max_length=MAX_SEQ_LENGTH)\r\n",
      "\r\n",
      "    \u001b[37m# Convert the text-based tokens to ids from the pre-trained BERT vocabulary\u001b[39;49;00m\r\n",
      "    input_ids = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    \u001b[37m# Specifies which tokens BERT should pay attention to (0 or 1)\u001b[39;49;00m\r\n",
      "    input_mask = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    \u001b[37m# Segment Ids are always 0 for single-sequence tasks (or 1 if two-sequence tasks)\u001b[39;49;00m\r\n",
      "    segment_ids = [\u001b[34m0\u001b[39;49;00m] * MAX_SEQ_LENGTH\r\n",
      "\r\n",
      "    \u001b[37m# Label for our training data (star_rating 1 through 5)\u001b[39;49;00m\r\n",
      "    label_id = label_map[label]\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m {\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: input_ids, \u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: input_mask, \u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: segment_ids, \u001b[33m'\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: [label_id]}\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mlist_arg\u001b[39;49;00m(raw_value):\r\n",
      "    \u001b[33m\"\"\"argparse type for a list of strings\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(raw_value).split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\r\n",
      "    \u001b[37m# Unlike SageMaker training jobs (which have `SM_HOSTS` and `SM_CURRENT_HOST` env vars), processing jobs to need to parse the resource config file directly\u001b[39;49;00m\r\n",
      "    resconfig = {}\r\n",
      "    \u001b[34mtry\u001b[39;49;00m:\r\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m cfgfile:\r\n",
      "            resconfig = json.load(cfgfile)\r\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mFileNotFoundError\u001b[39;49;00m:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json not found.  current_host is unknown.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[34mpass\u001b[39;49;00m \u001b[37m# Ignore\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# Local testing with CLI args\u001b[39;49;00m\r\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m'\u001b[39;49;00m\u001b[33mProcess\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=list_arg,\r\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mhosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, [\u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]),\r\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mComma-separated list of host names running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mcurrent_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\r\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mName of this host running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/output\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtransform\u001b[39;49;00m(spark, s3_input_data, s3_output_train_data, s3_output_validation_data, s3_output_test_data): \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mProcessing \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m => \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(s3_input_data, s3_output_train_data, s3_output_validation_data, s3_output_test_data))\r\n",
      " \r\n",
      "    schema = StructType([\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mmarketplace\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mcustomer_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mproduct_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mproduct_parent\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mproduct_title\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mproduct_category\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, IntegerType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mhelpful_votes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, IntegerType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mtotal_votes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, IntegerType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mvine\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mverified_purchase\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_headline\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_date\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    ])\r\n",
      "    \r\n",
      "    df_csv = spark.read.csv(path=s3_input_data,\r\n",
      "                            sep=\u001b[33m'\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                            schema=schema,\r\n",
      "                            header=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                            quote=\u001b[34mNone\u001b[39;49;00m)\r\n",
      "    df_csv.show()\r\n",
      "\r\n",
      "    \u001b[37m# This dataset should already be clean, but always good to double-check\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShowing null review_body rows...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    df_csv.where(col(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).isNull()).show()\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShowing cleaned csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    df_csv_dropped = df_csv.na.drop(subset=[\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    df_csv_dropped.show()\r\n",
      "\r\n",
      "    \u001b[37m# TODO:  Balance\u001b[39;49;00m\r\n",
      "    \r\n",
      "    features_df = df_csv_dropped.select([\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    features_df.show()\r\n",
      "\r\n",
      "    tfrecord_schema = StructType([\r\n",
      "      StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ArrayType(IntegerType(), \u001b[34mFalse\u001b[39;49;00m)),\r\n",
      "      StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ArrayType(IntegerType(), \u001b[34mFalse\u001b[39;49;00m)),\r\n",
      "      StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ArrayType(IntegerType(), \u001b[34mFalse\u001b[39;49;00m)),\r\n",
      "      StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ArrayType(IntegerType(), \u001b[34mFalse\u001b[39;49;00m))\r\n",
      "    ])\r\n",
      "\r\n",
      "    bert_transformer = udf(\u001b[34mlambda\u001b[39;49;00m text, label: convert_input(text, label), tfrecord_schema)\r\n",
      "\r\n",
      "    spark.udf.register(\u001b[33m'\u001b[39;49;00m\u001b[33mbert_transformer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, bert_transformer)\r\n",
      "\r\n",
      "    transformed_df = features_df.select(bert_transformer(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).alias(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecords\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    transformed_df.show(truncate=\u001b[34mFalse\u001b[39;49;00m)\r\n",
      "\r\n",
      "    flattened_df = transformed_df.select(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecords.*\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    flattened_df.show()\r\n",
      "\r\n",
      "    \u001b[37m# Split 90-5-5%\u001b[39;49;00m\r\n",
      "    train_df, validation_df, test_df = flattened_df.randomSplit([\u001b[34m0.9\u001b[39;49;00m, \u001b[34m0.05\u001b[39;49;00m, \u001b[34m0.05\u001b[39;49;00m])\r\n",
      "\r\n",
      "    train_df.write.format(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).option(\u001b[33m'\u001b[39;49;00m\u001b[33mrecordType\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mExample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).save(path=s3_output_train_data)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWrote to output file:  \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(s3_output_train_data))\r\n",
      "    \r\n",
      "    validation_df.write.format(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).option(\u001b[33m'\u001b[39;49;00m\u001b[33mrecordType\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mExample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).save(path=s3_output_validation_data)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWrote to output file:  \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(s3_output_validation_data))\r\n",
      "\r\n",
      "    test_df.write.format(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).option(\u001b[33m'\u001b[39;49;00m\u001b[33mrecordType\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mExample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).save(path=s3_output_test_data)    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWrote to output file:  \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(s3_output_test_data))\r\n",
      "\r\n",
      "    restored_test_df = spark.read.format(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).option(\u001b[33m'\u001b[39;49;00m\u001b[33mrecordType\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mExample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).load(path=s3_output_test_data)\r\n",
      "    restored_test_df.show()\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\r\n",
      "    spark = SparkSession.builder.appName(\u001b[33m'\u001b[39;49;00m\u001b[33mAmazonReviewsSparkProcessor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).getOrCreate()\r\n",
      "\r\n",
      "    \u001b[37m# Convert command line args into a map of args\u001b[39;49;00m\r\n",
      "    args_iter = \u001b[36miter\u001b[39;49;00m(sys.argv[\u001b[34m1\u001b[39;49;00m:])\r\n",
      "    args = \u001b[36mdict\u001b[39;49;00m(\u001b[36mzip\u001b[39;49;00m(args_iter, args_iter))\r\n",
      "\r\n",
      "    \u001b[37m# Retrieve the args and replace 's3://' with 's3a://' (used by Spark)\u001b[39;49;00m\r\n",
      "    s3_input_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_input_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_input_data)\r\n",
      "\r\n",
      "    s3_output_train_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_output_train_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_output_train_data)\r\n",
      "\r\n",
      "    s3_output_validation_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_output_validation_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_output_validation_data)\r\n",
      "\r\n",
      "    s3_output_test_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_output_test_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_output_test_data)\r\n",
      "\r\n",
      "    transform(spark, s3_input_data, s3_output_train_data, s3_output_validation_data, s3_output_test_data)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    main()\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src_dir/preprocess-spark-text-to-bert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "processor = ScriptProcessor(base_job_name='spark-amazon-reviews-processor',\n",
    "                            image_uri=image_uri,\n",
    "                            command=['/opt/program/submit'],\n",
    "                            role=role,\n",
    "                            instance_count=2, # instance_count needs to be > 1 or you will see the following error:  \"INFO yarn.Client: Application report for application_ (state: ACCEPTED)\"\n",
    "                            instance_type='ml.r5.xlarge',\n",
    "                            env={'mode': 'python'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "output_prefix = 'amazon-reviews-spark-processor-{}'.format(timestamp_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-08-01-06-14-52/output/bert-train\n",
      "s3://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-08-01-06-14-52/output/bert-validation\n",
      "s3://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-08-01-06-14-52/output/bert-test\n"
     ]
    }
   ],
   "source": [
    "train_data_bert_output = 's3://{}/{}/output/bert-train'.format(bucket, output_prefix)\n",
    "validation_data_bert_output = 's3://{}/{}/output/bert-validation'.format(bucket, output_prefix)\n",
    "test_data_bert_output = 's3://{}/{}/output/bert-test'.format(bucket, output_prefix)\n",
    "\n",
    "print(train_data_bert_output)\n",
    "print(validation_data_bert_output)\n",
    "print(test_data_bert_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  spark-amazon-reviews-processor-2020-08-01-06-14-52-123\n",
      "Inputs:  [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-322537213286/spark-amazon-reviews-processor-2020-08-01-06-14-52-123/input/code/preprocess-spark-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'dummy-output', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-322537213286/spark-amazon-reviews-processor-2020-08-01-06-14-52-123/output/dummy-output', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingOutput\n",
    "\n",
    "processor.run(code='./src_dir/preprocess-spark-text-to-bert.py',\n",
    "              arguments=['s3_input_data', s3_input_data,\n",
    "                         's3_output_train_data', train_data_bert_output,\n",
    "                         's3_output_validation_data', validation_data_bert_output,\n",
    "                         's3_output_test_data', test_data_bert_output,                         \n",
    "              ],\n",
    "              # We need this dummy output to allow us to call \n",
    "              #    ProcessingJob.from_processing_name() later \n",
    "              #    to describe the job and poll for Completed status\n",
    "              outputs=[\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='dummy-output',\n",
    "                                        source='/opt/ml/processing/output')\n",
    "              ],          \n",
    "              logs=True,\n",
    "              wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=spark-amazon-reviews-processor-2020-08-01-06-14-52-123;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "spark_processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, spark_processing_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-08-01-06-14-52/?region=us-east-1&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# This is different than the job name because we are not using ProcessingOutput's in this Spark ML case.\n",
    "spark_processing_job_s3_output_prefix = output_prefix\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>'.format(bucket, spark_processing_job_s3_output_prefix, region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Processing Jobs through boto3 Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ProcessingJobSummaries': [{'ProcessingJobName': 'spark-amazon-reviews-processor-2020-08-01-06-14-52-123',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:322537213286:processing-job/spark-amazon-reviews-processor-2020-08-01-06-14-52-123',\n",
       "   'CreationTime': datetime.datetime(2020, 8, 1, 6, 14, 52, 478000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 8, 1, 6, 14, 52, 478000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'InProgress'},\n",
       "  {'ProcessingJobName': 'sagemaker-scikit-learn-2020-08-01-05-52-12-044',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:322537213286:processing-job/sagemaker-scikit-learn-2020-08-01-05-52-12-044',\n",
       "   'CreationTime': datetime.datetime(2020, 8, 1, 5, 52, 12, 520000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 8, 1, 5, 56, 59, 919000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 8, 1, 5, 56, 59, 922000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'sagemaker-scikit-learn-2020-08-01-05-07-19-914',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:322537213286:processing-job/sagemaker-scikit-learn-2020-08-01-05-07-19-914',\n",
       "   'CreationTime': datetime.datetime(2020, 8, 1, 5, 7, 20, 294000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 8, 1, 5, 12, 35, 314000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 8, 1, 5, 12, 35, 317000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'sagemaker-scikit-learn-2020-08-01-03-47-19-276',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:322537213286:processing-job/sagemaker-scikit-learn-2020-08-01-03-47-19-276',\n",
       "   'CreationTime': datetime.datetime(2020, 8, 1, 3, 47, 19, 657000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 8, 1, 3, 52, 0, 445000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 8, 1, 3, 52, 0, 449000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'pr-1-09ad3615213a40859366cee16f99b5f2689ec9e528684104ba3d5b138c',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:322537213286:processing-job/pr-1-09ad3615213a40859366cee16f99b5f2689ec9e528684104ba3d5b138c',\n",
       "   'CreationTime': datetime.datetime(2020, 7, 31, 8, 56, 21, 564000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 7, 31, 9, 1, 28, 28000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 7, 31, 9, 1, 28, 32000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'db-1-86fd3b6e4b7c454e8dbfb32fa925f523566db8dac0a149a3829b0eff00',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:322537213286:processing-job/db-1-86fd3b6e4b7c454e8dbfb32fa925f523566db8dac0a149a3829b0eff00',\n",
       "   'CreationTime': datetime.datetime(2020, 7, 31, 8, 52, 14, 535000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 7, 31, 8, 56, 5, 172000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 7, 31, 8, 56, 5, 176000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'DAGM-training-job-15814242-ExplodingTensor-172488f3',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:322537213286:processing-job/dagm-training-job-15814242-explodingtensor-172488f3',\n",
       "   'CreationTime': datetime.datetime(2020, 2, 11, 12, 31, 36, 266000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 2, 11, 12, 37, 17, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 2, 11, 12, 37, 17, 201000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Stopped'},\n",
       "  {'ProcessingJobName': 'DAGM-training-job-15814242-VanishingGradient-7181f93f',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:322537213286:processing-job/dagm-training-job-15814242-vanishinggradient-7181f93f',\n",
       "   'CreationTime': datetime.datetime(2020, 2, 11, 12, 31, 35, 53000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 2, 11, 12, 37, 4, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 2, 11, 12, 37, 4, 291000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Stopped'}],\n",
       " 'ResponseMetadata': {'RequestId': '17e61cce-0ad9-433e-9a48-3b92b0dc4d74',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '17e61cce-0ad9-433e-9a48-3b92b0dc4d74',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2761',\n",
       "   'date': 'Sat, 01 Aug 2020 06:14:51 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client('sagemaker')\n",
    "client.list_processing_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Wait Until the Processing Job Completes\n",
    "Re-run this next cell until the job status shows `Completed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "InProgress\n",
      "\n",
      "\n",
      "{'ProcessingInputs': [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-322537213286/spark-amazon-reviews-processor-2020-08-01-06-14-52-123/input/code/preprocess-spark-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'dummy-output', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-322537213286/spark-amazon-reviews-processor-2020-08-01-06-14-52-123/output/dummy-output', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]}, 'ProcessingJobName': 'spark-amazon-reviews-processor-2020-08-01-06-14-52-123', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.r5.xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}, 'AppSpecification': {'ImageUri': '322537213286.dkr.ecr.us-east-1.amazonaws.com/amazon-reviews-spark-processor:latest', 'ContainerEntrypoint': ['/opt/program/submit', '/opt/ml/processing/input/code/preprocess-spark-text-to-bert.py'], 'ContainerArguments': ['s3_input_data', 's3://sagemaker-us-east-1-322537213286/amazon-reviews-pds/tsv/', 's3_output_train_data', 's3://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-08-01-06-14-52/output/bert-train', 's3_output_validation_data', 's3://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-08-01-06-14-52/output/bert-validation', 's3_output_test_data', 's3://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-08-01-06-14-52/output/bert-test']}, 'Environment': {'mode': 'python'}, 'RoleArn': 'arn:aws:iam::322537213286:role/service-role/AIMLWorkshop-SageMakerIamRole-12B201FVOZYAC', 'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:322537213286:processing-job/spark-amazon-reviews-processor-2020-08-01-06-14-52-123', 'ProcessingJobStatus': 'InProgress', 'LastModifiedTime': datetime.datetime(2020, 8, 1, 6, 14, 52, 478000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 8, 1, 6, 14, 52, 478000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': 'b6977adf-aa04-458c-af98-c28b4a3a88ba', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'b6977adf-aa04-458c-af98-c28b4a3a88ba', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1971', 'date': 'Sat, 01 Aug 2020 06:14:52 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=spark_processing_job_name,\n",
    "                                                                            sagemaker_session=sagemaker_session)\n",
    "\n",
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "processing_job_status = processing_job_description['ProcessingJobStatus']\n",
    "print('\\n')\n",
    "print(processing_job_status)\n",
    "print('\\n')\n",
    "\n",
    "print(processing_job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................\u001b[34mWARNING: /usr/hadoop-3.2.1/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:38,961 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.109.111\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.2.1/etc/hadoop:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-databind-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-lang3-3.7.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/curator-client-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-text-1.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/curator-framework-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/zookeeper-3.4.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-io-2.5.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/httpcore-4.4.10.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/hadoop-annotations-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-net-3.6.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-codec-1.11.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/httpclient-4.5.6.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-compress-1.18.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/hadoop-auth-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-annotations-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.375.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/hadoop-aws-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-kms-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-nfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-databind-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-io-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-text-1.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/httpcore-4.4.10.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-annotations-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/httpclient-4.5.6.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-security-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-auth-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-annotations-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-api-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-submarine-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-client-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-api-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-common-3.2\u001b[0m\n",
      "\u001b[34m.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-router-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-registry-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-core-3.2.1.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_265\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:38,971 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,047 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-7489d5d3-5c2d-45ce-a2c0-0defc4e8b55a\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,450 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,466 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,467 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,467 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,472 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,472 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,472 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,472 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,514 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,524 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,524 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,529 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,531 INFO blockmanagement.BlockManager: The block deletion will start around 2020 Aug 01 06:18:39\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,533 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,533 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,534 INFO util.GSet: 2.0% max memory 6.7 GB = 136.4 MB\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,534 INFO util.GSet: capacity      = 2^24 = 16777216 entries\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,574 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,574 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,582 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,583 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,583 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,583 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,583 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,583 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,583 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,583 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,584 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,584 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,584 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,607 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,607 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,607 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,607 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,620 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,620 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,621 INFO util.GSet: 1.0% max memory 6.7 GB = 68.2 MB\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,621 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,638 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,638 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,638 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,639 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,643 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,645 INFO snapshot.SnapshotManager: SkipList is disabled\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,648 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,648 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,648 INFO util.GSet: 0.25% max memory 6.7 GB = 17.0 MB\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,649 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,655 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,655 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,655 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,658 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,659 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,660 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,660 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,661 INFO util.GSet: 0.029999999329447746% max memory 6.7 GB = 2.0 MB\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,661 INFO util.GSet: capacity      = 2^18 = 262144 entries\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,681 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1890955932-10.2.109.111-1596262719674\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,694 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,722 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,807 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,819 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,823 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:39,823 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.2.109.111\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34mStarting namenodes on [algo-1]\u001b[0m\n",
      "\u001b[34malgo-1: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34mStarting datanodes\u001b[0m\n",
      "\u001b[34mlocalhost: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34mStarting secondary namenodes [ip-10-2-109-111.ec2.internal]\u001b[0m\n",
      "\u001b[34mip-10-2-109-111.ec2.internal: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[35mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[35mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[35mWARNING: /usr/hadoop-3.2.1/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[35mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[35mWARNING: Use of this script to start YARN daemons is deprecated.\u001b[0m\n",
      "\u001b[35mWARNING: Attempting to execute replacement \"yarn --daemon start\" instead.\u001b[0m\n",
      "\u001b[35mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[35mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[34mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[34mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[34mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mStarting resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mStarting nodemanagers\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mlocalhost: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:52,428 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:53.340334: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:53.340420: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:53.340429: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\u001b[0m\n",
      "\u001b[34m2.1.0\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 232k/232k [00:00<00:00, 61.3MB/s]\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:54,590 INFO spark.SparkContext: Running Spark version 2.4.6\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:54,610 INFO spark.SparkContext: Submitted application: AmazonReviewsSparkProcessor\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:54,651 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:54,651 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:54,651 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:54,651 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:54,651 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:54,867 INFO util.Utils: Successfully started service 'sparkDriver' on port 36941.\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:54,891 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:54,906 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:54,909 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:54,909 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:54,916 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-d5e871ba-efa1-4397-a5ea-97e125a88c06\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:54,929 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:54,970 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,037 INFO util.log: Logging initialized @3674ms\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,085 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,096 INFO server.Server: Started @3734ms\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,109 INFO server.AbstractConnector: Started ServerConnector@679f0cbf{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,109 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,127 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1f5e9a7c{/jobs,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,128 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@48d657eb{/jobs/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,128 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64237d89{/jobs/job,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,131 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53607b33{/jobs/job/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,132 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cd09be3{/stages,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,132 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2156c2b7{/stages/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,133 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10d3b224{/stages/stage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,134 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7fc95a47{/stages/stage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,134 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@72069508{/stages/pool,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,135 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65fcec3c{/stages/pool/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,136 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@66963fac{/storage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,136 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53384a5b{/storage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,137 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@15d02be1{/storage/rdd,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,137 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@17cfa86b{/storage/rdd/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,138 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40739e61{/environment,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,139 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51087ea6{/environment/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,139 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4593c845{/executors,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,140 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f75ac6c{/executors/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,140 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@73a9af75{/executors/threadDump,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,141 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61c602e8{/executors/threadDump/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,146 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a4c3f75{/static,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,147 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3cd2cc80{/,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,148 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@265934e9{/api,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,148 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5cded15c{/jobs/job/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,149 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63b1af85{/stages/stage/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,150 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.2.109.111:4040\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:55,831 INFO client.RMProxy: Connecting to ResourceManager at /10.2.109.111:8032\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:56,105 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:56,168 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:56,168 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:56,182 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (31706 MB per container)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:56,183 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:56,183 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:56,186 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:56,191 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:56,239 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:57,040 INFO yarn.Client: Uploading resource file:/tmp/spark-1a27af0f-5eff-4c6f-be05-4168c2eaa25d/__spark_libs__7971246126962118357.zip -> hdfs://10.2.109.111/user/root/.sparkStaging/application_1596262730870_0001/__spark_libs__7971246126962118357.zip\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:57,441 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:58,219 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:58,446 INFO yarn.Client: Uploading resource file:/usr/spark-2.4.6/python/lib/pyspark.zip -> hdfs://10.2.109.111/user/root/.sparkStaging/application_1596262730870_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:58,455 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:58,473 INFO yarn.Client: Uploading resource file:/usr/spark-2.4.6/python/lib/py4j-0.10.7-src.zip -> hdfs://10.2.109.111/user/root/.sparkStaging/application_1596262730870_0001/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:58,481 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:58,618 INFO yarn.Client: Uploading resource file:/tmp/spark-1a27af0f-5eff-4c6f-be05-4168c2eaa25d/__spark_conf__62095150956359756.zip -> hdfs://10.2.109.111/user/root/.sparkStaging/application_1596262730870_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:58,627 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:58,664 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:58,664 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:58,664 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:58,664 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:58,664 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:59,479 INFO yarn.Client: Submitting application application_1596262730870_0001 to ResourceManager\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-08-01 06:18:59,691 INFO impl.YarnClientImpl: Submitted application application_1596262730870_0001\u001b[0m\n",
      "\u001b[34m2020-08-01 06:18:59,694 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1596262730870_0001 and attemptId None\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:00,704 INFO yarn.Client: Application report for application_1596262730870_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:00,707 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1596262739576\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1596262730870_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:01,709 INFO yarn.Client: Application report for application_1596262730870_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:02,711 INFO yarn.Client: Application report for application_1596262730870_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:03,714 INFO yarn.Client: Application report for application_1596262730870_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:04,717 INFO yarn.Client: Application report for application_1596262730870_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:04,717 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.2.75.179\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1596262739576\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1596262730870_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:04,718 INFO cluster.YarnClientSchedulerBackend: Application application_1596262730870_0001 has started running.\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:04,741 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43237.\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:04,742 INFO netty.NettyBlockTransferService: Server created on 10.2.109.111:43237\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:04,743 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:04,762 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.2.109.111, 43237, None)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:04,764 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.2.109.111:43237 with 366.3 MB RAM, BlockManagerId(driver, 10.2.109.111, 43237, None)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:04,767 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.2.109.111, 43237, None)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:04,767 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.2.109.111, 43237, None)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:04,881 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ba780ca{/metrics/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:04,919 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1596262730870_0001), /proxy/application_1596262730870_0001\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:05,123 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:08,087 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.2.75.179:40206) with ID 1\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:08,167 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-2:40239 with 11.9 GB RAM, BlockManagerId(1, algo-2, 40239, None)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:25,222 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:25,444 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/spark-2.4.6/spark-warehouse').\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:25,444 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-2.4.6/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:25,450 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:25,456 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2e5b706c{/SQL,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:25,457 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:25,457 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3aae8489{/SQL/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:25,458 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:25,458 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@698aeddc{/SQL/execution,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:25,459 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:25,459 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@546d7c48{/SQL/execution/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:25,460 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:25,461 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fb191e2{/static/sql,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:25,799 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-east-1-322537213286/amazon-reviews-pds/tsv/\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-08-01-06-14-52/output/bert-train\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-08-01-06-14-52/output/bert-validation\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-08-01-06-14-52/output/bert-test\u001b[0m\n",
      "\u001b[34mProcessing s3a://sagemaker-us-east-1-322537213286/amazon-reviews-pds/tsv/ => s3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-08-01-06-14-52/output/bert-train\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:25,988 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:26,039 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:26,039 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:27,384 INFO datasources.InMemoryFileIndex: It took 104 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:27,798 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:27,803 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:27,806 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:27,811 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:28,176 INFO codegen.CodeGenerator: Code generated in 213.91073 ms\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:28,372 INFO codegen.CodeGenerator: Code generated in 36.96832 ms\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:28,418 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 401.9 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:28,462 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 42.9 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:28,464 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.2.109.111:43237 (size: 42.9 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:28,465 INFO spark.SparkContext: Created broadcast 0 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:28,491 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 84137401 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:28,570 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:28,583 INFO scheduler.DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:28,584 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:28,584 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:28,585 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:28,588 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:28,657 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.3 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:28,660 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.3 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:28,660 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.2.109.111:43237 (size: 7.3 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:28,661 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:28,672 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:28,673 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:28,698 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:28,877 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-2:40239 (size: 7.3 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:29,422 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-2:40239 (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-08-01 06:19:31,312 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2622 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,314 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,319 INFO scheduler.DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 2.721 s\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,322 INFO scheduler.DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 2.752107 s\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34m|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|   product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34m|         US|   45610553| RMDCHWD0Y5OZ9|B00HH62VB6|     618218723|AGPtek® 10 Isolat...|Musical Instruments|          3|            0|          1|   N|                N|         Three Stars|Works very good, ...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   14640079| RZSL0BALIYUNU|B003LRN53I|     986692292|Sennheiser HD203 ...|Musical Instruments|          5|            0|          0|   N|                Y|          Five Stars|Nice headphones a...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    6111003| RIZR67JKUDBI0|B0006VMBHI|     603261968|AudioQuest LP rec...|Musical Instruments|          3|            0|          1|   N|                Y|         Three Stars|removes dust. doe...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    1546619|R27HL570VNL85F|B002B55TRG|     575084461|Hohner Inc. 560BX...|Musical Instruments|          5|            0|          0|   N|                Y|I purchase these ...|I purchase these ...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   12222213|R34EBU9QDWJ1GD|B00N1YPXW2|     165236328|Blue Yeti USB Mic...|Musical Instruments|          5|            0|          0|   N|                Y|          Five Stars|This is an awesom...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   46018513|R1WCUI4Z1SIQEO|B001N4GRGS|     134151483|Middle Atlantic P...|Musical Instruments|          5|            0|          0|   N|                N|          Five Stars|Used to cool equi...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   10225065| RL5LNO26GAVJ1|B009PJRMHQ|     694166585|Kmise 1pc Pickgua...|Musical Instruments|          2|            3|          4|   N|                Y|Will not Fit Epip...|Note- Does not Fi...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    6356995|R3GYQ5W8JHP8SB|B00NKBDAZS|     446431775|Kealoha Concert U...|Musical Instruments|          5|            0|          0|   N|                Y|          Five Stars|Well built Ukulel...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   35297198|R30SHYQXGG5EYC|B006MIU7U2|     125871705|Halco 80000 - MR1...|Musical Instruments|          5|            0|          0|   N|                Y|Works fine. Hope ...|Had to replace a ...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   32139520|R14YLXA56NP51I|B000FIBD0I|     771888534|Gator GPTBLACK Pl...|Musical Instruments|          5|            1|          1|   N|                N|I upgraded the po...|I've owned multip...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   36060782|R1ZH0HSH38IOTZ|B0002E52GG|      68535945|Hetman 1 - Light ...|Musical Instruments|          5|            0|          0|   N|                Y|My son's favourit...|Consistent qualit...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    5301309|R3H53KLLC210XI|B00RZIH52G|     725541773|Dragonpad pop fil...|Musical Instruments|          4|            0|          0|   N|                Y|Great pop filter ...|by far the best p...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   37472935|R3OOR877NGA8JK|B001792BAU|      46570323|DharmaObjects Rel...|Musical Instruments|          3|            0|          0|   N|                Y|                  Ok|Beautiful set. On...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   33578270|R1BY7WKOZ3KMH0|B009GSKW1Y|     547963417|Musiclily SSS Pla...|Musical Instruments|          2|            0|          0|   N|                Y|           Two Stars|Bridge pickup was...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22070226| RXP1TFSWE8EG9|B0002F4TKA|     436074323|Vic Firth America...|Musical Instruments|          5|            0|          0|   N|                Y|          Five Stars|Feels good and la...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   52862655|R3J44DPP12OTLJ|B00K17YFBW|      81933093|Guitar Stand for ...|Musical Instruments|          5|            0|          0|   N|                Y|Great stand... on...|I love the stand....| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    4427243| RFOV69SK0T676|B00EQ24HJS|     669249276|Generic 3PLY Faux...|Musical Instruments|          5|            0|          0|   N|                Y|Looks great. You ...|On time. Looks gr...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   14108571|R2HUWDNW62FOL3|B00IBOYTUE|     749537231|Audio 2000 6525 3...|Musical Instruments|          1|            0|          0|   N|                Y|  Poor sound quality|I was hoping it w...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   27314089|R1KSU30XZGR452|B00FBRUSAE|     792472601|Sawtooth ST-AMP-1...|Musical Instruments|          5|            0|          0|   N|                Y|Perfect for the b...|Good sound for it...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   16735445|R2TZVLLTSHA07N|B0113D2QUO|     269114019|Upado Unlimited G...|Musical Instruments|          5|            1|          1|   N|                Y|It really is a mu...|Wow! I didn't exp...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34mShowing null review_body rows...\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,406 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,407 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnull(review_body#13)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,408 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,410 INFO execution.FileSourceScanExec: Pushed Filters: IsNull(review_body)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,479 INFO codegen.CodeGenerator: Code generated in 28.71414 ms\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,487 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 401.9 KB, free 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,505 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 42.9 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,506 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.2.109.111:43237 (size: 42.9 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,507 INFO spark.SparkContext: Created broadcast 2 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,508 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 84137401 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,517 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,518 INFO scheduler.DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,519 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,519 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,519 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,519 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,524 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 16.6 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,526 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.4 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,526 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.2.109.111:43237 (size: 7.4 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,526 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,527 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,527 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,528 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,571 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-2:40239 (size: 7.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:31,667 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-2:40239 (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:32,870 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1342 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:32,870 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:32,871 INFO scheduler.DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 1.350 s\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:32,871 INFO scheduler.DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 1.353899 s\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+-----------+-----------+\u001b[0m\n",
      "\u001b[34m|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|   product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|review_body|review_date|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+-----------+-----------+\u001b[0m\n",
      "\u001b[34m|         US|   24889557|R1ALT15XW84SOW|B000RW81AM|     561741789|Stanton N500 Repl...|Musical Instruments|          5|            3|          3|   N|                Y| I mean my record...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|    4788420|R2MJ6IVQYBIGWB|B00Y7DB5ZA|     351956175|Scalze Mechanical...|Musical Instruments|          5|            0|          0|   N|                N|       Great Product|       null| 2015-08-24|\u001b[0m\n",
      "\u001b[34m|         US|   13756017|R1LY7OFXW7D83G|B00V39XDYW|     389322049|Strukture SSS57 S...|Musical Instruments|          1|            0|          1|   N|                Y|I think we have a...|       null| 2015-08-24|\u001b[0m\n",
      "\u001b[34m|         US|    1049988|R3IBRUXXKOD6HU|B008X040QE|     438050292|Surfing Blue Pear...|Musical Instruments|          5|            5|          5|   N|                Y|Should have done ...|       null| 2015-08-21|\u001b[0m\n",
      "\u001b[34m|         US|   41216360| RVVPQ9PKPSY9M|B00CFP5ESG|     954028768|Gator G-PG ACOUST...|Musical Instruments|          5|            1|          1|   N|                Y|          Five Stars|       null| 2015-08-18|\u001b[0m\n",
      "\u001b[34m|         US|    2291033| RH0M7W3FKAQE6|B008277N80|     412593952|Diamond Head Ukulele|Musical Instruments|          4|           21|         23|   N|                Y|          Four Stars|       null| 2015-08-15|\u001b[0m\n",
      "\u001b[34m|         US|   16594880|R39WV41UT37NOV|B000DLAO0C|     695688148|Jargar 4/4 Cello ...|Musical Instruments|          5|            0|          0|   N|                Y|          Five Stars|       null| 2015-08-15|\u001b[0m\n",
      "\u001b[34m|         US|    2000949|R16YVBT2IJDGSA|B000P9W5QI|     328210138|GP Percussion GP5...|Musical Instruments|          5|            3|          3|   N|                Y|          Five Stars|       null| 2015-08-14|\u001b[0m\n",
      "\u001b[34m|         US|    3991529|R1D7QB14L9UM0P|B00NKBDAZS|     446431775|Kealoha Concert U...|Musical Instruments|          5|            6|          6|   N|                Y|          Five Stars|       null| 2015-08-11|\u001b[0m\n",
      "\u001b[34m|         US|   20379821|R3HHDM1WQGE59V|B002DP594W|     153828378|Shure Professiona...|Musical Instruments|          4|            2|          3|   N|                Y|          Four Stars|       null| 2015-08-09|\u001b[0m\n",
      "\u001b[34m|         US|   47810643|R1HMENPQD85LN1|B00MO6KKSK|     308823533|Donner Dt-1 Chrom...|Musical Instruments|          5|            1|          1|   N|                Y|          Five Stars|       null| 2015-08-09|\u001b[0m\n",
      "\u001b[34m|         US|    2713246|R1NDQRG6NQW4LB|B00DS4OMUY|     613112255|Generic Amzdeal M...|Musical Instruments|          5|           22|         26|   N|                Y|          Five Stars|       null| 2015-08-08|\u001b[0m\n",
      "\u001b[34m|         US|    1025635|R2JC9Y0TV73V3T|B00KWOX51K|     412419029|Pro Tec SWPB2 Sto...|Musical Instruments|          4|           16|         17|   N|                N|          Four Stars|       null| 2015-08-06|\u001b[0m\n",
      "\u001b[34m|         US|     244839|R2YC4WBS1L2XCL|B00P63ZY2U|     378733816|WOWTOU LED Stage ...|Musical Instruments|          5|            2|          3|   N|                Y| Its very bright ...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   37622146|R369T16AY1NHCF|B00P08R23A|     642176027|Glory Black Resin...|Musical Instruments|          5|            9|          9|   N|                Y|Good piccolo. Not...|       null| 2015-07-30|\u001b[0m\n",
      "\u001b[34m|         US|   21145093| RJE12YGTJSEK3|B00I3XG5C8|     638790804|Epiphone CASINO C...|Musical Instruments|          5|            2|          3|   N|                Y|Not just a great ...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|    2562323|R194MQJW0BZTAT|B0007LCKPU|     191392034|PYLE-PRO PADH1079...|Musical Instruments|          5|            4|          4|   N|                Y|          Five Stars|       null| 2015-07-20|\u001b[0m\n",
      "\u001b[34m|         US|   14703927|R1F2ZEGR2NB7G0|B005MR6IHK|     675226467|Fender FT-004 Chr...|Musical Instruments|          5|            6|          6|   N|                Y|works great but w...|       null| 2015-07-18|\u001b[0m\n",
      "\u001b[34m|         US|   50584198| RVSWCRTL9MUHM|B000VBH2IG|     714474785|Zoom H2 Handy Por...|Musical Instruments|          4|            0|          0|   N|                Y| The quality is p...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|     362137|R18BYWE28FZF4U|B00KC4JMAS|     768480778|Audio-Technica AT...|Musical Instruments|          5|            0|          0|   N|                Y|          Five Stars|       null| 2015-07-10|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+-----------+-----------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34mShowing cleaned csv\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:32,924 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:32,925 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:32,925 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:32,926 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:32,987 INFO codegen.CodeGenerator: Code generated in 26.13047 ms\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:32,993 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 401.9 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,014 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 42.9 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,016 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.2.109.111:43237 (size: 42.9 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,017 INFO spark.SparkContext: Created broadcast 4 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,017 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 84137401 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,026 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,027 INFO scheduler.DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,027 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,027 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,028 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,028 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,033 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 16.8 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,035 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.5 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,035 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.2.109.111:43237 (size: 7.5 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,036 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,037 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,037 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,038 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,052 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-2:40239 (size: 7.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,103 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-2:40239 (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,219 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 181 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,220 INFO scheduler.DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.190 s\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,220 INFO scheduler.DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.194195 s\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,220 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34m|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|   product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34m|         US|   45610553| RMDCHWD0Y5OZ9|B00HH62VB6|     618218723|AGPtek® 10 Isolat...|Musical Instruments|          3|            0|          1|   N|                N|         Three Stars|Works very good, ...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   14640079| RZSL0BALIYUNU|B003LRN53I|     986692292|Sennheiser HD203 ...|Musical Instruments|          5|            0|          0|   N|                Y|          Five Stars|Nice headphones a...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    6111003| RIZR67JKUDBI0|B0006VMBHI|     603261968|AudioQuest LP rec...|Musical Instruments|          3|            0|          1|   N|                Y|         Three Stars|removes dust. doe...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    1546619|R27HL570VNL85F|B002B55TRG|     575084461|Hohner Inc. 560BX...|Musical Instruments|          5|            0|          0|   N|                Y|I purchase these ...|I purchase these ...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   12222213|R34EBU9QDWJ1GD|B00N1YPXW2|     165236328|Blue Yeti USB Mic...|Musical Instruments|          5|            0|          0|   N|                Y|          Five Stars|This is an awesom...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   46018513|R1WCUI4Z1SIQEO|B001N4GRGS|     134151483|Middle Atlantic P...|Musical Instruments|          5|            0|          0|   N|                N|          Five Stars|Used to cool equi...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   10225065| RL5LNO26GAVJ1|B009PJRMHQ|     694166585|Kmise 1pc Pickgua...|Musical Instruments|          2|            3|          4|   N|                Y|Will not Fit Epip...|Note- Does not Fi...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    6356995|R3GYQ5W8JHP8SB|B00NKBDAZS|     446431775|Kealoha Concert U...|Musical Instruments|          5|            0|          0|   N|                Y|          Five Stars|Well built Ukulel...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   35297198|R30SHYQXGG5EYC|B006MIU7U2|     125871705|Halco 80000 - MR1...|Musical Instruments|          5|            0|          0|   N|                Y|Works fine. Hope ...|Had to replace a ...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   32139520|R14YLXA56NP51I|B000FIBD0I|     771888534|Gator GPTBLACK Pl...|Musical Instruments|          5|            1|          1|   N|                N|I upgraded the po...|I've owned multip...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   36060782|R1ZH0HSH38IOTZ|B0002E52GG|      68535945|Hetman 1 - Light ...|Musical Instruments|          5|            0|          0|   N|                Y|My son's favourit...|Consistent qualit...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    5301309|R3H53KLLC210XI|B00RZIH52G|     725541773|Dragonpad pop fil...|Musical Instruments|          4|            0|          0|   N|                Y|Great pop filter ...|by far the best p...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   37472935|R3OOR877NGA8JK|B001792BAU|      46570323|DharmaObjects Rel...|Musical Instruments|          3|            0|          0|   N|                Y|                  Ok|Beautiful set. On...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   33578270|R1BY7WKOZ3KMH0|B009GSKW1Y|     547963417|Musiclily SSS Pla...|Musical Instruments|          2|            0|          0|   N|                Y|           Two Stars|Bridge pickup was...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22070226| RXP1TFSWE8EG9|B0002F4TKA|     436074323|Vic Firth America...|Musical Instruments|          5|            0|          0|   N|                Y|          Five Stars|Feels good and la...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   52862655|R3J44DPP12OTLJ|B00K17YFBW|      81933093|Guitar Stand for ...|Musical Instruments|          5|            0|          0|   N|                Y|Great stand... on...|I love the stand....| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    4427243| RFOV69SK0T676|B00EQ24HJS|     669249276|Generic 3PLY Faux...|Musical Instruments|          5|            0|          0|   N|                Y|Looks great. You ...|On time. Looks gr...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   14108571|R2HUWDNW62FOL3|B00IBOYTUE|     749537231|Audio 2000 6525 3...|Musical Instruments|          1|            0|          0|   N|                Y|  Poor sound quality|I was hoping it w...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   27314089|R1KSU30XZGR452|B00FBRUSAE|     792472601|Sawtooth ST-AMP-1...|Musical Instruments|          5|            0|          0|   N|                Y|Perfect for the b...|Good sound for it...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   16735445|R2TZVLLTSHA07N|B0113D2QUO|     269114019|Upado Unlimited G...|Musical Instruments|          5|            1|          1|   N|                Y|It really is a mu...|Wow! I didn't exp...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,288 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,288 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,289 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,289 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,297 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.2.109.111:43237 in memory (size: 42.9 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,306 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-2:40239 in memory (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,317 INFO codegen.CodeGenerator: Code generated in 13.727489 ms\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,321 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.2.109.111:43237 in memory (size: 7.3 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,321 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-2:40239 in memory (size: 7.3 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,339 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on algo-2:40239 in memory (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,341 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.2.109.111:43237 in memory (size: 42.9 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,343 INFO codegen.CodeGenerator: Code generated in 20.727718 ms\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,348 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 401.9 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,359 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.2.109.111:43237 in memory (size: 7.4 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,361 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-2:40239 in memory (size: 7.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,373 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-2:40239 in memory (size: 7.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,374 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 42.9 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,375 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.2.109.111:43237 in memory (size: 7.5 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,377 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.2.109.111:43237 (size: 42.9 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,378 INFO spark.SparkContext: Created broadcast 6 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,378 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 84137401 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,385 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,389 INFO scheduler.DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,389 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,389 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,389 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,389 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,394 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-2:40239 in memory (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,398 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.2.109.111:43237 in memory (size: 42.9 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,400 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 12.2 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,401 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.4 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,401 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.2.109.111:43237 (size: 6.4 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,402 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,402 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,402 INFO cluster.YarnScheduler: Adding task set 3.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,403 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,412 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-2:40239 (size: 6.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,453 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-2:40239 (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,552 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 149 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,552 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,553 INFO scheduler.DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.163 s\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:33,554 INFO scheduler.DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0.167940 s\u001b[0m\n",
      "\u001b[34m+-----------+--------------------+\u001b[0m\n",
      "\u001b[34m|star_rating|         review_body|\u001b[0m\n",
      "\u001b[34m+-----------+--------------------+\u001b[0m\n",
      "\u001b[34m|          3|Works very good, ...|\u001b[0m\n",
      "\u001b[34m|          5|Nice headphones a...|\u001b[0m\n",
      "\u001b[34m|          3|removes dust. doe...|\u001b[0m\n",
      "\u001b[34m|          5|I purchase these ...|\u001b[0m\n",
      "\u001b[34m|          5|This is an awesom...|\u001b[0m\n",
      "\u001b[34m|          5|Used to cool equi...|\u001b[0m\n",
      "\u001b[34m|          2|Note- Does not Fi...|\u001b[0m\n",
      "\u001b[34m|          5|Well built Ukulel...|\u001b[0m\n",
      "\u001b[34m|          5|Had to replace a ...|\u001b[0m\n",
      "\u001b[34m|          5|I've owned multip...|\u001b[0m\n",
      "\u001b[34m|          5|Consistent qualit...|\u001b[0m\n",
      "\u001b[34m|          4|by far the best p...|\u001b[0m\n",
      "\u001b[34m|          3|Beautiful set. On...|\u001b[0m\n",
      "\u001b[34m|          2|Bridge pickup was...|\u001b[0m\n",
      "\u001b[34m|          5|Feels good and la...|\u001b[0m\n",
      "\u001b[34m|          5|I love the stand....|\u001b[0m\n",
      "\u001b[34m|          5|On time. Looks gr...|\u001b[0m\n",
      "\u001b[34m|          1|I was hoping it w...|\u001b[0m\n",
      "\u001b[34m|          5|Good sound for it...|\u001b[0m\n",
      "\u001b[34m|          5|Wow! I didn't exp...|\u001b[0m\n",
      "\u001b[34m+-----------+--------------------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,153 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,153 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,154 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,154 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,175 INFO codegen.CodeGenerator: Code generated in 8.394908 ms\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,204 INFO codegen.CodeGenerator: Code generated in 20.099889 ms\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,215 INFO codegen.CodeGenerator: Code generated in 7.704016 ms\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,220 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 401.9 KB, free 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,236 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 42.9 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,237 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.2.109.111:43237 (size: 42.9 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,238 INFO spark.SparkContext: Created broadcast 8 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,238 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 84137401 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,294 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,295 INFO scheduler.DAGScheduler: Got job 4 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,295 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,295 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,295 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,295 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[22] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,301 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 849.5 KB, free 364.6 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,305 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 644.8 KB, free 364.0 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,306 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.2.109.111:43237 (size: 644.8 KB, free: 365.6 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,307 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,308 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[22] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,308 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,309 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,323 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-2:40239 (size: 644.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:34,804 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-2:40239 (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,586 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 2277 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,587 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,588 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 50503\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,589 INFO scheduler.DAGScheduler: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0) finished in 2.293 s\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,589 INFO scheduler.DAGScheduler: Job 4 finished: showString at NativeMethodAccessorImpl.java:0, took 2.295664 s\u001b[0m\n",
      "\u001b[34m+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34m|tfrecords                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\u001b[0m\n",
      "\u001b[34m+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34m|[[101, 2573, 2200, 2204, 1010, 2021, 19653, 2015, 2632, 4140, 1997, 5005, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2]]                                                                                                                                                                                                                                                                                                                                                         |\u001b[0m\n",
      "\u001b[34m|[[101, 3835, 2132, 19093, 2012, 1037, 9608, 3976, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                     |\u001b[0m\n",
      "\u001b[34m|[[101, 20362, 6497, 1012, 2515, 2025, 4550, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2]]                                                                                                                                                                                                                                                                                                                                                                           |\u001b[0m\n",
      "\u001b[34m|[[101, 1045, 5309, 2122, 2005, 1037, 2767, 1999, 2709, 2005, 2652, 2068, 2005, 2026, 2269, 1998, 2060, 12455, 2012, 1037, 2334, 5075, 2188, 1012, 4067, 2017, 1054, 1012, 24869, 16523, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                    |\u001b[0m\n",
      "\u001b[34m|[[101, 2023, 2003, 2019, 12476, 23025, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                           |\u001b[0m\n",
      "\u001b[34m|[[101, 2109, 2000, 4658, 3941, 2503, 13675, 14728, 16786, 2015, 1012, 2499, 2307, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                    |\u001b[0m\n",
      "\u001b[34m|[[101, 3602, 1011, 2515, 2025, 4906, 4958, 11514, 27406, 22214, 2569, 999, 3067, 2003, 12800, 2526, 2944, 1998, 2009, 2097, 2025, 6039, 1996, 2686, 2090, 1996, 14910, 24204, 2545, 3294, 1012, 2673, 2842, 2003, 2307, 1998, 2204, 7477, 21530, 4728, 1012, 1045, 2018, 2000, 5587, 2026, 2219, 7661, 2081, 6039, 2121, 5127, 2000, 3143, 1996, 3857, 1012, 1045, 2031, 2025, 2042, 2583, 2000, 2424, 2023, 2806, 4060, 3457, 2008, 2097, 4906, 2026, 2858, 2007, 5372, 12403, 6129, 2090, 14910, 24204, 2545, 2302, 1037, 1004, 1001, 4090, 1025, 7661, 1004, 1001, 4090, 1025, 3976, 1998, 2383, 2000, 4604, 2115, 2858, 2041, 1012, 1026, 7987, 1013, 1028, 2204, 3976, 1010, 2021, 2022, 4810, 2000, 2079, 2070, 7661, 2147, 4426, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1]]                       |\u001b[0m\n",
      "\u001b[34m|[[101, 2092, 2328, 2866, 9307, 2571, 999, 2026, 2684, 7459, 2009, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                               |\u001b[0m\n",
      "\u001b[34m|[[101, 2018, 2000, 5672, 1037, 2047, 2422, 2044, 1037, 7407, 4040, 1012, 2573, 2986, 1012, 3246, 2009, 16180, 2083, 1996, 10943, 2100, 2558, 2023, 2051, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                  |\u001b[0m\n",
      "\u001b[34m|[[101, 1045, 1005, 2310, 3079, 3674, 4964, 7923, 2058, 1996, 2086, 1010, 2021, 1045, 2172, 2738, 6293, 2007, 3265, 3466, 3197, 1008, 1008, 1008, 1045, 1005, 2310, 4156, 1037, 2261, 4064, 7923, 1998, 2027, 6357, 2039, 4634, 4237, 1010, 2059, 2009, 1005, 1055, 2784, 2416, 2051, 1008, 1008, 1008, 2026, 2567, 1011, 1999, 1011, 2375, 4156, 1996, 14246, 2102, 16558, 28400, 2099, 2004, 1037, 1060, 1011, 3742, 5592, 1019, 2086, 3283, 1010, 1998, 2172, 2000, 2026, 4474, 1010, 2009, 1005, 1055, 2145, 10209, 4632, 1008, 1008, 1008, 1045, 1005, 2310, 3333, 2009, 1037, 2261, 2335, 1010, 2009, 1005, 1055, 2042, 9554, 3706, 2006, 1037, 2261, 2335, 1010, 6476, 2105, 3365, 2335, 1010, 2021, 2044, 2035, 2023, 6905, 1010, 2016, 1005, 1055, 2145, 2600, 2378, 1005, 2004, 2524, 102], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]|\u001b[0m\n",
      "\u001b[34m|[[101, 8335, 3737, 1010, 2053, 2844, 1051, 26797, 2869, 1010, 2573, 2092, 1012, 2026, 2365, 1005, 1055, 8837, 9368, 10764, 3514, 1012, 2002, 2003, 1037, 3809, 2447, 1998, 2023, 2003, 2036, 2054, 2010, 3836, 1006, 1037, 2658, 2102, 19379, 22327, 2121, 1007, 3594, 1012, 2009, 1005, 1055, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                |\u001b[0m\n",
      "\u001b[34m|[[101, 2011, 2521, 1996, 2190, 3769, 11307, 1045, 2031, 2109, 1010, 5186, 4621, 1010, 1045, 2106, 2025, 6065, 2151, 2250, 2012, 2035, 1010, 2026, 2069, 3291, 2003, 2008, 1996, 13020, 1011, 3300, 1998, 15986, 18856, 16613, 2024, 10036, 1998, 2524, 2000, 14171, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3]]                                                                                                                                                                                                                                                            |\u001b[0m\n",
      "\u001b[34m|[[101, 3376, 2275, 1012, 2069, 1996, 2614, 2025, 2012, 2146, 1998, 26709, 2361, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2]]                                                                                                                                                                                                                                                                                                                                                      |\u001b[0m\n",
      "\u001b[34m|[[101, 2958, 15373, 2001, 3714, 1012, 1045, 5672, 1040, 1996, 15373, 1998, 7929, 2085, 1012, 2000, 10036, 2000, 4604, 2067, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1]]                                                                                                                                                                                                                                                                                                                               |\u001b[0m\n",
      "\u001b[34m|[[101, 5683, 2204, 1998, 2197, 2146, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                               |\u001b[0m\n",
      "\u001b[34m|[[101, 1045, 2293, 1996, 3233, 1012, 1045, 4149, 2048, 1012, 1996, 2069, 1043, 15909, 2818, 2003, 2008, 2043, 1045, 4987, 1037, 5830, 2000, 2026, 2858, 1010, 1996, 5830, 1005, 1055, 13354, 2718, 1996, 2723, 2043, 1996, 2858, 2001, 1999, 1996, 3233, 1012, 1037, 2157, 6466, 13354, 13332, 1996, 3291, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                       |\u001b[0m\n",
      "\u001b[34m|[[101, 2006, 2051, 1012, 3504, 2307, 1012, 2017, 2031, 2000, 3424, 6895, 17585, 15135, 2047, 11224, 19990, 2043, 2017, 5672, 1037, 4060, 18405, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                     |\u001b[0m\n",
      "\u001b[34m|[[101, 1045, 2001, 5327, 2009, 2052, 2147, 2092, 1010, 2021, 2699, 1037, 2117, 11601, 1998, 2052, 2069, 4374, 1037, 2843, 1997, 10763, 1999, 2119, 5363, 1012, 2001, 2196, 2583, 2000, 2131, 3154, 2614, 2012, 2035, 1998, 2699, 3048, 2185, 2013, 2151, 2825, 11099, 4385, 1012, 6410, 10858, 2151, 2785, 1997, 4390, 5008, 1998, 3784, 2001, 11158, 2004, 2092, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0]]                                                                                                                                                                                                           |\u001b[0m\n",
      "\u001b[34m|[[101, 2204, 2614, 2005, 2049, 2946, 1998, 3976, 1012, 2307, 1997, 1037, 2402, 2858, 3076, 1996, 4553, 2006, 1059, 1013, 1051, 4911, 1996, 2924, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                      |\u001b[0m\n",
      "\u001b[34m|[[101, 10166, 999, 1045, 2134, 1005, 1056, 5987, 1996, 3737, 1998, 2046, 9323, 1997, 2023, 4031, 2000, 2022, 2061, 2152, 1012, 2009, 2428, 2003, 1037, 3315, 6602, 1998, 1996, 2338, 2003, 1037, 10392, 2126, 2000, 4088, 1012, 4283, 2005, 1996, 16465, 2791, 2008, 2253, 2046, 2023, 4031, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                |\u001b[0m\n",
      "\u001b[34m+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,675 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,675 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,675 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,675 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,698 INFO codegen.CodeGenerator: Code generated in 9.817036 ms\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,724 INFO codegen.CodeGenerator: Code generated in 18.075587 ms\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,733 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 401.9 KB, free 363.6 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,759 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 42.9 KB, free 363.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,760 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.2.109.111:43237 (size: 42.9 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,761 INFO spark.SparkContext: Created broadcast 10 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,761 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 84137401 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,787 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,789 INFO scheduler.DAGScheduler: Got job 5 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,789 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,790 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,790 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,790 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,797 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 849.1 KB, free 362.7 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,802 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 644.8 KB, free 362.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,803 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.2.109.111:43237 (size: 644.8 KB, free: 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,803 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,804 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,804 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,805 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,817 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-2:40239 (size: 644.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:36,845 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-2:40239 (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:38,993 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 2188 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:38,993 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:38,994 INFO scheduler.DAGScheduler: ResultStage 5 (showString at NativeMethodAccessorImpl.java:0) finished in 2.202 s\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:38,995 INFO scheduler.DAGScheduler: Job 5 finished: showString at NativeMethodAccessorImpl.java:0, took 2.207512 s\u001b[0m\n",
      "\u001b[34m+--------------------+--------------------+--------------------+---------+\u001b[0m\n",
      "\u001b[34m|           input_ids|          input_mask|         segment_ids|label_ids|\u001b[0m\n",
      "\u001b[34m+--------------------+--------------------+--------------------+---------+\u001b[0m\n",
      "\u001b[34m|[101, 2573, 2200,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [2]|\u001b[0m\n",
      "\u001b[34m|[101, 3835, 2132,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 20362, 6497...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [2]|\u001b[0m\n",
      "\u001b[34m|[101, 1045, 5309,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 2023, 2003,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 2109, 2000,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 3602, 1011,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [1]|\u001b[0m\n",
      "\u001b[34m|[101, 2092, 2328,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 2018, 2000,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 1045, 1005,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 8335, 3737,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 2011, 2521,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [3]|\u001b[0m\n",
      "\u001b[34m|[101, 3376, 2275,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [2]|\u001b[0m\n",
      "\u001b[34m|[101, 2958, 15373...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [1]|\u001b[0m\n",
      "\u001b[34m|[101, 5683, 2204,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 1045, 2293,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 2006, 2051,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 1045, 2001,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [0]|\u001b[0m\n",
      "\u001b[34m|[101, 2204, 2614,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 10166, 999,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m+--------------------+--------------------+--------------------+---------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,134 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-2:40239 in memory (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,140 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.2.109.111:43237 in memory (size: 42.9 KB, free: 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,153 INFO spark.ContextCleaner: Cleaned accumulator 184\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,153 INFO spark.ContextCleaner: Cleaned accumulator 176\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,153 INFO spark.ContextCleaner: Cleaned accumulator 166\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,153 INFO spark.ContextCleaner: Cleaned accumulator 172\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,153 INFO spark.ContextCleaner: Cleaned accumulator 167\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,153 INFO spark.ContextCleaner: Cleaned accumulator 168\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,153 INFO spark.ContextCleaner: Cleaned accumulator 169\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,153 INFO spark.ContextCleaner: Cleaned accumulator 163\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,153 INFO spark.ContextCleaner: Cleaned accumulator 171\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,153 INFO spark.ContextCleaner: Cleaned accumulator 175\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,153 INFO spark.ContextCleaner: Cleaned accumulator 174\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,153 INFO spark.ContextCleaner: Cleaned accumulator 181\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,153 INFO spark.ContextCleaner: Cleaned accumulator 186\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,153 INFO spark.ContextCleaner: Cleaned accumulator 170\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,153 INFO spark.ContextCleaner: Cleaned accumulator 177\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,153 INFO spark.ContextCleaner: Cleaned accumulator 185\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,153 INFO spark.ContextCleaner: Cleaned accumulator 178\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,153 INFO spark.ContextCleaner: Cleaned accumulator 180\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,153 INFO spark.ContextCleaner: Cleaned accumulator 182\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,153 INFO spark.ContextCleaner: Cleaned accumulator 164\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,153 INFO spark.ContextCleaner: Cleaned accumulator 173\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,153 INFO spark.ContextCleaner: Cleaned accumulator 179\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,153 INFO spark.ContextCleaner: Cleaned accumulator 183\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,153 INFO spark.ContextCleaner: Cleaned accumulator 165\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,156 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.2.109.111:43237 in memory (size: 644.8 KB, free: 365.6 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,157 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-2:40239 in memory (size: 644.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,172 INFO spark.ContextCleaner: Cleaned accumulator 187\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,172 INFO spark.ContextCleaner: Cleaned accumulator 93\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,172 INFO spark.ContextCleaner: Cleaned accumulator 7\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,172 INFO spark.ContextCleaner: Cleaned accumulator 23\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,172 INFO spark.ContextCleaner: Cleaned accumulator 89\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,172 INFO spark.ContextCleaner: Cleaned accumulator 131\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,172 INFO spark.ContextCleaner: Cleaned accumulator 114\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,172 INFO spark.ContextCleaner: Cleaned accumulator 122\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,172 INFO spark.ContextCleaner: Cleaned accumulator 19\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,172 INFO spark.ContextCleaner: Cleaned accumulator 86\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,175 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-2:40239 in memory (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,185 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.2.109.111:43237 in memory (size: 42.9 KB, free: 365.6 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 60\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 146\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 111\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 99\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 58\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 151\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 5\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 136\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 123\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 138\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 119\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 134\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 62\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 51\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 112\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 110\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 74\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 46\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 43\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 118\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 61\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 135\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 98\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 12\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 153\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 105\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 6\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 10\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 124\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 14\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 97\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 85\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,187 INFO spark.ContextCleaner: Cleaned accumulator 88\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 70\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 117\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 38\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 95\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 132\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 108\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 142\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 15\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 2\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 91\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 68\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 55\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 159\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 28\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 126\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 29\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 66\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 24\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 92\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 133\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 145\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 64\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 94\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 72\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 11\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 128\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 78\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 73\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 69\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 40\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 130\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 27\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 54\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 157\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 121\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 39\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 80\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 109\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 42\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 9\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 30\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 44\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 36\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 102\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 67\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 160\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 17\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 103\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 56\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 16\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,188 INFO spark.ContextCleaner: Cleaned accumulator 35\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,189 INFO spark.ContextCleaner: Cleaned accumulator 76\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,189 INFO spark.ContextCleaner: Cleaned accumulator 87\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,189 INFO spark.ContextCleaner: Cleaned accumulator 18\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,189 INFO spark.ContextCleaner: Cleaned accumulator 148\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,189 INFO spark.ContextCleaner: Cleaned accumulator 147\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,191 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-2:40239 in memory (size: 644.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,193 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.2.109.111:43237 in memory (size: 644.8 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,213 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,214 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,214 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,214 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,227 INFO spark.ContextCleaner: Cleaned accumulator 31\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,227 INFO spark.ContextCleaner: Cleaned accumulator 140\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,227 INFO spark.ContextCleaner: Cleaned accumulator 100\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,227 INFO spark.ContextCleaner: Cleaned accumulator 77\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,228 INFO spark.ContextCleaner: Cleaned accumulator 50\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,228 INFO spark.ContextCleaner: Cleaned accumulator 71\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,228 INFO spark.ContextCleaner: Cleaned accumulator 116\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,231 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-2:40239 in memory (size: 6.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,242 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.2.109.111:43237 in memory (size: 6.4 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,252 INFO spark.ContextCleaner: Cleaned accumulator 81\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,253 INFO spark.ContextCleaner: Cleaned accumulator 59\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,253 INFO spark.ContextCleaner: Cleaned accumulator 90\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,253 INFO spark.ContextCleaner: Cleaned accumulator 65\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,253 INFO spark.ContextCleaner: Cleaned accumulator 21\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,254 INFO spark.ContextCleaner: Cleaned accumulator 13\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,254 INFO spark.ContextCleaner: Cleaned accumulator 152\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,257 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.2.109.111:43237 in memory (size: 42.9 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,258 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-2:40239 in memory (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,261 INFO spark.ContextCleaner: Cleaned accumulator 101\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,261 INFO spark.ContextCleaner: Cleaned accumulator 41\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,261 INFO spark.ContextCleaner: Cleaned accumulator 161\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,261 INFO spark.ContextCleaner: Cleaned accumulator 155\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,261 INFO spark.ContextCleaner: Cleaned accumulator 137\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,261 INFO spark.ContextCleaner: Cleaned accumulator 149\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,261 INFO spark.ContextCleaner: Cleaned accumulator 156\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,261 INFO spark.ContextCleaner: Cleaned accumulator 83\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,261 INFO spark.ContextCleaner: Cleaned accumulator 4\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,261 INFO spark.ContextCleaner: Cleaned accumulator 1\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,261 INFO spark.ContextCleaner: Cleaned accumulator 104\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,261 INFO spark.ContextCleaner: Cleaned accumulator 144\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,261 INFO spark.ContextCleaner: Cleaned accumulator 75\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,261 INFO spark.ContextCleaner: Cleaned accumulator 32\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,261 INFO spark.ContextCleaner: Cleaned accumulator 120\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,261 INFO spark.ContextCleaner: Cleaned accumulator 52\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,261 INFO spark.ContextCleaner: Cleaned accumulator 47\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,261 INFO spark.ContextCleaner: Cleaned accumulator 139\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 127\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 129\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 33\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 34\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 63\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 96\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 143\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 84\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 22\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 26\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 57\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 79\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 115\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 107\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 20\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 37\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 150\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 141\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 8\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 45\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 154\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 49\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 162\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 53\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 125\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 82\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 158\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 48\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 113\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 106\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 3\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,262 INFO spark.ContextCleaner: Cleaned accumulator 25\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,361 WARN commit.AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,361 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,362 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,362 INFO commit.AbstractS3ACommitterFactory: Using Commmitter FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20200801061939_0000}; taskId=attempt_20200801061939_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@32ea6ef4}; outputPath=s3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-08-01-06-14-52/output/bert-train, workPath=s3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-08-01-06-14-52/output/bert-train/_temporary/0/_temporary/attempt_20200801061939_0000_m_000000_0, algorithmVersion=2, skipCleanup=false, ignoreCleanupFailures=false} for s3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-08-01-06-14-52/output/bert-train\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:39,362 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:40,142 INFO codegen.CodeGenerator: Code generated in 34.660198 ms\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:40,150 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 401.9 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:40,166 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 42.9 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:40,167 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.2.109.111:43237 (size: 42.9 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:40,168 INFO spark.SparkContext: Created broadcast 12 from save at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:40,169 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 84137401 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:40,237 INFO spark.SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:40,238 INFO scheduler.DAGScheduler: Got job 6 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:40,238 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (save at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:40,238 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:40,238 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:40,239 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[34] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:40,271 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 1096.1 KB, free 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:40,278 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 734.5 KB, free 364.1 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:40,279 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.2.109.111:43237 (size: 734.5 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:40,280 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:40,280 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 6 (MapPartitionsRDD[34] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:40,281 INFO cluster.YarnScheduler: Adding task set 6.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:40,282 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:40,282 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 7, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8478 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:40,295 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-2:40239 (size: 734.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 06:19:40,397 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-2:40239 (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-08-01 06:42:28,361 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 7) in 1368079 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:41,171 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 4500890 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:41,171 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:41,173 INFO scheduler.DAGScheduler: ResultStage 6 (save at NativeMethodAccessorImpl.java:0) finished in 4500.933 s\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:41,173 INFO scheduler.DAGScheduler: Job 6 finished: save at NativeMethodAccessorImpl.java:0, took 4500.935914 s\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:41,905 INFO datasources.FileFormatWriter: Write Job c88f19af-c3b4-409c-b311-a7612bc49853 committed.\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:41,909 INFO datasources.FileFormatWriter: Finished processing stats for write job c88f19af-c3b4-409c-b311-a7612bc49853.\u001b[0m\n",
      "\u001b[34mWrote to output file:  s3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-08-01-06-14-52/output/bert-train\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:41,959 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:41,959 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:41,960 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:41,960 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,014 WARN commit.AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,014 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,014 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,014 INFO commit.AbstractS3ACommitterFactory: Using Commmitter FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20200801073442_0000}; taskId=attempt_20200801073442_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@a397d31}; outputPath=s3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-08-01-06-14-52/output/bert-validation, workPath=s3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-08-01-06-14-52/output/bert-validation/_temporary/0/_temporary/attempt_20200801073442_0000_m_000000_0, algorithmVersion=2, skipCleanup=false, ignoreCleanupFailures=false} for s3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-08-01-06-14-52/output/bert-validation\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,014 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,340 INFO codegen.CodeGenerator: Code generated in 31.549198 ms\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,346 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 401.9 KB, free 363.7 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,363 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 42.9 KB, free 363.6 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,364 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.2.109.111:43237 (size: 42.9 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,365 INFO spark.SparkContext: Created broadcast 14 from save at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,365 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 84137401 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,460 INFO spark.SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,460 INFO scheduler.DAGScheduler: Got job 7 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,461 INFO scheduler.DAGScheduler: Final stage: ResultStage 7 (save at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,461 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,461 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,461 INFO scheduler.DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[41] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,488 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 1096.2 KB, free 362.6 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,493 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 734.5 KB, free 361.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,493 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.2.109.111:43237 (size: 734.5 KB, free: 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,493 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,496 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[41] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,496 INFO cluster.YarnScheduler: Adding task set 7.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,499 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 8, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,499 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 7.0 (TID 9, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8478 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,512 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-2:40239 (size: 734.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 07:34:42,585 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-2:40239 (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,090 INFO spark.ContextCleaner: Cleaned accumulator 192\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,090 INFO spark.ContextCleaner: Cleaned accumulator 206\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,090 INFO spark.ContextCleaner: Cleaned accumulator 212\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,090 INFO spark.ContextCleaner: Cleaned accumulator 199\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,090 INFO spark.ContextCleaner: Cleaned accumulator 197\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,090 INFO spark.ContextCleaner: Cleaned accumulator 224\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,090 INFO spark.ContextCleaner: Cleaned accumulator 211\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,097 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.2.109.111:43237 in memory (size: 42.9 KB, free: 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,110 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-2:40239 in memory (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,113 INFO spark.ContextCleaner: Cleaned accumulator 215\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,113 INFO spark.ContextCleaner: Cleaned accumulator 209\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,113 INFO spark.ContextCleaner: Cleaned accumulator 191\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,113 INFO spark.ContextCleaner: Cleaned accumulator 204\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,114 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.2.109.111:43237 in memory (size: 734.5 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,115 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-2:40239 in memory (size: 734.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 214\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 225\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 193\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 213\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 221\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 194\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 223\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 216\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 226\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 217\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 210\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 208\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 189\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 222\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 205\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 201\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 203\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 190\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 196\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 219\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 195\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 200\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 218\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 207\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 202\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 188\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 227\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 198\u001b[0m\n",
      "\u001b[34m2020-08-01 07:49:05,117 INFO spark.ContextCleaner: Cleaned accumulator 220\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-08-01 07:57:27,275 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 7.0 (TID 9) in 1364776 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:14,736 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 8) in 4472239 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:14,736 INFO cluster.YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:14,737 INFO scheduler.DAGScheduler: ResultStage 7 (save at NativeMethodAccessorImpl.java:0) finished in 4472.274 s\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:14,738 INFO scheduler.DAGScheduler: Job 7 finished: save at NativeMethodAccessorImpl.java:0, took 4472.277808 s\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,286 INFO datasources.FileFormatWriter: Write Job a8f8cec1-2211-43a4-ab0b-34dab34d62ef committed.\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,286 INFO datasources.FileFormatWriter: Finished processing stats for write job a8f8cec1-2211-43a4-ab0b-34dab34d62ef.\u001b[0m\n",
      "\u001b[34mWrote to output file:  s3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-08-01-06-14-52/output/bert-validation\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,319 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,319 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,320 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,320 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,367 WARN commit.AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,367 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,367 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,367 INFO commit.AbstractS3ACommitterFactory: Using Commmitter FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20200801084915_0000}; taskId=attempt_20200801084915_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d9e4f6f}; outputPath=s3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-08-01-06-14-52/output/bert-test, workPath=s3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-08-01-06-14-52/output/bert-test/_temporary/0/_temporary/attempt_20200801084915_0000_m_000000_0, algorithmVersion=2, skipCleanup=false, ignoreCleanupFailures=false} for s3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-08-01-06-14-52/output/bert-test\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,367 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,665 INFO codegen.CodeGenerator: Code generated in 27.646197 ms\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,671 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 401.9 KB, free 363.7 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,687 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 42.9 KB, free 363.6 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,688 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.2.109.111:43237 (size: 42.9 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,689 INFO spark.SparkContext: Created broadcast 16 from save at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,689 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 84137401 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,767 INFO spark.SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,768 INFO scheduler.DAGScheduler: Got job 8 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,768 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (save at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,768 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,768 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,768 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[48] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,794 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 1096.1 KB, free 362.6 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,799 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 734.5 KB, free 361.9 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,800 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.2.109.111:43237 (size: 734.5 KB, free: 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,800 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,802 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 8 (MapPartitionsRDD[48] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,802 INFO cluster.YarnScheduler: Adding task set 8.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,805 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 10, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,805 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 11, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8478 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:15,817 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-2:40239 (size: 734.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 08:49:16,127 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-2:40239 (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 09:11:47,188 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 8.0 (TID 11) in 1351383 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,013 INFO spark.ContextCleaner: Cleaned accumulator 231\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,013 INFO spark.ContextCleaner: Cleaned accumulator 232\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,014 INFO spark.ContextCleaner: Cleaned accumulator 259\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,014 INFO spark.ContextCleaner: Cleaned accumulator 236\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,014 INFO spark.ContextCleaner: Cleaned accumulator 245\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,014 INFO spark.ContextCleaner: Cleaned accumulator 260\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,014 INFO spark.ContextCleaner: Cleaned accumulator 264\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,014 INFO spark.ContextCleaner: Cleaned accumulator 253\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,014 INFO spark.ContextCleaner: Cleaned accumulator 261\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,014 INFO spark.ContextCleaner: Cleaned accumulator 238\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,014 INFO spark.ContextCleaner: Cleaned accumulator 242\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,014 INFO spark.ContextCleaner: Cleaned accumulator 255\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,014 INFO spark.ContextCleaner: Cleaned accumulator 246\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,014 INFO spark.ContextCleaner: Cleaned accumulator 243\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,019 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.2.109.111:43237 in memory (size: 734.5 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,020 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-2:40239 in memory (size: 734.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,022 INFO spark.ContextCleaner: Cleaned accumulator 247\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,022 INFO spark.ContextCleaner: Cleaned accumulator 257\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,022 INFO spark.ContextCleaner: Cleaned accumulator 263\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,022 INFO spark.ContextCleaner: Cleaned accumulator 254\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,022 INFO spark.ContextCleaner: Cleaned accumulator 248\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,023 INFO spark.ContextCleaner: Cleaned accumulator 228\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,023 INFO spark.ContextCleaner: Cleaned accumulator 244\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,023 INFO spark.ContextCleaner: Cleaned accumulator 229\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,023 INFO spark.ContextCleaner: Cleaned accumulator 237\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,023 INFO spark.ContextCleaner: Cleaned accumulator 262\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,023 INFO spark.ContextCleaner: Cleaned accumulator 252\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,023 INFO spark.ContextCleaner: Cleaned accumulator 249\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,023 INFO spark.ContextCleaner: Cleaned accumulator 241\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,023 INFO spark.ContextCleaner: Cleaned accumulator 265\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,023 INFO spark.ContextCleaner: Cleaned accumulator 234\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,023 INFO spark.ContextCleaner: Cleaned accumulator 258\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,023 INFO spark.ContextCleaner: Cleaned accumulator 233\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,023 INFO spark.ContextCleaner: Cleaned accumulator 239\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,023 INFO spark.ContextCleaner: Cleaned accumulator 250\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,023 INFO spark.ContextCleaner: Cleaned accumulator 256\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,024 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.2.109.111:43237 in memory (size: 42.9 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,025 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-2:40239 in memory (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,026 INFO spark.ContextCleaner: Cleaned accumulator 266\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,026 INFO spark.ContextCleaner: Cleaned accumulator 230\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,026 INFO spark.ContextCleaner: Cleaned accumulator 240\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,026 INFO spark.ContextCleaner: Cleaned accumulator 235\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,026 INFO spark.ContextCleaner: Cleaned accumulator 267\u001b[0m\n",
      "\u001b[34m2020-08-01 09:19:05,026 INFO spark.ContextCleaner: Cleaned accumulator 251\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-08-01 10:03:33,049 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 10) in 4457247 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,049 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,050 INFO scheduler.DAGScheduler: ResultStage 8 (save at NativeMethodAccessorImpl.java:0) finished in 4457.280 s\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,050 INFO scheduler.DAGScheduler: Job 8 finished: save at NativeMethodAccessorImpl.java:0, took 4457.282827 s\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,472 INFO datasources.FileFormatWriter: Write Job a5a4d70d-5818-426e-8dd3-651e7e3508a2 committed.\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,472 INFO datasources.FileFormatWriter: Finished processing stats for write job a5a4d70d-5818-426e-8dd3-651e7e3508a2.\u001b[0m\n",
      "\u001b[34mWrote to output file:  s3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-08-01-06-14-52/output/bert-test\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,653 INFO datasources.InMemoryFileIndex: It took 60 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,663 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 389.9 KB, free 363.7 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,676 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 42.4 KB, free 363.7 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,676 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.2.109.111:43237 (size: 42.4 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,676 INFO spark.SparkContext: Created broadcast 18 from newAPIHadoopFile at DefaultSource.scala:37\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,774 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,792 INFO spark.SparkContext: Starting job: aggregate at TensorFlowInferSchema.scala:39\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,793 INFO scheduler.DAGScheduler: Got job 9 (aggregate at TensorFlowInferSchema.scala:39) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,793 INFO scheduler.DAGScheduler: Final stage: ResultStage 9 (aggregate at TensorFlowInferSchema.scala:39)\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,793 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,793 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,793 INFO scheduler.DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[52] at map at DefaultSource.scala:41), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,796 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 3.8 KB, free 363.7 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,797 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 2.2 KB, free 363.6 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,797 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.2.109.111:43237 (size: 2.2 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,798 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,800 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[52] at map at DefaultSource.scala:41) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,800 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,809 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 12, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8085 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,827 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-2:40239 (size: 2.2 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:33,846 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-2:40239 (size: 42.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,297 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 12) in 1497 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,297 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,297 INFO scheduler.DAGScheduler: ResultStage 9 (aggregate at TensorFlowInferSchema.scala:39) finished in 1.502 s\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,299 INFO scheduler.DAGScheduler: Job 9 finished: aggregate at TensorFlowInferSchema.scala:39, took 1.507061 s\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,313 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,313 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,314 INFO datasources.FileSourceStrategy: Output Data Schema: struct<label_ids: bigint, input_ids: array<bigint>, input_mask: array<bigint>, segment_ids: array<bigint> ... 2 more fields>\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,314 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,344 INFO codegen.CodeGenerator: Code generated in 17.26343 ms\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,349 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 398.1 KB, free 363.3 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,365 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 42.9 KB, free 363.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,366 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.2.109.111:43237 (size: 42.9 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,367 INFO spark.SparkContext: Created broadcast 20 from broadcast at DefaultSource.scala:94\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,368 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 13442024 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,375 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,376 INFO scheduler.DAGScheduler: Got job 10 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,376 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,376 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,376 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,377 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[56] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,390 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 12.6 KB, free 363.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,391 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 5.6 KB, free 363.2 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,392 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.2.109.111:43237 (size: 5.6 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,392 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,393 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[56] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,394 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,396 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 13, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8395 bytes)\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,405 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-2:40239 (size: 5.6 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,531 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-2:40239 (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,649 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 13) in 255 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,649 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,649 INFO scheduler.DAGScheduler: ResultStage 10 (showString at NativeMethodAccessorImpl.java:0) finished in 0.272 s\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,650 INFO scheduler.DAGScheduler: Job 10 finished: showString at NativeMethodAccessorImpl.java:0, took 0.274876 s\u001b[0m\n",
      "\u001b[34m+---------+--------------------+--------------------+--------------------+\u001b[0m\n",
      "\u001b[34m|label_ids|           input_ids|          input_mask|         segment_ids|\u001b[0m\n",
      "\u001b[34m+---------+--------------------+--------------------+--------------------+\u001b[0m\n",
      "\u001b[34m|        0|[101, 100, 102, 0...|[1, 1, 1, 0, 0, 0...|[0, 0, 0, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 100, 102, 0...|[1, 1, 1, 0, 0, 0...|[0, 0, 0, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 100, 102, 0...|[1, 1, 1, 0, 0, 0...|[0, 0, 0, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 100, 102, 0...|[1, 1, 1, 0, 0, 0...|[0, 0, 0, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 100, 2572, ...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        3|[101, 999, 6581, ...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        2|[101, 1001, 1015,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1002, 1018,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        0|[101, 1002, 2322,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1002, 2382,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        2|[101, 1002, 3998,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        0|[101, 1002, 4749,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1002, 5174,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1002, 14840...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1004, 1001,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1004, 1001,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1004, 1001,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1004, 1001,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        2|[101, 1004, 1001,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        3|[101, 1004, 1001,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m+---------+--------------------+--------------------+--------------------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,911 INFO spark.SparkContext: Invoking stop() from shutdown hook\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,914 INFO server.AbstractConnector: Stopped Spark@679f0cbf{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,916 INFO ui.SparkUI: Stopped Spark web UI at http://10.2.109.111:4040\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,919 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,937 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,937 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,939 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices\u001b[0m\n",
      "\u001b[34m(serviceOption=None,\n",
      " services=List(),\n",
      " started=false)\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,940 INFO cluster.YarnClientSchedulerBackend: Stopped\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,944 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,951 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,952 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,952 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,953 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,958 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,958 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,958 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-1a27af0f-5eff-4c6f-be05-4168c2eaa25d\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,961 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-1a27af0f-5eff-4c6f-be05-4168c2eaa25d/pyspark-641b18c5-a048-4960-9064-007951ea2bdb\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,962 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-14dc4056-c760-4285-b510-278d33e40865\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,966 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,966 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\u001b[0m\n",
      "\u001b[34m2020-08-01 10:03:35,966 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2020-08-01 10:03:37\u001b[0m\n",
      "\u001b[35mFinished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\u001b[35mReceived end of job signal, exiting...\u001b[0m\n",
      "\u001b[34mFinished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "running_processor.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span style=\"color:red\">위 Processing Job이 완료되기 전까지 기다려 주시기 바랍니다.</span></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the Processed Output Dataset 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $train_data_bert_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $validation_data_bert_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $test_data_bert_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = './data-tfrecord/bert-train'\n",
    "validation_data = './data-tfrecord/bert-validation'\n",
    "test_data = './data-tfrecord/bert-test'\n",
    "\n",
    "!aws s3 cp $train_data_bert_output $train_data --recursive\n",
    "!aws s3 cp $validation_data_bert_output $validation_data --recursive\n",
    "!aws s3 cp $test_data_bert_output $test_data --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store train_data_bert_output train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store validation_data_bert_output validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store test_data_bert_output test_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
