{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Processing Job \n",
    "\n",
    "\n",
    "기계 학습 (ML) 프로세스는 몇 단계로 구성됩니다. 먼저, 다양한 ETL 작업으로 데이터를 수집 한 다음 data의 pre-processing, 전통적인 기법 또는 사전 knowledge를 이용하여 데이터의 feature화, 마지막으로 알고리즘을 이용한 ML 모델을 학습합니다.\n",
    "\n",
    "Apache Spark와 같은 분산 데이터 처리 프레임 워크는 학습을 위해 dataset의 pre-processing하는데 사용합니다. 이 노트북에서는 Amazon SageMaker Processing에서 기본 설치된 Apache Spark의 기능을 활용하여 처리 워크로드를 실행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/prepare_dataset_bert.png)\n",
    "\n",
    "![](img/processing.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment\n",
    "\n",
    "\n",
    "* 모델 학습에 사용되는 S3 bucket과 prefix 가 필요합니다.\n",
    "* 학습과 processing을 위해 IAM role은 dataset에 액세스가 가능해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-322537213286/amazon-reviews-pds/tsv/\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "s3_input_data = 's3://{}/amazon-reviews-pds/tsv/'.format(bucket)\n",
    "print(s3_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-08 04:14:36   18997559 amazon_reviews_us_Digital_Software_v1_00.tsv.gz\r\n",
      "2020-12-08 04:14:38   27442648 amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\r\n",
      "2020-12-08 04:14:41  193389086 amazon_reviews_us_Musical_Instruments_v1_00.tsv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $s3_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Job을 수행할 Spark Docker Image\n",
    "\n",
    "이 HOL에서는 `./container` 폴더 내에 Spark container 이미지를 포함합니다. container는 모든 Spark 구성의 부트스트랩을 처리하고 `spark-submit` CLI를 wrapper해서 제공합니다. 상위 레벨에서는,\n",
    "\n",
    "* A set of default Spark/YARN/Hadoop configurations\n",
    "* A bootstrapping script for configuring and starting up Spark master/worker nodes\n",
    "* A wrapper around the `spark-submit` CLI to submit a Spark application\n",
    "\n",
    "container 빌드와 push 절차가 완료된 후 dataset의 처리를 수행하는 관리형 분산 Spark 어플리케이션을 수행사는 것은 Amazon SageMaker Python SDK 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_repo = 'amazon-reviews-spark-processor'\n",
    "docker_tag = 'latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  4.385MB\n",
      "Step 1/37 : FROM openjdk:8-jre-slim\n",
      " ---> 8c3c0e49c694\n",
      "Step 2/37 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> 4b70a64d75f7\n",
      "Step 3/37 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\n",
      " ---> Using cache\n",
      " ---> 3c014c905e2f\n",
      "Step 4/37 : RUN pip3 install py4j psutil==5.6.5 numpy==1.17.4\n",
      " ---> Using cache\n",
      " ---> bbbebd176ae5\n",
      "Step 5/37 : RUN apt-get clean\n",
      " ---> Using cache\n",
      " ---> 1a537b0b84c5\n",
      "Step 6/37 : RUN rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 57462dab1261\n",
      "Step 7/37 : ENV PYTHONHASHSEED 0\n",
      " ---> Using cache\n",
      " ---> de525fa99d2e\n",
      "Step 8/37 : ENV PYTHONIOENCODING UTF-8\n",
      " ---> Using cache\n",
      " ---> 8c0bf04bc4b6\n",
      "Step 9/37 : ENV PIP_DISABLE_PIP_VERSION_CHECK 1\n",
      " ---> Using cache\n",
      " ---> 3e66672125fd\n",
      "Step 10/37 : ENV HADOOP_VERSION 3.2.1\n",
      " ---> Using cache\n",
      " ---> 4b1ff6b5ab23\n",
      "Step 11/37 : ENV HADOOP_HOME /usr/hadoop-$HADOOP_VERSION\n",
      " ---> Using cache\n",
      " ---> 6740c3f3450e\n",
      "Step 12/37 : ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n",
      " ---> Using cache\n",
      " ---> e78012115315\n",
      "Step 13/37 : ENV PATH $PATH:$HADOOP_HOME/bin\n",
      " ---> Using cache\n",
      " ---> 4b8c80281caa\n",
      "Step 14/37 : RUN curl -sL --retry 3   \"http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\"   | gunzip   | tar -x -C /usr/  && rm -rf $HADOOP_HOME/share/doc  && chown -R root:root $HADOOP_HOME\n",
      " ---> Using cache\n",
      " ---> e812fe073522\n",
      "Step 15/37 : ENV SPARK_VERSION 2.4.6\n",
      " ---> Using cache\n",
      " ---> 30159694d39d\n",
      "Step 16/37 : ENV SPARK_PACKAGE spark-${SPARK_VERSION}-bin-without-hadoop\n",
      " ---> Using cache\n",
      " ---> da99bfe4b13e\n",
      "Step 17/37 : ENV SPARK_HOME /usr/spark-${SPARK_VERSION}\n",
      " ---> Using cache\n",
      " ---> 7f5df4797567\n",
      "Step 18/37 : ENV SPARK_DIST_CLASSPATH=\"$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*\"\n",
      " ---> Using cache\n",
      " ---> 001c34aa6b34\n",
      "Step 19/37 : ENV PATH $PATH:${SPARK_HOME}/bin\n",
      " ---> Using cache\n",
      " ---> 7d78ba3e3b9c\n",
      "Step 20/37 : RUN curl -sL --retry 3   \"https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/${SPARK_PACKAGE}.tgz\"   | gunzip   | tar x -C /usr/  && mv /usr/$SPARK_PACKAGE $SPARK_HOME  && chown -R root:root $SPARK_HOME\n",
      " ---> Using cache\n",
      " ---> 4885afd4f102\n",
      "Step 21/37 : ENV PYSPARK_PYTHON=/usr/bin/python3\n",
      " ---> Using cache\n",
      " ---> e4a788a553e0\n",
      "Step 22/37 : ENV PATH=\"/usr/bin:/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 56c52a8b6b5b\n",
      "Step 23/37 : ENV YARN_RESOURCEMANAGER_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 3153dac9f172\n",
      "Step 24/37 : ENV YARN_NODEMANAGER_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 250744b9b7ed\n",
      "Step 25/37 : ENV HDFS_NAMENODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 4c766738fcea\n",
      "Step 26/37 : ENV HDFS_DATANODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 2bc38472c74c\n",
      "Step 27/37 : ENV HDFS_SECONDARYNAMENODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 60766f9c201d\n",
      "Step 28/37 : COPY program /opt/program\n",
      " ---> Using cache\n",
      " ---> 63e3d995d723\n",
      "Step 29/37 : RUN chmod +x /opt/program/submit\n",
      " ---> Using cache\n",
      " ---> 5d478356a5c3\n",
      "Step 30/37 : COPY hadoop-config /opt/hadoop-config\n",
      " ---> Using cache\n",
      " ---> 0c0e530aec1d\n",
      "Step 31/37 : COPY jars /usr/jars\n",
      " ---> Using cache\n",
      " ---> 441c3abd0fb4\n",
      "Step 32/37 : WORKDIR $SPARK_HOME\n",
      " ---> Using cache\n",
      " ---> f35fed71ec06\n",
      "Step 33/37 : RUN pip3 install -q pip --upgrade\n",
      " ---> Using cache\n",
      " ---> c77a747323da\n",
      "Step 34/37 : RUN pip3 install -q wrapt --upgrade --ignore-installed\n",
      " ---> Using cache\n",
      " ---> a707efc5ca73\n",
      "Step 35/37 : RUN pip3 install -q transformers==2.8.0\n",
      " ---> Using cache\n",
      " ---> 8eb8499ff275\n",
      "Step 36/37 : RUN pip3 install -q tensorflow==2.1.0 --upgrade --ignore-installed\n",
      " ---> Using cache\n",
      " ---> c6abf341d2ad\n",
      "Step 37/37 : ENTRYPOINT [\"/opt/program/submit\"]\n",
      " ---> Using cache\n",
      " ---> cdb988189823\n",
      "Successfully built cdb988189823\n",
      "Successfully tagged amazon-reviews-spark-processor:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $docker_repo:$docker_tag -f container/Dockerfile ./container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark container의 Amazon Elastic Container Registry(Amazon ECR) 리포지토리를 생성하고 image를 push합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322537213286.dkr.ecr.us-east-1.amazonaws.com/amazon-reviews-spark-processor:latest\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "image_uri = '{}.dkr.ecr.{}.amazonaws.com/{}:{}'.format(account_id, region, docker_repo, docker_tag)\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECR repository 생성과 docker image를 push하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `RepositoryNotFoundException` 오류는 무시하셔도 됩니다. 즉시 repository를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"repositories\": [\r\n",
      "        {\r\n",
      "            \"repositoryArn\": \"arn:aws:ecr:us-east-1:322537213286:repository/amazon-reviews-spark-processor\",\r\n",
      "            \"registryId\": \"322537213286\",\r\n",
      "            \"repositoryName\": \"amazon-reviews-spark-processor\",\r\n",
      "            \"repositoryUri\": \"322537213286.dkr.ecr.us-east-1.amazonaws.com/amazon-reviews-spark-processor\",\r\n",
      "            \"createdAt\": 1596262176.0,\r\n",
      "            \"imageTagMutability\": \"MUTABLE\",\r\n",
      "            \"imageScanningConfiguration\": {\r\n",
      "                \"scanOnPush\": false\r\n",
      "            },\r\n",
      "            \"encryptionConfiguration\": {\r\n",
      "                \"encryptionType\": \"AES256\"\r\n",
      "            }\r\n",
      "        }\r\n",
      "    ]\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!aws ecr describe-repositories --repository-names $docker_repo || aws ecr create-repository --repository-name $docker_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker tag $docker_repo:$docker_tag $image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [322537213286.dkr.ecr.us-east-1.amazonaws.com/amazon-reviews-spark-processor]\n",
      "\n",
      "\u001b[1Beaa0235e: Preparing \n",
      "\u001b[1Bedaba6e6: Preparing \n",
      "\u001b[1Be5ebb758: Preparing \n",
      "\u001b[1B4ee3cec3: Preparing \n",
      "\u001b[1B5a78eb75: Preparing \n",
      "\u001b[1B80f9b7a3: Preparing \n",
      "\u001b[1Bce9863fb: Preparing \n",
      "\u001b[1B865a3806: Preparing \n",
      "\u001b[1B4961b07e: Preparing \n",
      "\u001b[1B24123e10: Preparing \n",
      "\u001b[1B0f9685ba: Preparing \n",
      "\u001b[1B0ae3ee2c: Preparing \n",
      "\u001b[1Bf6d86fae: Preparing \n",
      "\u001b[1Ba741a80e: Preparing \n",
      "\u001b[1B7975ff2a: Preparing \n",
      "\u001b[1B9ffee0cb: Preparing \n",
      "\u001b[1B5206ae8c: Preparing \n",
      "\u001b[1B98b79cde: Preparing \n",
      "\u001b[3B5206ae8c: Layer already exists \u001b[14A\u001b[2K\u001b[8A\u001b[2K\u001b[2A\u001b[2Klatest: digest: sha256:e105201545d54b91696c127bfb809ad88c5339e7e54a5864863f532806322114 size: 4318\n"
     ]
    }
   ],
   "source": [
    "!docker push $image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Processing Jobs 으로 Job 수행\n",
    "\n",
    "Amazon SageMaker Python SDK를 사용하여 Processing job을 실행합니다. Spark container와 job configuration에서 processing에 대한 Spark ML script를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m unicode_literals\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mshutil\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'pip', '--upgrade'])\u001b[39;49;00m\r\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'wrapt', '--upgrade', '--ignore-installed'])\u001b[39;49;00m\r\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0', '--ignore-installed'])\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[36mprint\u001b[39;49;00m(tf.__version__)\r\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'transformers==2.8.0'])\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SparkSession\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mml\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Pipeline\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mml\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mlinalg\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DenseVector\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m split\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m udf, col\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtypes\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\r\n",
      "\r\n",
      "tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[37m# We set sequences to be at most 128 tokens long.\u001b[39;49;00m\r\n",
      "MAX_SEQ_LENGTH = \u001b[34m128\u001b[39;49;00m\r\n",
      "DATA_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "LABEL_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "LABEL_VALUES = [\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\r\n",
      "\r\n",
      "label_map = {}\r\n",
      "\u001b[34mfor\u001b[39;49;00m (i, label) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(LABEL_VALUES):\r\n",
      "    label_map[label] = i\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInputFeatures\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\r\n",
      "  \u001b[33m\"\"\"BERT feature vectors.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m,\r\n",
      "               input_ids,\r\n",
      "               input_mask,\r\n",
      "               segment_ids,\r\n",
      "               label_id):\r\n",
      "    \u001b[36mself\u001b[39;49;00m.input_ids = input_ids\r\n",
      "    \u001b[36mself\u001b[39;49;00m.input_mask = input_mask\r\n",
      "    \u001b[36mself\u001b[39;49;00m.segment_ids = segment_ids\r\n",
      "    \u001b[36mself\u001b[39;49;00m.label_id = label_id\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInput\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\r\n",
      "  \u001b[33m\"\"\"A single training/test input for sequence classification.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, text, label=\u001b[34mNone\u001b[39;49;00m):\r\n",
      "    \u001b[33m\"\"\"Constructs an Input.\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m      text: string. The untokenized text of the first sequence. For single\u001b[39;49;00m\r\n",
      "\u001b[33m        sequence tasks, only this sequence must be specified.\u001b[39;49;00m\r\n",
      "\u001b[33m      label: (Optional) string. The label of the example. This should be\u001b[39;49;00m\r\n",
      "\u001b[33m        specified for train and dev examples, but not for test examples.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[36mself\u001b[39;49;00m.text = text\r\n",
      "    \u001b[36mself\u001b[39;49;00m.label = label\r\n",
      "    \r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mconvert_input\u001b[39;49;00m(label, text):\r\n",
      "    \u001b[37m# First, we need to preprocess our data so that it matches the data BERT was trained on:\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# 1. Lowercase our text (if we're using a BERT lowercase model)\u001b[39;49;00m\r\n",
      "    \u001b[37m# 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m# 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# Fortunately, the Transformers tokenizer does this for us!\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m#    tokens = tokenizer.tokenize(text_input.text)\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# Next, we need to do the following:\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# 4. Map our words to indexes using a vocab file that BERT provides\u001b[39;49;00m\r\n",
      "    \u001b[37m# 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\u001b[39;49;00m\r\n",
      "    \u001b[37m# 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# Again, the Transformers tokenizer does this for us!\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    encode_plus_tokens = tokenizer.encode_plus(text,\r\n",
      "                                               pad_to_max_length=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                                               max_length=MAX_SEQ_LENGTH)\r\n",
      "\r\n",
      "    \u001b[37m# Convert the text-based tokens to ids from the pre-trained BERT vocabulary\u001b[39;49;00m\r\n",
      "    input_ids = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    \u001b[37m# Specifies which tokens BERT should pay attention to (0 or 1)\u001b[39;49;00m\r\n",
      "    input_mask = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    \u001b[37m# Segment Ids are always 0 for single-sequence tasks (or 1 if two-sequence tasks)\u001b[39;49;00m\r\n",
      "    segment_ids = [\u001b[34m0\u001b[39;49;00m] * MAX_SEQ_LENGTH\r\n",
      "\r\n",
      "    \u001b[37m# Label for our training data (star_rating 1 through 5)\u001b[39;49;00m\r\n",
      "    label_id = label_map[label]\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m {\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: input_ids, \u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: input_mask, \u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: segment_ids, \u001b[33m'\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: [label_id]}\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mlist_arg\u001b[39;49;00m(raw_value):\r\n",
      "    \u001b[33m\"\"\"argparse type for a list of strings\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(raw_value).split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\r\n",
      "    \u001b[37m# Unlike SageMaker training jobs (which have `SM_HOSTS` and `SM_CURRENT_HOST` env vars), processing jobs to need to parse the resource config file directly\u001b[39;49;00m\r\n",
      "    resconfig = {}\r\n",
      "    \u001b[34mtry\u001b[39;49;00m:\r\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m cfgfile:\r\n",
      "            resconfig = json.load(cfgfile)\r\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mFileNotFoundError\u001b[39;49;00m:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json not found.  current_host is unknown.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[34mpass\u001b[39;49;00m \u001b[37m# Ignore\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# Local testing with CLI args\u001b[39;49;00m\r\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m'\u001b[39;49;00m\u001b[33mProcess\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=list_arg,\r\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mhosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, [\u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]),\r\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mComma-separated list of host names running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mcurrent_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\r\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mName of this host running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/output\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtransform\u001b[39;49;00m(spark, s3_input_data, s3_output_train_data, s3_output_validation_data, s3_output_test_data): \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mProcessing \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m => \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(s3_input_data, s3_output_train_data, s3_output_validation_data, s3_output_test_data))\r\n",
      " \r\n",
      "    schema = StructType([\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mmarketplace\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mcustomer_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mproduct_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mproduct_parent\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mproduct_title\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mproduct_category\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, IntegerType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mhelpful_votes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, IntegerType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mtotal_votes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, IntegerType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mvine\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mverified_purchase\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_headline\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_date\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    ])\r\n",
      "    \r\n",
      "    df_csv = spark.read.csv(path=s3_input_data,\r\n",
      "                            sep=\u001b[33m'\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                            schema=schema,\r\n",
      "                            header=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                            quote=\u001b[34mNone\u001b[39;49;00m)\r\n",
      "    df_csv.show()\r\n",
      "\r\n",
      "    \u001b[37m# This dataset should already be clean, but always good to double-check\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShowing null review_body rows...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    df_csv.where(col(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).isNull()).show()\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShowing cleaned csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    df_csv_dropped = df_csv.na.drop(subset=[\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    df_csv_dropped.show()\r\n",
      "\r\n",
      "    \u001b[37m# TODO:  Balance\u001b[39;49;00m\r\n",
      "    \r\n",
      "    features_df = df_csv_dropped.select([\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    features_df.show()\r\n",
      "\r\n",
      "    tfrecord_schema = StructType([\r\n",
      "      StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ArrayType(IntegerType(), \u001b[34mFalse\u001b[39;49;00m)),\r\n",
      "      StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ArrayType(IntegerType(), \u001b[34mFalse\u001b[39;49;00m)),\r\n",
      "      StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ArrayType(IntegerType(), \u001b[34mFalse\u001b[39;49;00m)),\r\n",
      "      StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ArrayType(IntegerType(), \u001b[34mFalse\u001b[39;49;00m))\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ])\r\n",
      "\r\n",
      "    bert_transformer = udf(\u001b[34mlambda\u001b[39;49;00m text, label: convert_input(text, label), tfrecord_schema)\r\n",
      "\r\n",
      "    spark.udf.register(\u001b[33m'\u001b[39;49;00m\u001b[33mbert_transformer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, bert_transformer)\r\n",
      "\r\n",
      "    transformed_df = features_df.select(bert_transformer(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).alias(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecords\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    transformed_df.show(truncate=\u001b[34mFalse\u001b[39;49;00m)\r\n",
      "\r\n",
      "    flattened_df = transformed_df.select(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecords.*\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    flattened_df.show()\r\n",
      "\r\n",
      "    \u001b[37m# Split 90-5-5%\u001b[39;49;00m\r\n",
      "    train_df, validation_df, test_df = flattened_df.randomSplit([\u001b[34m0.9\u001b[39;49;00m, \u001b[34m0.05\u001b[39;49;00m, \u001b[34m0.05\u001b[39;49;00m])\r\n",
      "\r\n",
      "    train_df.write.format(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).option(\u001b[33m'\u001b[39;49;00m\u001b[33mrecordType\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mExample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).save(path=s3_output_train_data)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWrote to output file:  \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(s3_output_train_data))\r\n",
      "    \r\n",
      "    validation_df.write.format(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).option(\u001b[33m'\u001b[39;49;00m\u001b[33mrecordType\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mExample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).save(path=s3_output_validation_data)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWrote to output file:  \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(s3_output_validation_data))\r\n",
      "\r\n",
      "    test_df.write.format(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).option(\u001b[33m'\u001b[39;49;00m\u001b[33mrecordType\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mExample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).save(path=s3_output_test_data)    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWrote to output file:  \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(s3_output_test_data))\r\n",
      "\r\n",
      "    restored_test_df = spark.read.format(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).option(\u001b[33m'\u001b[39;49;00m\u001b[33mrecordType\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mExample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).load(path=s3_output_test_data)\r\n",
      "    restored_test_df.show()\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\r\n",
      "    spark = SparkSession.builder.appName(\u001b[33m'\u001b[39;49;00m\u001b[33mAmazonReviewsSparkProcessor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).getOrCreate()\r\n",
      "\r\n",
      "    \u001b[37m# Convert command line args into a map of args\u001b[39;49;00m\r\n",
      "    args_iter = \u001b[36miter\u001b[39;49;00m(sys.argv[\u001b[34m1\u001b[39;49;00m:])\r\n",
      "    args = \u001b[36mdict\u001b[39;49;00m(\u001b[36mzip\u001b[39;49;00m(args_iter, args_iter))\r\n",
      "\r\n",
      "    \u001b[37m# Retrieve the args and replace 's3://' with 's3a://' (used by Spark)\u001b[39;49;00m\r\n",
      "    s3_input_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_input_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_input_data)\r\n",
      "\r\n",
      "    s3_output_train_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_output_train_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_output_train_data)\r\n",
      "\r\n",
      "    s3_output_validation_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_output_validation_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_output_validation_data)\r\n",
      "\r\n",
      "    s3_output_test_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_output_test_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_output_test_data)\r\n",
      "\r\n",
      "    transform(spark, s3_input_data, s3_output_train_data, s3_output_validation_data, s3_output_test_data)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    main()\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src_dir/preprocess-spark-text-to-bert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "processor = ScriptProcessor(base_job_name='spark-amazon-reviews-processor',\n",
    "                            image_uri=image_uri,\n",
    "                            command=['/opt/program/submit'],\n",
    "                            role=role,\n",
    "                            instance_count=2, # instance_count needs to be > 1 or you will see the following error:  \"INFO yarn.Client: Application report for application_ (state: ACCEPTED)\"\n",
    "                            instance_type='ml.r5.xlarge',\n",
    "                            env={'mode': 'python'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "output_prefix = 'amazon-reviews-spark-processor-{}'.format(timestamp_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-12-08-14-18-08/output/bert-train\n",
      "s3://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-12-08-14-18-08/output/bert-validation\n",
      "s3://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-12-08-14-18-08/output/bert-test\n"
     ]
    }
   ],
   "source": [
    "train_data_bert_output = 's3://{}/{}/output/bert-train'.format(bucket, output_prefix)\n",
    "validation_data_bert_output = 's3://{}/{}/output/bert-validation'.format(bucket, output_prefix)\n",
    "test_data_bert_output = 's3://{}/{}/output/bert-test'.format(bucket, output_prefix)\n",
    "\n",
    "print(train_data_bert_output)\n",
    "print(validation_data_bert_output)\n",
    "print(test_data_bert_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  spark-amazon-reviews-processor-2020-12-08-14-18-08-603\n",
      "Inputs:  [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-322537213286/spark-amazon-reviews-processor-2020-12-08-14-18-08-603/input/code/preprocess-spark-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'dummy-output', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-322537213286/spark-amazon-reviews-processor-2020-12-08-14-18-08-603/output/dummy-output', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingOutput\n",
    "\n",
    "processor.run(code='./src_dir/preprocess-spark-text-to-bert.py',\n",
    "              arguments=['s3_input_data', s3_input_data,\n",
    "                         's3_output_train_data', train_data_bert_output,\n",
    "                         's3_output_validation_data', validation_data_bert_output,\n",
    "                         's3_output_test_data', test_data_bert_output,                         \n",
    "              ],\n",
    "              # We need this dummy output to allow us to call \n",
    "              #    ProcessingJob.from_processing_name() later \n",
    "              #    to describe the job and poll for Completed status\n",
    "              outputs=[\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='dummy-output',\n",
    "                                        source='/opt/ml/processing/output')\n",
    "              ],          \n",
    "              logs=True,\n",
    "              wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=spark-amazon-reviews-processor-2020-12-08-14-18-08-603;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "spark_processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, spark_processing_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-12-08-14-18-08/?region=us-east-1&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# This is different than the job name because we are not using ProcessingOutput's in this Spark ML case.\n",
    "spark_processing_job_s3_output_prefix = output_prefix\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>'.format(bucket, spark_processing_job_s3_output_prefix, region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Processing Jobs through boto3 Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ProcessingJobSummaries': [{'ProcessingJobName': 'spark-amazon-reviews-processor-2020-12-08-14-18-08-603',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:322537213286:processing-job/spark-amazon-reviews-processor-2020-12-08-14-18-08-603',\n",
       "   'CreationTime': datetime.datetime(2020, 12, 8, 14, 18, 9, 6000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 12, 8, 14, 18, 9, 6000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'InProgress'},\n",
       "  {'ProcessingJobName': 'tensorflow-training-2020-1-Overtraining-fd755f53',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:322537213286:processing-job/tensorflow-training-2020-1-overtraining-fd755f53',\n",
       "   'CreationTime': datetime.datetime(2020, 12, 8, 12, 55, 21, 768000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 12, 8, 13, 3, 21, 336000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 12, 8, 13, 3, 21, 340000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'tensorflow-training-2020-1-LossNotDecreasing-b99afe0e',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:322537213286:processing-job/tensorflow-training-2020-1-lossnotdecreasing-b99afe0e',\n",
       "   'CreationTime': datetime.datetime(2020, 12, 8, 12, 55, 20, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 12, 8, 13, 4, 21, 875000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 12, 8, 13, 4, 21, 879000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'spark-amazon-reviews-processor-2020-12-08-12-21-18-642',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:322537213286:processing-job/spark-amazon-reviews-processor-2020-12-08-12-21-18-642',\n",
       "   'CreationTime': datetime.datetime(2020, 12, 8, 12, 21, 19, 62000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 12, 8, 12, 25, 44, 748000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'InProgress'},\n",
       "  {'ProcessingJobName': 'sagemaker-scikit-learn-2020-12-08-12-16-33-543',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:322537213286:processing-job/sagemaker-scikit-learn-2020-12-08-12-16-33-543',\n",
       "   'CreationTime': datetime.datetime(2020, 12, 8, 12, 16, 33, 925000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 12, 8, 12, 21, 35, 680000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 12, 8, 12, 21, 35, 684000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'pr-1-2db57bd54c924124897d2f44cefa90514b42a9abfa4b4cc58829ac6030',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:322537213286:processing-job/pr-1-2db57bd54c924124897d2f44cefa90514b42a9abfa4b4cc58829ac6030',\n",
       "   'CreationTime': datetime.datetime(2020, 12, 8, 4, 22, 47, 74000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 12, 8, 4, 28, 23, 245000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 12, 8, 4, 28, 23, 248000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'db-1-4104413a55bb43bbbee821dc5bf68b43d50e44652c1e42ad810c7b5a88',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:322537213286:processing-job/db-1-4104413a55bb43bbbee821dc5bf68b43d50e44652c1e42ad810c7b5a88',\n",
       "   'CreationTime': datetime.datetime(2020, 12, 8, 4, 18, 55, 438000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 12, 8, 4, 22, 37, 973000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 12, 8, 4, 22, 37, 977000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'pipelines-pt11p0vrmpf6-EvaluateAbaloneModel-j2JbgLLEro',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:322537213286:processing-job/pipelines-pt11p0vrmpf6-evaluateabalonemodel-j2jbgllero',\n",
       "   'CreationTime': datetime.datetime(2020, 12, 8, 0, 26, 37, 908000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 12, 8, 0, 30, 41, 372000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 12, 8, 0, 30, 41, 377000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'pipelines-pt11p0vrmpf6-PreprocessAbaloneDat-LY2JSeu2GF',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:322537213286:processing-job/pipelines-pt11p0vrmpf6-preprocessabalonedat-ly2jseu2gf',\n",
       "   'CreationTime': datetime.datetime(2020, 12, 8, 0, 19, 20, 39000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 12, 8, 0, 23, 39, 745000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 12, 8, 0, 23, 39, 748000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'pipelines-r1xfb4axiq8b-EvaluateAbaloneModel-HnYaW8ZukP',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:322537213286:processing-job/pipelines-r1xfb4axiq8b-evaluateabalonemodel-hnyaw8zukp',\n",
       "   'CreationTime': datetime.datetime(2020, 12, 8, 0, 13, 46, 383000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 12, 8, 0, 17, 56, 257000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 12, 8, 0, 17, 56, 260000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'}],\n",
       " 'NextToken': 'cIws2QhTXUIa8bi8XqFFSFgJoq5/N5VqPat8zDAW89U64DprP+2aAFY9YJ2MwWi+v96L33A7MB4+I5rLc2GRwfm7W7lPaQJnUP3woB69M39O4kzVwJCYfnmq236sVH6d25luG4gkWfuIdiTw45Hx6pMjydKPbJijA1MnY1gePgNVIJ6RuZIYi4KaA7DqfzgMdXN5e8RQHhnPMZNkch2RoucU/spEQAAXPStt/2hFzCBrhk45eRUFw1hbinSSUKpWxFhH51g/cBMQMzXsnFt5oeWykgsT3zjUr0RzH79f9cVEP0WtdQQrv/uYevnPL2HgP+39rhbDazBUzO0MB8JcrwZh743kL/d2mfpJg1pWHyTg6i3/mkAImd8lY7O+8SvRi6f0xB1M8UWH3II2Wb8ceAWRryeYSTY9tsYOxUVBrFBNulW2S9k/Eb5dAj0AMfKLbsWORgtPZlIOx4O9ITfdTFJA0qlSZRScBq9zjGQKTSwNBO895EfhgDzaJajHThD5tsvTutR5zoBm5KbuHiakiIknh7yb1qu5YucabIB9KE9QlXzFjPiSqP/w',\n",
       " 'ResponseMetadata': {'RequestId': '127d9a6d-0656-47dc-83d3-2c117bf7b942',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '127d9a6d-0656-47dc-83d3-2c117bf7b942',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '4038',\n",
       "   'date': 'Tue, 08 Dec 2020 14:18:08 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client('sagemaker')\n",
    "client.list_processing_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Wait Until the Processing Job Completes\n",
    "Re-run this next cell until the job status shows `Completed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "InProgress\n",
      "\n",
      "\n",
      "{'ProcessingInputs': [{'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-322537213286/spark-amazon-reviews-processor-2020-12-08-14-18-08-603/input/code/preprocess-spark-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'dummy-output', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-322537213286/spark-amazon-reviews-processor-2020-12-08-14-18-08-603/output/dummy-output', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}, 'AppManaged': False}]}, 'ProcessingJobName': 'spark-amazon-reviews-processor-2020-12-08-14-18-08-603', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.r5.xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}, 'AppSpecification': {'ImageUri': '322537213286.dkr.ecr.us-east-1.amazonaws.com/amazon-reviews-spark-processor:latest', 'ContainerEntrypoint': ['/opt/program/submit', '/opt/ml/processing/input/code/preprocess-spark-text-to-bert.py'], 'ContainerArguments': ['s3_input_data', 's3://sagemaker-us-east-1-322537213286/amazon-reviews-pds/tsv/', 's3_output_train_data', 's3://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-12-08-14-18-08/output/bert-train', 's3_output_validation_data', 's3://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-12-08-14-18-08/output/bert-validation', 's3_output_test_data', 's3://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-12-08-14-18-08/output/bert-test']}, 'Environment': {'mode': 'python'}, 'RoleArn': 'arn:aws:iam::322537213286:role/service-role/AIMLWorkshop-update-SageMakerIamRole-131X8MPN3KSN2', 'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:322537213286:processing-job/spark-amazon-reviews-processor-2020-12-08-14-18-08-603', 'ProcessingJobStatus': 'InProgress', 'LastModifiedTime': datetime.datetime(2020, 12, 8, 14, 18, 9, 6000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 12, 8, 14, 18, 9, 6000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '1c15a24e-91d6-47e4-9966-dbc176e2aaca', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '1c15a24e-91d6-47e4-9966-dbc176e2aaca', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2016', 'date': 'Tue, 08 Dec 2020 14:18:08 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=spark_processing_job_name,\n",
    "                                                                            sagemaker_session=sagemaker_session)\n",
    "\n",
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "processing_job_status = processing_job_description['ProcessingJobStatus']\n",
    "print('\\n')\n",
    "print(processing_job_status)\n",
    "print('\\n')\n",
    "\n",
    "print(processing_job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................\u001b[35mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[35mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[35mWARNING: /usr/hadoop-3.2.1/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[35mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[35mWARNING: Use of this script to start YARN daemons is deprecated.\u001b[0m\n",
      "\u001b[35mWARNING: Attempting to execute replacement \"yarn --daemon start\" instead.\u001b[0m\n",
      "\u001b[35mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[35mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.2.1/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,259 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.238.61\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.2.1/etc/hadoop:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/hadoop-annotations-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-lang3-3.7.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/curator-client-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-codec-1.11.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/httpcore-4.4.10.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/curator-framework-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-text-1.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-annotations-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-net-3.6.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-io-2.5.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/httpclient-4.5.6.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-compress-1.18.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/hadoop-auth-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-databind-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/zookeeper-3.4.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/hadoop-aws-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.375.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-nfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-kms-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-annotations-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/httpcore-4.4.10.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-io-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-text-1.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-security-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-annotations-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/httpclient-4.5.6.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-auth-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-databind-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-api-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-registry-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-client-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-router-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-api-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-core-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.\u001b[0m\n",
      "\u001b[34m1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-submarine-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.1.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_275\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,269 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,347 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-22b407e3-d81a-4ee9-aeee-d9e68ba46130\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,767 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,782 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,783 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,784 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,788 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,788 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,788 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,788 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,834 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,844 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,844 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,851 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,852 INFO blockmanagement.BlockManager: The block deletion will start around 2020 Dec 08 14:22:09\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,853 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,853 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,854 INFO util.GSet: 2.0% max memory 6.7 GB = 136.2 MB\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,854 INFO util.GSet: capacity      = 2^24 = 16777216 entries\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,895 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,895 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,901 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,901 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,901 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,901 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,902 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,902 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,902 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,902 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,902 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,902 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,902 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,924 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,924 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,924 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,924 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,936 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,936 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,936 INFO util.GSet: 1.0% max memory 6.7 GB = 68.1 MB\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,936 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,953 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,953 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,953 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,953 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,958 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,960 INFO snapshot.SnapshotManager: SkipList is disabled\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,964 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,964 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,964 INFO util.GSet: 0.25% max memory 6.7 GB = 17.0 MB\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,964 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,971 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,971 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,971 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,974 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,975 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,976 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,976 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,976 INFO util.GSet: 0.029999999329447746% max memory 6.7 GB = 2.0 MB\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,976 INFO util.GSet: capacity      = 2^18 = 262144 entries\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:09,997 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1639854255-10.0.238.61-1607437329991\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:10,010 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:10,040 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:10,130 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:10,142 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:10,146 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:10,146 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.238.61\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34mStarting namenodes on [algo-1]\u001b[0m\n",
      "\u001b[34malgo-1: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34mStarting datanodes\u001b[0m\n",
      "\u001b[34mlocalhost: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34mStarting secondary namenodes [ip-10-0-238-61.ec2.internal]\u001b[0m\n",
      "\u001b[34mip-10-0-238-61.ec2.internal: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[34mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[34mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[34mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mStarting resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34mStarting nodemanagers\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mlocalhost: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:22,842 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:23.695715: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:23.695809: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:23.695820: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\u001b[0m\n",
      "\u001b[34m2.1.0\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 232k/232k [00:00<00:00, 46.4MB/s]\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:24,982 INFO spark.SparkContext: Running Spark version 2.4.6\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,000 INFO spark.SparkContext: Submitted application: AmazonReviewsSparkProcessor\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,036 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,036 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,036 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,036 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,036 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,245 INFO util.Utils: Successfully started service 'sparkDriver' on port 36957.\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,266 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,278 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,280 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,281 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,287 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-7a693ec4-e9c3-44c9-a00a-35561a232b50\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,300 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,340 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,403 INFO util.log: Logging initialized @3619ms\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,452 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,471 INFO server.Server: Started @3688ms\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,488 INFO server.AbstractConnector: Started ServerConnector@44f8376a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,489 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,513 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@684a29c6{/jobs,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,513 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6cc2eaac{/jobs/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,514 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@556d1d9b{/jobs/job,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,517 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1fede741{/jobs/job/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,518 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2251b0e2{/stages,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,518 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@738b8c28{/stages/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,519 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18d6fae0{/stages/stage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,520 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@68cd940d{/stages/stage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,521 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4763720e{/stages/pool,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,521 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21fee0d7{/stages/pool/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,522 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2324560a{/storage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,522 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27cdd39f{/storage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,523 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7b62266a{/storage/rdd,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,523 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d4c8989{/storage/rdd/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,524 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c590e2c{/environment,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,525 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65417113{/environment/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,525 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@f7a5aa7{/executors,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,526 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46b39a00{/executors/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,526 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@227fc494{/executors/threadDump,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,527 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4cd8e021{/executors/threadDump/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,533 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@11aa247f{/static,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,534 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33f19ec9{/,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,535 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@11a35e1d{/api,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,535 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c66b51f{/jobs/job/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,536 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3293e9e3{/stages/stage/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:25,538 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.238.61:4040\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:26,210 INFO client.RMProxy: Connecting to ResourceManager at /10.0.238.61:8032\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:26,553 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:26,614 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:26,615 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:26,629 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (31706 MB per container)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:26,629 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:26,629 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:26,632 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:26,637 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:26,684 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:27,512 INFO yarn.Client: Uploading resource file:/tmp/spark-fca8e064-c28d-4a26-8a73-038fa147db00/__spark_libs__3131989924780632872.zip -> hdfs://10.0.238.61/user/root/.sparkStaging/application_1607437341230_0001/__spark_libs__3131989924780632872.zip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-12-08 14:22:27,865 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:28,623 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:28,864 INFO yarn.Client: Uploading resource file:/usr/spark-2.4.6/python/lib/pyspark.zip -> hdfs://10.0.238.61/user/root/.sparkStaging/application_1607437341230_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:28,873 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:28,895 INFO yarn.Client: Uploading resource file:/usr/spark-2.4.6/python/lib/py4j-0.10.7-src.zip -> hdfs://10.0.238.61/user/root/.sparkStaging/application_1607437341230_0001/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:28,902 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:29,050 INFO yarn.Client: Uploading resource file:/tmp/spark-fca8e064-c28d-4a26-8a73-038fa147db00/__spark_conf__3400037309725028400.zip -> hdfs://10.0.238.61/user/root/.sparkStaging/application_1607437341230_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:29,059 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:29,089 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:29,089 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:29,089 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:29,090 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:29,090 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:29,967 INFO yarn.Client: Submitting application application_1607437341230_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:30,162 INFO impl.YarnClientImpl: Submitted application application_1607437341230_0001\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:30,164 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1607437341230_0001 and attemptId None\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:31,181 INFO yarn.Client: Application report for application_1607437341230_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:31,184 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Tue Dec 08 14:22:31 +0000 2020] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1607437350060\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1607437341230_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:32,187 INFO yarn.Client: Application report for application_1607437341230_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:33,189 INFO yarn.Client: Application report for application_1607437341230_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:34,192 INFO yarn.Client: Application report for application_1607437341230_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:35,195 INFO yarn.Client: Application report for application_1607437341230_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:36,197 INFO yarn.Client: Application report for application_1607437341230_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:36,197 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.234.32\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1607437350060\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1607437341230_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:36,198 INFO cluster.YarnClientSchedulerBackend: Application application_1607437341230_0001 has started running.\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:36,219 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36471.\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:36,220 INFO netty.NettyBlockTransferService: Server created on 10.0.238.61:36471\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:36,221 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:36,240 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.238.61, 36471, None)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:36,243 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.238.61:36471 with 366.3 MB RAM, BlockManagerId(driver, 10.0.238.61, 36471, None)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:36,245 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.238.61, 36471, None)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:36,245 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.238.61, 36471, None)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:36,366 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18ff1524{/metrics/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:36,391 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1607437341230_0001), /proxy/application_1607437341230_0001\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:36,546 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:40,357 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.234.32:33068) with ID 1\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:40,457 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-2:35201 with 11.9 GB RAM, BlockManagerId(1, algo-2, 35201, None)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:55,594 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:55,798 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/spark-2.4.6/spark-warehouse').\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:55,798 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-2.4.6/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:55,804 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:55,810 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@20aa916f{/SQL,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:55,810 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:55,810 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@779b1f81{/SQL/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:55,811 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:55,811 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@131bab01{/SQL/execution,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:55,811 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:55,812 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1cc14621{/SQL/execution/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:55,812 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:55,813 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6aba0609{/static/sql,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:56,109 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-east-1-322537213286/amazon-reviews-pds/tsv/\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-12-08-14-18-08/output/bert-train\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-12-08-14-18-08/output/bert-validation\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-12-08-14-18-08/output/bert-test\u001b[0m\n",
      "\u001b[34mProcessing s3a://sagemaker-us-east-1-322537213286/amazon-reviews-pds/tsv/ => s3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-12-08-14-18-08/output/bert-train\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:56,303 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:56,364 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:56,364 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:57,640 INFO datasources.InMemoryFileIndex: It took 105 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:58,043 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:58,047 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:58,049 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:58,055 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:58,404 INFO codegen.CodeGenerator: Code generated in 192.220119 ms\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:58,616 INFO codegen.CodeGenerator: Code generated in 34.817589 ms\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:58,668 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 401.9 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:58,710 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 42.8 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:58,712 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.238.61:36471 (size: 42.8 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:58,714 INFO spark.SparkContext: Created broadcast 0 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:58,732 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 84137401 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:58,807 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:58,821 INFO scheduler.DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:58,822 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:58,822 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:58,823 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:58,828 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:58,887 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.3 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:58,889 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.3 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:58,890 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.238.61:36471 (size: 7.3 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:58,890 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:58,900 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:58,900 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:58,921 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:59,095 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-2:35201 (size: 7.3 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:22:59,724 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-2:35201 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-12-08 14:23:01,267 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2351 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,270 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,274 INFO scheduler.DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 2.434 s\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,277 INFO scheduler.DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 2.469554 s\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34m|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|   product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34m|         US|   45610553| RMDCHWD0Y5OZ9|B00HH62VB6|     618218723|AGPtek® 10 Isolat...|Musical Instruments|          3|            0|          1|   N|                N|         Three Stars|Works very good, ...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   14640079| RZSL0BALIYUNU|B003LRN53I|     986692292|Sennheiser HD203 ...|Musical Instruments|          5|            0|          0|   N|                Y|          Five Stars|Nice headphones a...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    6111003| RIZR67JKUDBI0|B0006VMBHI|     603261968|AudioQuest LP rec...|Musical Instruments|          3|            0|          1|   N|                Y|         Three Stars|removes dust. doe...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    1546619|R27HL570VNL85F|B002B55TRG|     575084461|Hohner Inc. 560BX...|Musical Instruments|          5|            0|          0|   N|                Y|I purchase these ...|I purchase these ...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   12222213|R34EBU9QDWJ1GD|B00N1YPXW2|     165236328|Blue Yeti USB Mic...|Musical Instruments|          5|            0|          0|   N|                Y|          Five Stars|This is an awesom...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   46018513|R1WCUI4Z1SIQEO|B001N4GRGS|     134151483|Middle Atlantic P...|Musical Instruments|          5|            0|          0|   N|                N|          Five Stars|Used to cool equi...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   10225065| RL5LNO26GAVJ1|B009PJRMHQ|     694166585|Kmise 1pc Pickgua...|Musical Instruments|          2|            3|          4|   N|                Y|Will not Fit Epip...|Note- Does not Fi...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    6356995|R3GYQ5W8JHP8SB|B00NKBDAZS|     446431775|Kealoha Concert U...|Musical Instruments|          5|            0|          0|   N|                Y|          Five Stars|Well built Ukulel...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   35297198|R30SHYQXGG5EYC|B006MIU7U2|     125871705|Halco 80000 - MR1...|Musical Instruments|          5|            0|          0|   N|                Y|Works fine. Hope ...|Had to replace a ...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   32139520|R14YLXA56NP51I|B000FIBD0I|     771888534|Gator GPTBLACK Pl...|Musical Instruments|          5|            1|          1|   N|                N|I upgraded the po...|I've owned multip...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   36060782|R1ZH0HSH38IOTZ|B0002E52GG|      68535945|Hetman 1 - Light ...|Musical Instruments|          5|            0|          0|   N|                Y|My son's favourit...|Consistent qualit...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    5301309|R3H53KLLC210XI|B00RZIH52G|     725541773|Dragonpad pop fil...|Musical Instruments|          4|            0|          0|   N|                Y|Great pop filter ...|by far the best p...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   37472935|R3OOR877NGA8JK|B001792BAU|      46570323|DharmaObjects Rel...|Musical Instruments|          3|            0|          0|   N|                Y|                  Ok|Beautiful set. On...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   33578270|R1BY7WKOZ3KMH0|B009GSKW1Y|     547963417|Musiclily SSS Pla...|Musical Instruments|          2|            0|          0|   N|                Y|           Two Stars|Bridge pickup was...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22070226| RXP1TFSWE8EG9|B0002F4TKA|     436074323|Vic Firth America...|Musical Instruments|          5|            0|          0|   N|                Y|          Five Stars|Feels good and la...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   52862655|R3J44DPP12OTLJ|B00K17YFBW|      81933093|Guitar Stand for ...|Musical Instruments|          5|            0|          0|   N|                Y|Great stand... on...|I love the stand....| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    4427243| RFOV69SK0T676|B00EQ24HJS|     669249276|Generic 3PLY Faux...|Musical Instruments|          5|            0|          0|   N|                Y|Looks great. You ...|On time. Looks gr...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   14108571|R2HUWDNW62FOL3|B00IBOYTUE|     749537231|Audio 2000 6525 3...|Musical Instruments|          1|            0|          0|   N|                Y|  Poor sound quality|I was hoping it w...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   27314089|R1KSU30XZGR452|B00FBRUSAE|     792472601|Sawtooth ST-AMP-1...|Musical Instruments|          5|            0|          0|   N|                Y|Perfect for the b...|Good sound for it...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   16735445|R2TZVLLTSHA07N|B0113D2QUO|     269114019|Upado Unlimited G...|Musical Instruments|          5|            1|          1|   N|                Y|It really is a mu...|Wow! I didn't exp...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34mShowing null review_body rows...\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,362 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,363 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnull(review_body#13)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,363 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,366 INFO execution.FileSourceScanExec: Pushed Filters: IsNull(review_body)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,436 INFO codegen.CodeGenerator: Code generated in 31.619434 ms\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,450 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 401.9 KB, free 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,478 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 42.8 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,479 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.238.61:36471 (size: 42.8 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,480 INFO spark.SparkContext: Created broadcast 2 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,481 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 84137401 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,490 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,492 INFO scheduler.DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,492 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,492 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,492 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,493 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,498 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 16.6 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,500 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.4 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,500 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.238.61:36471 (size: 7.4 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,500 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,502 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,502 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,503 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,556 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-2:35201 (size: 7.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:01,642 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-2:35201 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,284 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1781 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,285 INFO scheduler.DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 1.791 s\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,285 INFO scheduler.DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 1.794998 s\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,286 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+-----------+-----------+\u001b[0m\n",
      "\u001b[34m|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|   product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|review_body|review_date|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+-----------+-----------+\u001b[0m\n",
      "\u001b[34m|         US|   24889557|R1ALT15XW84SOW|B000RW81AM|     561741789|Stanton N500 Repl...|Musical Instruments|          5|            3|          3|   N|                Y| I mean my record...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|    4788420|R2MJ6IVQYBIGWB|B00Y7DB5ZA|     351956175|Scalze Mechanical...|Musical Instruments|          5|            0|          0|   N|                N|       Great Product|       null| 2015-08-24|\u001b[0m\n",
      "\u001b[34m|         US|   13756017|R1LY7OFXW7D83G|B00V39XDYW|     389322049|Strukture SSS57 S...|Musical Instruments|          1|            0|          1|   N|                Y|I think we have a...|       null| 2015-08-24|\u001b[0m\n",
      "\u001b[34m|         US|    1049988|R3IBRUXXKOD6HU|B008X040QE|     438050292|Surfing Blue Pear...|Musical Instruments|          5|            5|          5|   N|                Y|Should have done ...|       null| 2015-08-21|\u001b[0m\n",
      "\u001b[34m|         US|   41216360| RVVPQ9PKPSY9M|B00CFP5ESG|     954028768|Gator G-PG ACOUST...|Musical Instruments|          5|            1|          1|   N|                Y|          Five Stars|       null| 2015-08-18|\u001b[0m\n",
      "\u001b[34m|         US|    2291033| RH0M7W3FKAQE6|B008277N80|     412593952|Diamond Head Ukulele|Musical Instruments|          4|           21|         23|   N|                Y|          Four Stars|       null| 2015-08-15|\u001b[0m\n",
      "\u001b[34m|         US|   16594880|R39WV41UT37NOV|B000DLAO0C|     695688148|Jargar 4/4 Cello ...|Musical Instruments|          5|            0|          0|   N|                Y|          Five Stars|       null| 2015-08-15|\u001b[0m\n",
      "\u001b[34m|         US|    2000949|R16YVBT2IJDGSA|B000P9W5QI|     328210138|GP Percussion GP5...|Musical Instruments|          5|            3|          3|   N|                Y|          Five Stars|       null| 2015-08-14|\u001b[0m\n",
      "\u001b[34m|         US|    3991529|R1D7QB14L9UM0P|B00NKBDAZS|     446431775|Kealoha Concert U...|Musical Instruments|          5|            6|          6|   N|                Y|          Five Stars|       null| 2015-08-11|\u001b[0m\n",
      "\u001b[34m|         US|   20379821|R3HHDM1WQGE59V|B002DP594W|     153828378|Shure Professiona...|Musical Instruments|          4|            2|          3|   N|                Y|          Four Stars|       null| 2015-08-09|\u001b[0m\n",
      "\u001b[34m|         US|   47810643|R1HMENPQD85LN1|B00MO6KKSK|     308823533|Donner Dt-1 Chrom...|Musical Instruments|          5|            1|          1|   N|                Y|          Five Stars|       null| 2015-08-09|\u001b[0m\n",
      "\u001b[34m|         US|    2713246|R1NDQRG6NQW4LB|B00DS4OMUY|     613112255|Generic Amzdeal M...|Musical Instruments|          5|           22|         26|   N|                Y|          Five Stars|       null| 2015-08-08|\u001b[0m\n",
      "\u001b[34m|         US|    1025635|R2JC9Y0TV73V3T|B00KWOX51K|     412419029|Pro Tec SWPB2 Sto...|Musical Instruments|          4|           16|         17|   N|                N|          Four Stars|       null| 2015-08-06|\u001b[0m\n",
      "\u001b[34m|         US|     244839|R2YC4WBS1L2XCL|B00P63ZY2U|     378733816|WOWTOU LED Stage ...|Musical Instruments|          5|            2|          3|   N|                Y| Its very bright ...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   37622146|R369T16AY1NHCF|B00P08R23A|     642176027|Glory Black Resin...|Musical Instruments|          5|            9|          9|   N|                Y|Good piccolo. Not...|       null| 2015-07-30|\u001b[0m\n",
      "\u001b[34m|         US|   21145093| RJE12YGTJSEK3|B00I3XG5C8|     638790804|Epiphone CASINO C...|Musical Instruments|          5|            2|          3|   N|                Y|Not just a great ...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|    2562323|R194MQJW0BZTAT|B0007LCKPU|     191392034|PYLE-PRO PADH1079...|Musical Instruments|          5|            4|          4|   N|                Y|          Five Stars|       null| 2015-07-20|\u001b[0m\n",
      "\u001b[34m|         US|   14703927|R1F2ZEGR2NB7G0|B005MR6IHK|     675226467|Fender FT-004 Chr...|Musical Instruments|          5|            6|          6|   N|                Y|works great but w...|       null| 2015-07-18|\u001b[0m\n",
      "\u001b[34m|         US|   50584198| RVSWCRTL9MUHM|B000VBH2IG|     714474785|Zoom H2 Handy Por...|Musical Instruments|          4|            0|          0|   N|                Y| The quality is p...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|     362137|R18BYWE28FZF4U|B00KC4JMAS|     768480778|Audio-Technica AT...|Musical Instruments|          5|            0|          0|   N|                Y|          Five Stars|       null| 2015-07-10|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+-----------+-----------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34mShowing cleaned csv\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,339 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,340 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,340 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,340 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,403 INFO codegen.CodeGenerator: Code generated in 27.031388 ms\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,409 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 401.9 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,427 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 42.8 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,428 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.238.61:36471 (size: 42.8 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,429 INFO spark.SparkContext: Created broadcast 4 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,429 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 84137401 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,437 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,438 INFO scheduler.DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,438 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,438 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,439 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,439 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,443 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 16.8 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,444 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.5 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,445 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.238.61:36471 (size: 7.5 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,446 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,447 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,447 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,448 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,458 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-2:35201 (size: 7.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,507 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-2:35201 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,620 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 172 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,620 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,621 INFO scheduler.DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.181 s\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,622 INFO scheduler.DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.184417 s\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34m|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|   product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34m|         US|   45610553| RMDCHWD0Y5OZ9|B00HH62VB6|     618218723|AGPtek® 10 Isolat...|Musical Instruments|          3|            0|          1|   N|                N|         Three Stars|Works very good, ...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   14640079| RZSL0BALIYUNU|B003LRN53I|     986692292|Sennheiser HD203 ...|Musical Instruments|          5|            0|          0|   N|                Y|          Five Stars|Nice headphones a...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    6111003| RIZR67JKUDBI0|B0006VMBHI|     603261968|AudioQuest LP rec...|Musical Instruments|          3|            0|          1|   N|                Y|         Three Stars|removes dust. doe...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    1546619|R27HL570VNL85F|B002B55TRG|     575084461|Hohner Inc. 560BX...|Musical Instruments|          5|            0|          0|   N|                Y|I purchase these ...|I purchase these ...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   12222213|R34EBU9QDWJ1GD|B00N1YPXW2|     165236328|Blue Yeti USB Mic...|Musical Instruments|          5|            0|          0|   N|                Y|          Five Stars|This is an awesom...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   46018513|R1WCUI4Z1SIQEO|B001N4GRGS|     134151483|Middle Atlantic P...|Musical Instruments|          5|            0|          0|   N|                N|          Five Stars|Used to cool equi...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   10225065| RL5LNO26GAVJ1|B009PJRMHQ|     694166585|Kmise 1pc Pickgua...|Musical Instruments|          2|            3|          4|   N|                Y|Will not Fit Epip...|Note- Does not Fi...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    6356995|R3GYQ5W8JHP8SB|B00NKBDAZS|     446431775|Kealoha Concert U...|Musical Instruments|          5|            0|          0|   N|                Y|          Five Stars|Well built Ukulel...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   35297198|R30SHYQXGG5EYC|B006MIU7U2|     125871705|Halco 80000 - MR1...|Musical Instruments|          5|            0|          0|   N|                Y|Works fine. Hope ...|Had to replace a ...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   32139520|R14YLXA56NP51I|B000FIBD0I|     771888534|Gator GPTBLACK Pl...|Musical Instruments|          5|            1|          1|   N|                N|I upgraded the po...|I've owned multip...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   36060782|R1ZH0HSH38IOTZ|B0002E52GG|      68535945|Hetman 1 - Light ...|Musical Instruments|          5|            0|          0|   N|                Y|My son's favourit...|Consistent qualit...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    5301309|R3H53KLLC210XI|B00RZIH52G|     725541773|Dragonpad pop fil...|Musical Instruments|          4|            0|          0|   N|                Y|Great pop filter ...|by far the best p...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   37472935|R3OOR877NGA8JK|B001792BAU|      46570323|DharmaObjects Rel...|Musical Instruments|          3|            0|          0|   N|                Y|                  Ok|Beautiful set. On...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   33578270|R1BY7WKOZ3KMH0|B009GSKW1Y|     547963417|Musiclily SSS Pla...|Musical Instruments|          2|            0|          0|   N|                Y|           Two Stars|Bridge pickup was...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22070226| RXP1TFSWE8EG9|B0002F4TKA|     436074323|Vic Firth America...|Musical Instruments|          5|            0|          0|   N|                Y|          Five Stars|Feels good and la...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   52862655|R3J44DPP12OTLJ|B00K17YFBW|      81933093|Guitar Stand for ...|Musical Instruments|          5|            0|          0|   N|                Y|Great stand... on...|I love the stand....| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    4427243| RFOV69SK0T676|B00EQ24HJS|     669249276|Generic 3PLY Faux...|Musical Instruments|          5|            0|          0|   N|                Y|Looks great. You ...|On time. Looks gr...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   14108571|R2HUWDNW62FOL3|B00IBOYTUE|     749537231|Audio 2000 6525 3...|Musical Instruments|          1|            0|          0|   N|                Y|  Poor sound quality|I was hoping it w...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   27314089|R1KSU30XZGR452|B00FBRUSAE|     792472601|Sawtooth ST-AMP-1...|Musical Instruments|          5|            0|          0|   N|                Y|Perfect for the b...|Good sound for it...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   16735445|R2TZVLLTSHA07N|B0113D2QUO|     269114019|Upado Unlimited G...|Musical Instruments|          5|            1|          1|   N|                Y|It really is a mu...|Wow! I didn't exp...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,664 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,665 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,665 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,665 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,686 INFO codegen.CodeGenerator: Code generated in 11.158125 ms\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,703 INFO codegen.CodeGenerator: Code generated in 12.00038 ms\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,708 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 401.9 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,726 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 42.8 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,727 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.238.61:36471 (size: 42.8 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,728 INFO spark.SparkContext: Created broadcast 6 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,728 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 84137401 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,736 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,738 INFO scheduler.DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,738 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,738 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,738 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,738 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,743 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 12.2 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,745 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.4 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,746 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.238.61:36471 (size: 6.4 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,746 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,747 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,747 INFO cluster.YarnScheduler: Adding task set 3.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,748 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,881 INFO spark.ContextCleaner: Cleaned accumulator 63\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,881 INFO spark.ContextCleaner: Cleaned accumulator 7\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,881 INFO spark.ContextCleaner: Cleaned accumulator 19\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,881 INFO spark.ContextCleaner: Cleaned accumulator 77\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,881 INFO spark.ContextCleaner: Cleaned accumulator 26\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,881 INFO spark.ContextCleaner: Cleaned accumulator 48\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,881 INFO spark.ContextCleaner: Cleaned accumulator 20\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,881 INFO spark.ContextCleaner: Cleaned accumulator 91\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,881 INFO spark.ContextCleaner: Cleaned accumulator 56\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,881 INFO spark.ContextCleaner: Cleaned accumulator 86\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,881 INFO spark.ContextCleaner: Cleaned accumulator 2\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,881 INFO spark.ContextCleaner: Cleaned accumulator 78\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,881 INFO spark.ContextCleaner: Cleaned accumulator 55\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,881 INFO spark.ContextCleaner: Cleaned accumulator 89\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,881 INFO spark.ContextCleaner: Cleaned accumulator 62\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,881 INFO spark.ContextCleaner: Cleaned accumulator 38\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,882 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-2:35201 (size: 6.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,893 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.238.61:36471 in memory (size: 42.8 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,907 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-2:35201 in memory (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,922 INFO spark.ContextCleaner: Cleaned accumulator 32\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,927 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.238.61:36471 in memory (size: 7.4 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,930 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-2:35201 in memory (size: 7.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,931 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-2:35201 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,937 INFO spark.ContextCleaner: Cleaned accumulator 46\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,937 INFO spark.ContextCleaner: Cleaned accumulator 23\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,937 INFO spark.ContextCleaner: Cleaned accumulator 58\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,937 INFO spark.ContextCleaner: Cleaned accumulator 30\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,937 INFO spark.ContextCleaner: Cleaned accumulator 74\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,937 INFO spark.ContextCleaner: Cleaned accumulator 85\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,937 INFO spark.ContextCleaner: Cleaned accumulator 17\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,938 INFO spark.ContextCleaner: Cleaned accumulator 40\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,938 INFO spark.ContextCleaner: Cleaned accumulator 29\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,938 INFO spark.ContextCleaner: Cleaned accumulator 69\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,938 INFO spark.ContextCleaner: Cleaned accumulator 6\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,938 INFO spark.ContextCleaner: Cleaned accumulator 90\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,938 INFO spark.ContextCleaner: Cleaned accumulator 18\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,938 INFO spark.ContextCleaner: Cleaned accumulator 45\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,938 INFO spark.ContextCleaner: Cleaned accumulator 22\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,938 INFO spark.ContextCleaner: Cleaned accumulator 72\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,938 INFO spark.ContextCleaner: Cleaned accumulator 54\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,938 INFO spark.ContextCleaner: Cleaned accumulator 83\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,938 INFO spark.ContextCleaner: Cleaned accumulator 44\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,938 INFO spark.ContextCleaner: Cleaned accumulator 24\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,938 INFO spark.ContextCleaner: Cleaned accumulator 52\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,938 INFO spark.ContextCleaner: Cleaned accumulator 37\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,939 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.238.61:36471 in memory (size: 42.8 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,949 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on algo-2:35201 in memory (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,966 INFO spark.ContextCleaner: Cleaned accumulator 57\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,968 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.238.61:36471 in memory (size: 42.8 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,969 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-2:35201 in memory (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,973 INFO spark.ContextCleaner: Cleaned accumulator 15\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,973 INFO spark.ContextCleaner: Cleaned accumulator 76\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,973 INFO spark.ContextCleaner: Cleaned accumulator 33\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,975 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.238.61:36471 in memory (size: 7.3 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,976 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-2:35201 in memory (size: 7.3 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,981 INFO spark.ContextCleaner: Cleaned accumulator 84\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,981 INFO spark.ContextCleaner: Cleaned accumulator 39\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,981 INFO spark.ContextCleaner: Cleaned accumulator 64\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,983 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.238.61:36471 in memory (size: 7.5 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,984 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-2:35201 in memory (size: 7.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,988 INFO spark.ContextCleaner: Cleaned accumulator 25\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,988 INFO spark.ContextCleaner: Cleaned accumulator 12\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,988 INFO spark.ContextCleaner: Cleaned accumulator 16\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,988 INFO spark.ContextCleaner: Cleaned accumulator 73\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,988 INFO spark.ContextCleaner: Cleaned accumulator 14\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,988 INFO spark.ContextCleaner: Cleaned accumulator 87\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,988 INFO spark.ContextCleaner: Cleaned accumulator 9\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,988 INFO spark.ContextCleaner: Cleaned accumulator 3\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,988 INFO spark.ContextCleaner: Cleaned accumulator 31\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,988 INFO spark.ContextCleaner: Cleaned accumulator 70\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,988 INFO spark.ContextCleaner: Cleaned accumulator 35\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,988 INFO spark.ContextCleaner: Cleaned accumulator 4\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,988 INFO spark.ContextCleaner: Cleaned accumulator 80\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,988 INFO spark.ContextCleaner: Cleaned accumulator 8\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,988 INFO spark.ContextCleaner: Cleaned accumulator 21\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,988 INFO spark.ContextCleaner: Cleaned accumulator 61\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,988 INFO spark.ContextCleaner: Cleaned accumulator 10\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,988 INFO spark.ContextCleaner: Cleaned accumulator 27\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,988 INFO spark.ContextCleaner: Cleaned accumulator 49\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,988 INFO spark.ContextCleaner: Cleaned accumulator 59\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,988 INFO spark.ContextCleaner: Cleaned accumulator 28\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 1\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 60\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 53\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 88\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 79\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 71\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 47\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 42\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 92\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 11\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 68\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 67\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 82\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 66\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 75\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 43\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 13\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 81\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 51\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 50\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 34\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 36\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 5\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 65\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:03,989 INFO spark.ContextCleaner: Cleaned accumulator 41\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,106 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 358 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,107 INFO scheduler.DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.368 s\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,107 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,107 INFO scheduler.DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0.370925 s\u001b[0m\n",
      "\u001b[34m+-----------+--------------------+\u001b[0m\n",
      "\u001b[34m|star_rating|         review_body|\u001b[0m\n",
      "\u001b[34m+-----------+--------------------+\u001b[0m\n",
      "\u001b[34m|          3|Works very good, ...|\u001b[0m\n",
      "\u001b[34m|          5|Nice headphones a...|\u001b[0m\n",
      "\u001b[34m|          3|removes dust. doe...|\u001b[0m\n",
      "\u001b[34m|          5|I purchase these ...|\u001b[0m\n",
      "\u001b[34m|          5|This is an awesom...|\u001b[0m\n",
      "\u001b[34m|          5|Used to cool equi...|\u001b[0m\n",
      "\u001b[34m|          2|Note- Does not Fi...|\u001b[0m\n",
      "\u001b[34m|          5|Well built Ukulel...|\u001b[0m\n",
      "\u001b[34m|          5|Had to replace a ...|\u001b[0m\n",
      "\u001b[34m|          5|I've owned multip...|\u001b[0m\n",
      "\u001b[34m|          5|Consistent qualit...|\u001b[0m\n",
      "\u001b[34m|          4|by far the best p...|\u001b[0m\n",
      "\u001b[34m|          3|Beautiful set. On...|\u001b[0m\n",
      "\u001b[34m|          2|Bridge pickup was...|\u001b[0m\n",
      "\u001b[34m|          5|Feels good and la...|\u001b[0m\n",
      "\u001b[34m|          5|I love the stand....|\u001b[0m\n",
      "\u001b[34m|          5|On time. Looks gr...|\u001b[0m\n",
      "\u001b[34m|          1|I was hoping it w...|\u001b[0m\n",
      "\u001b[34m|          5|Good sound for it...|\u001b[0m\n",
      "\u001b[34m|          5|Wow! I didn't exp...|\u001b[0m\n",
      "\u001b[34m+-----------+--------------------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,719 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,720 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,720 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,720 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,740 INFO codegen.CodeGenerator: Code generated in 9.015685 ms\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,771 INFO codegen.CodeGenerator: Code generated in 19.989376 ms\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,784 INFO codegen.CodeGenerator: Code generated in 9.775263 ms\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,789 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 401.9 KB, free 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,804 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 42.8 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,804 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.238.61:36471 (size: 42.8 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,805 INFO spark.SparkContext: Created broadcast 8 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,805 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 84137401 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,861 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,863 INFO scheduler.DAGScheduler: Got job 4 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,863 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,863 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,863 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,863 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[22] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,870 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 849.5 KB, free 364.6 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,876 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 644.8 KB, free 364.0 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,877 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.238.61:36471 (size: 644.8 KB, free: 365.6 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,877 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,878 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[22] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,878 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,879 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:04,891 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-2:35201 (size: 644.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:05,394 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-2:35201 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-12-08 14:23:07,204 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 2324 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,204 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,205 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 53275\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,206 INFO scheduler.DAGScheduler: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0) finished in 2.342 s\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,206 INFO scheduler.DAGScheduler: Job 4 finished: showString at NativeMethodAccessorImpl.java:0, took 2.344743 s\u001b[0m\n",
      "\u001b[34m+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34m|tfrecords                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\u001b[0m\n",
      "\u001b[34m+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34m|[[101, 2573, 2200, 2204, 1010, 2021, 19653, 2015, 2632, 4140, 1997, 5005, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2]]                                                                                                                                                                                                                                                                                                                                                         |\u001b[0m\n",
      "\u001b[34m|[[101, 3835, 2132, 19093, 2012, 1037, 9608, 3976, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                     |\u001b[0m\n",
      "\u001b[34m|[[101, 20362, 6497, 1012, 2515, 2025, 4550, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2]]                                                                                                                                                                                                                                                                                                                                                                           |\u001b[0m\n",
      "\u001b[34m|[[101, 1045, 5309, 2122, 2005, 1037, 2767, 1999, 2709, 2005, 2652, 2068, 2005, 2026, 2269, 1998, 2060, 12455, 2012, 1037, 2334, 5075, 2188, 1012, 4067, 2017, 1054, 1012, 24869, 16523, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                    |\u001b[0m\n",
      "\u001b[34m|[[101, 2023, 2003, 2019, 12476, 23025, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                           |\u001b[0m\n",
      "\u001b[34m|[[101, 2109, 2000, 4658, 3941, 2503, 13675, 14728, 16786, 2015, 1012, 2499, 2307, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                    |\u001b[0m\n",
      "\u001b[34m|[[101, 3602, 1011, 2515, 2025, 4906, 4958, 11514, 27406, 22214, 2569, 999, 3067, 2003, 12800, 2526, 2944, 1998, 2009, 2097, 2025, 6039, 1996, 2686, 2090, 1996, 14910, 24204, 2545, 3294, 1012, 2673, 2842, 2003, 2307, 1998, 2204, 7477, 21530, 4728, 1012, 1045, 2018, 2000, 5587, 2026, 2219, 7661, 2081, 6039, 2121, 5127, 2000, 3143, 1996, 3857, 1012, 1045, 2031, 2025, 2042, 2583, 2000, 2424, 2023, 2806, 4060, 3457, 2008, 2097, 4906, 2026, 2858, 2007, 5372, 12403, 6129, 2090, 14910, 24204, 2545, 2302, 1037, 1004, 1001, 4090, 1025, 7661, 1004, 1001, 4090, 1025, 3976, 1998, 2383, 2000, 4604, 2115, 2858, 2041, 1012, 1026, 7987, 1013, 1028, 2204, 3976, 1010, 2021, 2022, 4810, 2000, 2079, 2070, 7661, 2147, 4426, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1]]                       |\u001b[0m\n",
      "\u001b[34m|[[101, 2092, 2328, 2866, 9307, 2571, 999, 2026, 2684, 7459, 2009, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                               |\u001b[0m\n",
      "\u001b[34m|[[101, 2018, 2000, 5672, 1037, 2047, 2422, 2044, 1037, 7407, 4040, 1012, 2573, 2986, 1012, 3246, 2009, 16180, 2083, 1996, 10943, 2100, 2558, 2023, 2051, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                  |\u001b[0m\n",
      "\u001b[34m|[[101, 1045, 1005, 2310, 3079, 3674, 4964, 7923, 2058, 1996, 2086, 1010, 2021, 1045, 2172, 2738, 6293, 2007, 3265, 3466, 3197, 1008, 1008, 1008, 1045, 1005, 2310, 4156, 1037, 2261, 4064, 7923, 1998, 2027, 6357, 2039, 4634, 4237, 1010, 2059, 2009, 1005, 1055, 2784, 2416, 2051, 1008, 1008, 1008, 2026, 2567, 1011, 1999, 1011, 2375, 4156, 1996, 14246, 2102, 16558, 28400, 2099, 2004, 1037, 1060, 1011, 3742, 5592, 1019, 2086, 3283, 1010, 1998, 2172, 2000, 2026, 4474, 1010, 2009, 1005, 1055, 2145, 10209, 4632, 1008, 1008, 1008, 1045, 1005, 2310, 3333, 2009, 1037, 2261, 2335, 1010, 2009, 1005, 1055, 2042, 9554, 3706, 2006, 1037, 2261, 2335, 1010, 6476, 2105, 3365, 2335, 1010, 2021, 2044, 2035, 2023, 6905, 1010, 2016, 1005, 1055, 2145, 2600, 2378, 1005, 2004, 2524, 102], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]|\u001b[0m\n",
      "\u001b[34m|[[101, 8335, 3737, 1010, 2053, 2844, 1051, 26797, 2869, 1010, 2573, 2092, 1012, 2026, 2365, 1005, 1055, 8837, 9368, 10764, 3514, 1012, 2002, 2003, 1037, 3809, 2447, 1998, 2023, 2003, 2036, 2054, 2010, 3836, 1006, 1037, 2658, 2102, 19379, 22327, 2121, 1007, 3594, 1012, 2009, 1005, 1055, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                |\u001b[0m\n",
      "\u001b[34m|[[101, 2011, 2521, 1996, 2190, 3769, 11307, 1045, 2031, 2109, 1010, 5186, 4621, 1010, 1045, 2106, 2025, 6065, 2151, 2250, 2012, 2035, 1010, 2026, 2069, 3291, 2003, 2008, 1996, 13020, 1011, 3300, 1998, 15986, 18856, 16613, 2024, 10036, 1998, 2524, 2000, 14171, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3]]                                                                                                                                                                                                                                                            |\u001b[0m\n",
      "\u001b[34m|[[101, 3376, 2275, 1012, 2069, 1996, 2614, 2025, 2012, 2146, 1998, 26709, 2361, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2]]                                                                                                                                                                                                                                                                                                                                                      |\u001b[0m\n",
      "\u001b[34m|[[101, 2958, 15373, 2001, 3714, 1012, 1045, 5672, 1040, 1996, 15373, 1998, 7929, 2085, 1012, 2000, 10036, 2000, 4604, 2067, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1]]                                                                                                                                                                                                                                                                                                                               |\u001b[0m\n",
      "\u001b[34m|[[101, 5683, 2204, 1998, 2197, 2146, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                               |\u001b[0m\n",
      "\u001b[34m|[[101, 1045, 2293, 1996, 3233, 1012, 1045, 4149, 2048, 1012, 1996, 2069, 1043, 15909, 2818, 2003, 2008, 2043, 1045, 4987, 1037, 5830, 2000, 2026, 2858, 1010, 1996, 5830, 1005, 1055, 13354, 2718, 1996, 2723, 2043, 1996, 2858, 2001, 1999, 1996, 3233, 1012, 1037, 2157, 6466, 13354, 13332, 1996, 3291, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                       |\u001b[0m\n",
      "\u001b[34m|[[101, 2006, 2051, 1012, 3504, 2307, 1012, 2017, 2031, 2000, 3424, 6895, 17585, 15135, 2047, 11224, 19990, 2043, 2017, 5672, 1037, 4060, 18405, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                     |\u001b[0m\n",
      "\u001b[34m|[[101, 1045, 2001, 5327, 2009, 2052, 2147, 2092, 1010, 2021, 2699, 1037, 2117, 11601, 1998, 2052, 2069, 4374, 1037, 2843, 1997, 10763, 1999, 2119, 5363, 1012, 2001, 2196, 2583, 2000, 2131, 3154, 2614, 2012, 2035, 1998, 2699, 3048, 2185, 2013, 2151, 2825, 11099, 4385, 1012, 6410, 10858, 2151, 2785, 1997, 4390, 5008, 1998, 3784, 2001, 11158, 2004, 2092, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0]]                                                                                                                                                                                                           |\u001b[0m\n",
      "\u001b[34m|[[101, 2204, 2614, 2005, 2049, 2946, 1998, 3976, 1012, 2307, 1997, 1037, 2402, 2858, 3076, 1996, 4553, 2006, 1059, 1013, 1051, 4911, 1996, 2924, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                      |\u001b[0m\n",
      "\u001b[34m|[[101, 10166, 999, 1045, 2134, 1005, 1056, 5987, 1996, 3737, 1998, 2046, 9323, 1997, 2023, 4031, 2000, 2022, 2061, 2152, 1012, 2009, 2428, 2003, 1037, 3315, 6602, 1998, 1996, 2338, 2003, 1037, 10392, 2126, 2000, 4088, 1012, 4283, 2005, 1996, 16465, 2791, 2008, 2253, 2046, 2023, 4031, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                |\u001b[0m\n",
      "\u001b[34m+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,300 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,300 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,301 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,301 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,324 INFO codegen.CodeGenerator: Code generated in 9.761886 ms\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,349 INFO codegen.CodeGenerator: Code generated in 17.130866 ms\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,358 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 401.9 KB, free 363.6 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,375 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 42.8 KB, free 363.5 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,375 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.238.61:36471 (size: 42.8 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,376 INFO spark.SparkContext: Created broadcast 10 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,376 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 84137401 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,402 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,404 INFO scheduler.DAGScheduler: Got job 5 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,404 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,404 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,404 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,405 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,415 INFO spark.ContextCleaner: Cleaned accumulator 139\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,416 INFO spark.ContextCleaner: Cleaned accumulator 138\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,416 INFO spark.ContextCleaner: Cleaned accumulator 153\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,416 INFO spark.ContextCleaner: Cleaned accumulator 144\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,416 INFO spark.ContextCleaner: Cleaned accumulator 152\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,417 INFO spark.ContextCleaner: Cleaned accumulator 134\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,417 INFO spark.ContextCleaner: Cleaned accumulator 148\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,417 INFO spark.ContextCleaner: Cleaned accumulator 129\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,417 INFO spark.ContextCleaner: Cleaned accumulator 133\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,417 INFO spark.ContextCleaner: Cleaned accumulator 151\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,419 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 849.1 KB, free 362.7 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,422 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-2:35201 in memory (size: 644.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,428 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 644.8 KB, free 362.1 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,429 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.238.61:36471 (size: 644.8 KB, free: 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,430 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,431 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,431 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,432 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.238.61:36471 in memory (size: 644.8 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,432 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,435 INFO spark.ContextCleaner: Cleaned accumulator 137\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,435 INFO spark.ContextCleaner: Cleaned accumulator 132\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,435 INFO spark.ContextCleaner: Cleaned accumulator 155\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,435 INFO spark.ContextCleaner: Cleaned accumulator 147\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,435 INFO spark.ContextCleaner: Cleaned accumulator 149\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,435 INFO spark.ContextCleaner: Cleaned accumulator 128\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,435 INFO spark.ContextCleaner: Cleaned accumulator 127\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,435 INFO spark.ContextCleaner: Cleaned accumulator 125\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,435 INFO spark.ContextCleaner: Cleaned accumulator 146\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,435 INFO spark.ContextCleaner: Cleaned accumulator 140\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,435 INFO spark.ContextCleaner: Cleaned accumulator 150\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,435 INFO spark.ContextCleaner: Cleaned accumulator 154\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,435 INFO spark.ContextCleaner: Cleaned accumulator 131\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,435 INFO spark.ContextCleaner: Cleaned accumulator 143\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,435 INFO spark.ContextCleaner: Cleaned accumulator 136\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,435 INFO spark.ContextCleaner: Cleaned accumulator 141\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,437 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.238.61:36471 in memory (size: 42.8 KB, free: 365.6 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,439 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-2:35201 in memory (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,443 INFO spark.ContextCleaner: Cleaned accumulator 126\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,443 INFO spark.ContextCleaner: Cleaned accumulator 145\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,443 INFO spark.ContextCleaner: Cleaned accumulator 135\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,443 INFO spark.ContextCleaner: Cleaned accumulator 124\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,443 INFO spark.ContextCleaner: Cleaned accumulator 142\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,443 INFO spark.ContextCleaner: Cleaned accumulator 130\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,443 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-2:35201 (size: 644.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:07,465 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-2:35201 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:09,590 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 2158 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:09,590 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:09,591 INFO scheduler.DAGScheduler: ResultStage 5 (showString at NativeMethodAccessorImpl.java:0) finished in 2.186 s\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:09,591 INFO scheduler.DAGScheduler: Job 5 finished: showString at NativeMethodAccessorImpl.java:0, took 2.188757 s\u001b[0m\n",
      "\u001b[34m+--------------------+--------------------+--------------------+---------+\u001b[0m\n",
      "\u001b[34m|           input_ids|          input_mask|         segment_ids|label_ids|\u001b[0m\n",
      "\u001b[34m+--------------------+--------------------+--------------------+---------+\u001b[0m\n",
      "\u001b[34m|[101, 2573, 2200,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [2]|\u001b[0m\n",
      "\u001b[34m|[101, 3835, 2132,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 20362, 6497...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [2]|\u001b[0m\n",
      "\u001b[34m|[101, 1045, 5309,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 2023, 2003,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 2109, 2000,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 3602, 1011,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [1]|\u001b[0m\n",
      "\u001b[34m|[101, 2092, 2328,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 2018, 2000,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 1045, 1005,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 8335, 3737,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 2011, 2521,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [3]|\u001b[0m\n",
      "\u001b[34m|[101, 3376, 2275,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [2]|\u001b[0m\n",
      "\u001b[34m|[101, 2958, 15373...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [1]|\u001b[0m\n",
      "\u001b[34m|[101, 5683, 2204,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 1045, 2293,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 2006, 2051,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 1045, 2001,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [0]|\u001b[0m\n",
      "\u001b[34m|[101, 2204, 2614,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 10166, 999,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m+--------------------+--------------------+--------------------+---------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:09,672 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:09,672 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:09,673 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:09,673 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:09,807 WARN commit.AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:09,808 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:09,808 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:09,808 INFO commit.AbstractS3ACommitterFactory: Using Commmitter FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20201208142309_0000}; taskId=attempt_20201208142309_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7b8263ec}; outputPath=s3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-12-08-14-18-08/output/bert-train, workPath=s3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-12-08-14-18-08/output/bert-train/_temporary/0/_temporary/attempt_20201208142309_0000_m_000000_0, algorithmVersion=2, skipCleanup=false, ignoreCleanupFailures=false} for s3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-12-08-14-18-08/output/bert-train\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:09,809 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:10,315 INFO codegen.CodeGenerator: Code generated in 35.946351 ms\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:10,323 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 401.9 KB, free 363.6 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:10,335 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 42.8 KB, free 363.5 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:10,335 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.238.61:36471 (size: 42.8 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:10,336 INFO spark.SparkContext: Created broadcast 12 from save at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:10,337 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 84137401 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:10,392 INFO spark.SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:10,393 INFO scheduler.DAGScheduler: Got job 6 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:10,393 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (save at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:10,393 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:10,393 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:10,393 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[34] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:10,422 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 1096.1 KB, free 362.4 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:10,427 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 734.5 KB, free 361.7 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:10,427 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.238.61:36471 (size: 734.5 KB, free: 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:10,428 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:10,428 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 6 (MapPartitionsRDD[34] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:10,428 INFO cluster.YarnScheduler: Adding task set 6.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:10,429 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:10,430 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 7, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8478 bytes)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:10,442 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-2:35201 (size: 734.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:23:10,560 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-2:35201 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-12-08 14:46:02,183 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 7) in 1371753 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,543 INFO spark.ContextCleaner: Cleaned accumulator 114\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,547 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.238.61:36471 in memory (size: 6.4 KB, free: 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,549 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-2:35201 in memory (size: 6.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,556 INFO spark.ContextCleaner: Cleaned accumulator 160\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,557 INFO spark.ContextCleaner: Cleaned accumulator 177\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,557 INFO spark.ContextCleaner: Cleaned accumulator 178\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,557 INFO spark.ContextCleaner: Cleaned accumulator 174\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,557 INFO spark.ContextCleaner: Cleaned accumulator 179\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,557 INFO spark.ContextCleaner: Cleaned accumulator 165\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,557 INFO spark.ContextCleaner: Cleaned accumulator 170\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,557 INFO spark.ContextCleaner: Cleaned accumulator 175\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,557 INFO spark.ContextCleaner: Cleaned accumulator 183\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,557 INFO spark.ContextCleaner: Cleaned accumulator 187\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,557 INFO spark.ContextCleaner: Cleaned accumulator 181\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,557 INFO spark.ContextCleaner: Cleaned accumulator 161\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,557 INFO spark.ContextCleaner: Cleaned accumulator 171\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,559 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.238.61:36471 in memory (size: 42.8 KB, free: 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,559 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-2:35201 in memory (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,561 INFO spark.ContextCleaner: Cleaned accumulator 184\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,561 INFO spark.ContextCleaner: Cleaned accumulator 172\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,561 INFO spark.ContextCleaner: Cleaned accumulator 176\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,561 INFO spark.ContextCleaner: Cleaned accumulator 168\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,561 INFO spark.ContextCleaner: Cleaned accumulator 180\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,561 INFO spark.ContextCleaner: Cleaned accumulator 185\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,561 INFO spark.ContextCleaner: Cleaned accumulator 166\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,561 INFO spark.ContextCleaner: Cleaned accumulator 169\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,561 INFO spark.ContextCleaner: Cleaned accumulator 182\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,562 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.238.61:36471 in memory (size: 644.8 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,563 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-2:35201 in memory (size: 644.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,565 INFO spark.ContextCleaner: Cleaned accumulator 158\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,565 INFO spark.ContextCleaner: Cleaned accumulator 156\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,565 INFO spark.ContextCleaner: Cleaned accumulator 164\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,565 INFO spark.ContextCleaner: Cleaned accumulator 173\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,565 INFO spark.ContextCleaner: Cleaned accumulator 157\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,565 INFO spark.ContextCleaner: Cleaned accumulator 163\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,565 INFO spark.ContextCleaner: Cleaned accumulator 186\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,565 INFO spark.ContextCleaner: Cleaned accumulator 159\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,565 INFO spark.ContextCleaner: Cleaned accumulator 162\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,565 INFO spark.ContextCleaner: Cleaned accumulator 167\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,565 INFO spark.ContextCleaner: Cleaned accumulator 106\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,565 INFO spark.ContextCleaner: Cleaned accumulator 116\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,565 INFO spark.ContextCleaner: Cleaned accumulator 109\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,565 INFO spark.ContextCleaner: Cleaned accumulator 99\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,565 INFO spark.ContextCleaner: Cleaned accumulator 98\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,565 INFO spark.ContextCleaner: Cleaned accumulator 117\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,565 INFO spark.ContextCleaner: Cleaned accumulator 96\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,567 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.238.61:36471 in memory (size: 42.8 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,567 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-2:35201 in memory (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 122\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 108\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 121\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 118\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 105\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 110\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 93\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 104\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 107\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 94\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 123\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 97\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 101\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 95\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 103\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 102\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 113\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 115\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 111\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 112\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 100\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 120\u001b[0m\n",
      "\u001b[34m2020-12-08 14:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 119\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:13,433 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 4503004 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:13,433 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:13,434 INFO scheduler.DAGScheduler: ResultStage 6 (save at NativeMethodAccessorImpl.java:0) finished in 4503.038 s\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:13,435 INFO scheduler.DAGScheduler: Job 6 finished: save at NativeMethodAccessorImpl.java:0, took 4503.042312 s\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,122 INFO datasources.FileFormatWriter: Write Job 31cda5e1-26ec-44bd-9f5d-eaffc4ea377c committed.\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,126 INFO datasources.FileFormatWriter: Finished processing stats for write job 31cda5e1-26ec-44bd-9f5d-eaffc4ea377c.\u001b[0m\n",
      "\u001b[34mWrote to output file:  s3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-12-08-14-18-08/output/bert-train\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,172 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,172 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,172 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,173 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,232 WARN commit.AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,232 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,232 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,232 INFO commit.AbstractS3ACommitterFactory: Using Commmitter FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20201208153814_0000}; taskId=attempt_20201208153814_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@472170ab}; outputPath=s3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-12-08-14-18-08/output/bert-validation, workPath=s3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-12-08-14-18-08/output/bert-validation/_temporary/0/_temporary/attempt_20201208153814_0000_m_000000_0, algorithmVersion=2, skipCleanup=false, ignoreCleanupFailures=false} for s3a://sagemaker-us-east-1-322537213286/amazon-reviews-spark-processor-2020-12-08-14-18-08/output/bert-validation\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,232 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,589 INFO codegen.CodeGenerator: Code generated in 28.445414 ms\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,597 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 401.9 KB, free 363.7 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,614 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 42.8 KB, free 363.6 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,615 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.238.61:36471 (size: 42.8 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,616 INFO spark.SparkContext: Created broadcast 14 from save at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,617 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 84137401 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,709 INFO spark.SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,710 INFO scheduler.DAGScheduler: Got job 7 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,710 INFO scheduler.DAGScheduler: Final stage: ResultStage 7 (save at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,710 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,710 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,711 INFO scheduler.DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[41] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,746 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 1096.1 KB, free 362.6 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,753 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 734.6 KB, free 361.9 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,753 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.238.61:36471 (size: 734.6 KB, free: 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,754 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,760 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[41] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,760 INFO cluster.YarnScheduler: Adding task set 7.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,762 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 8, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,763 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 7.0 (TID 9, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8478 bytes)\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,775 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-2:35201 (size: 734.6 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 15:38:14,830 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-2:35201 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 200\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 224\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 206\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 203\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 221\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 208\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 218\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 210\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 223\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 205\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 219\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 209\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 189\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 191\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 213\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 217\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 207\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 211\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 220\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 198\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 196\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 199\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 192\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 190\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 225\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 216\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 188\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 226\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,569 INFO spark.ContextCleaner: Cleaned accumulator 227\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,573 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.238.61:36471 in memory (size: 42.8 KB, free: 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,574 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-2:35201 in memory (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,577 INFO spark.ContextCleaner: Cleaned accumulator 201\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,577 INFO spark.ContextCleaner: Cleaned accumulator 212\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,578 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.238.61:36471 in memory (size: 734.5 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,579 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-2:35201 in memory (size: 734.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,581 INFO spark.ContextCleaner: Cleaned accumulator 204\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,581 INFO spark.ContextCleaner: Cleaned accumulator 214\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,581 INFO spark.ContextCleaner: Cleaned accumulator 195\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,581 INFO spark.ContextCleaner: Cleaned accumulator 197\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,581 INFO spark.ContextCleaner: Cleaned accumulator 193\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,581 INFO spark.ContextCleaner: Cleaned accumulator 222\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,581 INFO spark.ContextCleaner: Cleaned accumulator 215\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,581 INFO spark.ContextCleaner: Cleaned accumulator 202\u001b[0m\n",
      "\u001b[34m2020-12-08 15:52:36,581 INFO spark.ContextCleaner: Cleaned accumulator 194\u001b[0m\n",
      "\u001b[34m2020-12-08 16:00:57,869 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 7.0 (TID 9) in 1363106 ms on algo-2 (executor 1) (1/2)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "running_processor.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span style=\"color:red\">위 Processing Job이 완료되기 전까지 기다려 주시기 바랍니다.</span></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the Processed Output Dataset 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $train_data_bert_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $validation_data_bert_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $test_data_bert_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = './data-tfrecord/bert-train'\n",
    "validation_data = './data-tfrecord/bert-validation'\n",
    "test_data = './data-tfrecord/bert-test'\n",
    "\n",
    "!aws s3 cp $train_data_bert_output $train_data --recursive\n",
    "!aws s3 cp $validation_data_bert_output $validation_data --recursive\n",
    "!aws s3 cp $test_data_bert_output $test_data --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store train_data_bert_output train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store validation_data_bert_output validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store test_data_bert_output test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
