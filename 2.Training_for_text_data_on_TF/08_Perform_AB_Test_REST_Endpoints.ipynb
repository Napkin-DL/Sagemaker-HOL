{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REST Endpoints 기반 A/B Test 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“production variants”이라는 개념으로 하나의 SageMaker Endpoint 내에서 여러 신규 모델을 테스트하고 배포 할 수 있습니다. 이러한 variants은 하드웨어 (CPU/GPU), 데이터(코미디/영화), 지역(US West 또는 AP South)에 따라 다를 수 있습니다. Canary 배포와 블루/그린 배포를 위해 endpoint의 모델 간에 트래픽을 전환 할 수 있습니다. A/B 테스트를 위해 트래픽을 분할 할 수 있습니다. 또한 초당 requests와 같이 특정 메트릭을 기반으로 endpoints를 scale-out 또는 in이 가능합니다. 더 많은 request이 들어 오면 SageMaker는 자동으로 model prediction API를 확장합니다.\n",
    "#### scale-out/in 기능은 Endpoint가 T 시리즈 인스턴스를 사용하는 경우 disable 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/model_ab.png\" width=\"80%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제 production 환경에서 서로 다른 모델을 비교하고 테스트하기 위해 트래픽 분할을 사용하여 사용자의 subset에 다른 모델 variants로 지정할 수 있습니다. 목표는 어떤 variants가 더 좋은 성능을 가지고 있는지 확인하는 것입니다. 이러한 테스트는 통계적으로 유의미하게 수행하기 위해 오랜 기간(주) 동안 실행해야하는 경우가 종종 있습니다. 위 그림에서 2개의 variants 사이에 임의의 50-50 트래픽 분할을 사용하여 배포 된 2개의 추천 모델을 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이전 Notebook에서 Endpoints를 생성하신 경우에는 Endpoints를 삭제하신 후 이 노트북을 생성하시기 바랍니다. 그렇지 않은 경우 HOL를 수행하시는 동안에 ResourceLimitExceeded error를 보실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_endpoints = sm.list_endpoints()\n",
    "\n",
    "for ep in list_endpoints['Endpoints']:\n",
    "    sm.delete_endpoint(EndpointName=ep['EndpointName'])\n",
    "    \n",
    "\n",
    "NextToken = 'None'\n",
    "while NextToken !='':\n",
    "    lec = sm.list_endpoint_configs(NextToken=NextToken) if NextToken != 'None' else sm.list_endpoint_configs()\n",
    "    for epc in lec['EndpointConfigs']:\n",
    "        print(epc['EndpointConfigName'])\n",
    "        sm.delete_endpoint_config(EndpointConfigName=epc['EndpointConfigName'])\n",
    "        time.sleep(3)\n",
    "    NextToken = lec['NextToken'] if lec.get('NextToken') else ''\n",
    "\n",
    "NextToken = 'None'\n",
    "while NextToken !='':\n",
    "    lec = sm.list_models(NextToken=NextToken) if NextToken != 'None' else sm.list_models()\n",
    "    for epc in lec['Models']:\n",
    "        print(epc['ModelName'])\n",
    "        sm.delete_model(ModelName=epc['ModelName'])\n",
    "        time.sleep(3)\n",
    "    NextToken = lec['NextToken'] if lec.get('NextToken') else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 내 Model 복사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = './models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 cp s3://$bucket/$training_job_name/output/model.tar.gz $models_dir/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pickle as pkl\n",
    "\n",
    "#!ls -al ./models\n",
    "\n",
    "tar = tarfile.open('{}/model.tar.gz'.format(models_dir))\n",
    "tar.extractall(path=models_dir)\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Signature 살펴보기\n",
    "CLI 를 통해 모델의 Input/output을 조회할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --all --dir $models_dir/tensorflow/saved_model/0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "client = boto3.client(\"sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training job에서 Model A의 variant 생성\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Training과 Inference 이미지가 다르기 때문에 `primary_container_image`가 필요합니다.\n",
    "* 기본적으로 Training 이미지가 사용되므로, 추가적으로 재정의가 필요합니다.\n",
    "* https://github.com/aws/sagemaker-python-sdk/issues/1379 참조\n",
    "* 이 variant는 Elastic Inference를 사용하므로 Elastic Inference 이미지가 필요합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timestamp = '{}'.format(int(time.time()))\n",
    "\n",
    "model_a_name = '{}-{}-{}'.format(training_job_name, 'var-a', timestamp)\n",
    "\n",
    "sess.create_model_from_job(name=model_a_name,\n",
    "                           training_job_name=training_job_name,\n",
    "                           role=role,\n",
    "                           image_uri='763104351884.dkr.ecr.{}.amazonaws.com/tensorflow-inference-eia:2.0.0-cpu-py36-ubuntu18.04'.format(region))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training job에서 Model B의 variant 생성\n",
    "Notes:\n",
    "* 이 모델은 Variant A와 동일하지만, EIA를 사용하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b_name = '{}-{}-{}'.format(training_job_name, 'var-b', timestamp)\n",
    "\n",
    "sess.create_model_from_job(name=model_b_name,\n",
    "                           training_job_name=training_job_name,\n",
    "                           role=role,\n",
    "                           image_uri='763104351884.dkr.ecr.{}.amazonaws.com/tensorflow-inference:2.0.0-cpu-py36-ubuntu18.04'.format(region))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canary Rollouts and A/B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Canary rollouts은 5% 정도의 사용자에게만 신규 모델을 안전하게 제공하는데 활용됩니다. 전체 사용자 기반에 영향을 주지 않고 실제 production에서 테스트를 하려는 경우에 유용합니다. 대부분의 트래픽은 기존 모델로 이동하므로 Canary 모델의 클러스터 크기는 트래픽이 5%에 불과하기 때문에 상대적으로 작을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`deploy()`함수를 사용하는 대신에 Canary 배포와 A/B 테스팅에 대한 다중 variants로 `Endpoint Configuration`를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timestamp = '{}'.format(int(time.time()))\n",
    "\n",
    "endpoint_config_name = '{}-{}'.format(training_job_name, timestamp)\n",
    "\n",
    "endpoint_config = client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "         'VariantName': 'VariantA',\n",
    "         'ModelName': model_a_name,\n",
    "         'InstanceType':'ml.m5.large',\n",
    "         'InitialInstanceCount': 1,\n",
    "         'InitialVariantWeight': 50,\n",
    "        'AcceleratorType':'ml.eia2.medium' # This variant will use an Elastic Inference Adapter (GPU)            \n",
    "        },\n",
    "        {\n",
    "         'VariantName': 'VariantB',\n",
    "         'ModelName': model_b_name,\n",
    "         'InstanceType':'ml.m5.large',\n",
    "         'InitialInstanceCount': 1,\n",
    "         'InitialVariantWeight': 50,\n",
    "        }\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/endpointConfig/{}\">REST Endpoint Configuration</a></b>'.format(region, endpoint_config_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = '{}-{}'.format(training_job_name, timestamp)\n",
    "\n",
    "endpoint_response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/endpoints/{}\">REST Endpoint</a></b>'.format(region, endpoint_name)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span style=\"color:red\">위 Endpoint가 Deploy되기 전까지 기다려 주시기 바랍니다.</span></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('sagemaker')\n",
    "waiter = client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Raw Text를 BERT Tokens로 변환하기 위한 Request Handler 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "class RequestHandler(JSONSerializer):\n",
    "    import json\n",
    "    \n",
    "    def __init__(self, tokenizer, max_seq_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.content_type = \"application/json\"\n",
    "\n",
    "    def serialize(self, instances):\n",
    "        transformed_instances = []\n",
    "\n",
    "        for instance in instances:\n",
    "            encode_plus_tokens = tokenizer.encode_plus(instance,\n",
    "                                                       pad_to_max_length=True,\n",
    "                                                       max_length=self.max_seq_length)\n",
    "\n",
    "            input_ids = encode_plus_tokens['input_ids']\n",
    "            input_mask = encode_plus_tokens['attention_mask']\n",
    "            segment_ids = [0] * self.max_seq_length\n",
    "\n",
    "            transformed_instance = {\"input_ids\": input_ids, \n",
    "                                    \"input_mask\": input_mask, \n",
    "                                    \"segment_ids\": segment_ids}\n",
    "\n",
    "            transformed_instances.append(transformed_instance)\n",
    "\n",
    "        transformed_data = {\"instances\": transformed_instances}\n",
    "\n",
    "        return json.dumps(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  BERT Response를 Predicted Classes로 변환하기 위한 Response Handler 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "class ResponseHandler(JSONDeserializer):\n",
    "    import json\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    def __init__(self, classes):\n",
    "        self.classes = classes\n",
    "        self.accept = 'application/json'\n",
    "        \n",
    "    def deserialize(self, response, accept_header):\n",
    "        import tensorflow as tf\n",
    "\n",
    "        response_body = response.read().decode('utf-8')\n",
    "\n",
    "        response_json = json.loads(response_body)\n",
    "\n",
    "        log_probabilities = response_json[\"predictions\"]\n",
    "\n",
    "        predicted_classes = []\n",
    "\n",
    "        # Convert log_probabilities => softmax (all probabilities add up to 1) => argmax (final prediction)\n",
    "        for log_probability in log_probabilities:\n",
    "            softmax = tf.nn.softmax(log_probability)    \n",
    "            predicted_class_idx = tf.argmax(softmax, axis=-1, output_type=tf.int32)\n",
    "            predicted_class = self.classes[predicted_class_idx]\n",
    "            predicted_classes.append(predicted_class)\n",
    "\n",
    "        return predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "request_handler = RequestHandler(tokenizer=tokenizer,\n",
    "                                 max_seq_length=128)\n",
    "\n",
    "response_handler = ResponseHandler(classes=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor 객체 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.model import TensorFlowPredictor\n",
    "\n",
    "predictor = TensorFlowPredictor(\n",
    "                        endpoint_name=endpoint_name,\n",
    "                        sagemaker_session=sess,\n",
    "                        serializer=request_handler,\n",
    "                        deserializer=response_handler,\n",
    "                        model_name='saved_model',\n",
    "                        model_version=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import json\n",
    "    \n",
    "reviews = [\"This is great!\", \n",
    "           \"This is not good.\"]\n",
    "\n",
    "predicted_classes = predictor.predict(reviews)\n",
    "%timeit predicted_classes = predictor.predict(reviews)\n",
    "\n",
    "for predicted_class, review in zip(predicted_classes, reviews):\n",
    "    print('[Predicted Star Rating: {}]'.format(predicted_class), review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REST Endpoint 성능 지표 검토"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/endpoints/{}\">REST Endpoint Performance Metrics</a></b>'.format(region, endpoint_name)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variant A로 모든 트래픽 이동\n",
    "\n",
    "\n",
    "_**No downtime** 트래픽 이동을 위한 작업이 수행되는 동안에 downtime이 없습니다._\n",
    "\n",
    "이 작업에도 몇 분의 시간이 걸립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_endpoint_config = [\n",
    "    {\n",
    "        'VariantName': 'VariantA',\n",
    "        'DesiredWeight': 100,\n",
    "    },\n",
    "    {\n",
    "        'VariantName': 'VariantB',\n",
    "        'DesiredWeight': 0,\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.update_endpoint_weights_and_capacities(\n",
    "    EndpointName=endpoint_name,\n",
    "    DesiredWeightsAndCapacities=updated_endpoint_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/endpoints/{}\">REST Endpoint</a></b>'.format(region, endpoint_name)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span style=\"color:red\">위 Endpoint가 update되는 동안은 기다려 주시기 바랍니다.</span></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('sagemaker')\n",
    "waiter = client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import json\n",
    "    \n",
    "reviews = [\"This is great!\", \n",
    "           \"This is not good.\"]\n",
    "\n",
    "predicted_classes = predictor.predict(reviews)\n",
    "%timeit predicted_classes = predictor.predict(reviews)\n",
    "\n",
    "for predicted_class, review in zip(predicted_classes, reviews):\n",
    "    print('[Predicted Star Rating: {}]'.format(predicted_class), review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비용절감을 위한 Variant A 삭제\n",
    "\n",
    "Endpoint configuration은 variant B만 사용하도록 수정합니다.\n",
    "\n",
    "_**No downtime** 트래픽 이동을 위한 작업이 수행되는 동안에 downtime이 없습니다._\n",
    "\n",
    "이 작업에도 몇 분의 시간이 걸립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timestamp = '{}'.format(int(time.time()))\n",
    "\n",
    "updated_endpoint_config_name = '{}-{}'.format(training_job_name, timestamp)\n",
    "\n",
    "updated_endpoint_config = client.create_endpoint_config(\n",
    "    EndpointConfigName=updated_endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "         'VariantName': 'VariantB',\n",
    "         'ModelName': model_b_name,  # Only specify variant B to remove variant A\n",
    "         'InstanceType':'ml.m5.large',\n",
    "         'InitialInstanceCount': 1,\n",
    "         'InitialVariantWeight': 100\n",
    "        }\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client.update_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=updated_endpoint_config_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/endpoints/{}\">REST Endpoint</a></b>'.format(region, endpoint_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('sagemaker')\n",
    "waiter = client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import json\n",
    "    \n",
    "reviews = [\"This is great!\", \n",
    "           \"This is not good.\"]\n",
    "\n",
    "predicted_classes = predictor.predict(reviews)\n",
    "%timeit predicted_classes = predictor.predict(reviews)\n",
    "\n",
    "for predicted_class, review in zip(predicted_classes, reviews):\n",
    "    print('[Predicted Star Rating: {}]'.format(predicted_class), review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ab_endpoint = endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store model_ab_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_endpoints = sm.list_endpoints()\n",
    "\n",
    "# for ep in list_endpoints['Endpoints']:\n",
    "#     sm.delete_endpoint(EndpointName=ep['EndpointName'])\n",
    "    \n",
    "\n",
    "# NextToken = 'None'\n",
    "# while NextToken !='':\n",
    "#     lec = sm.list_endpoint_configs(NextToken=NextToken) if NextToken != 'None' else sm.list_endpoint_configs()\n",
    "#     for epc in lec['EndpointConfigs']:\n",
    "#         print(epc['EndpointConfigName'])\n",
    "#         sm.delete_endpoint_config(EndpointConfigName=epc['EndpointConfigName'])\n",
    "#         time.sleep(3)\n",
    "#     NextToken = lec['NextToken'] if lec.get('NextToken') else ''\n",
    "\n",
    "# NextToken = 'None'\n",
    "# while NextToken !='':\n",
    "#     lec = sm.list_models(NextToken=NextToken) if NextToken != 'None' else sm.list_models()\n",
    "#     for epc in lec['Models']:\n",
    "#         print(epc['ModelName'])\n",
    "#         sm.delete_model(ModelName=epc['ModelName'])\n",
    "#         time.sleep(3)\n",
    "#     NextToken = lec['NextToken'] if lec.get('NextToken') else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
